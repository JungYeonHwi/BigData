# 데이터 수집 및 저장 계획

## SECTION 01. 데이터 수집 및 전환

### 01. 데이터 수집

1) 데이터 수집 : 데이터 처리 시스템에 들어갈 데이터를 모으는 과정으로 여러 장소에 있는 데이터를 한 곳으로 모으는 것

2) 비즈니스 도메인과 원천 데이터 정보 수집

- 비즈니스 도메인 정보
    - 비즈니스 모델, 비즈니스 용어집, 비즈니스 프로세스로부터 관련 정보를 습득
    - 도메인 전문가 인터뷰를 통해 데이터의 종류, 유형, 특징 정보를 습득
- 원천 데이터 정보 : 데이터 분석에 필요한 대상 원천 데이터의 수집 가능성, 데이터의 보안, 정확성을 탐색하고, 데이터 수집의 난이도, 수집 비용 등 기초 자료 수집 가능
    - 데이터의 수집 가능성 : 원천 데이터 수집의 용이성과 데이터 발생 빈도를 탐색하고, 데이터 활용에 있어서 전처리 및 후처리 비용을 대략 산정
    - 데이터의 보안 : 수집 대상 데이터의 개인정보 포함 여부, 지적 재산권 존재 여부를 판단하여 데이터 분석 시 발생할 수 있는 문제 예방
    - 데이터 정확성 : 데이터 분석 목적에 맞는 적절한 데이터 항목이 존재하고, 적절한 데이터 품질이 확보될 수 있는지 탐색
    - 수집 난이도 : 원천 데이터의 존재 위치, 데이터의 유형, 데이터 수집 용량, 구축비용, 정제 과정의 복잡성을 고려하여 데이터 탐색
    - 수집 비용 : 데이터를 수집하기 위해 발생할 수 있는 데이터 획득 비용을 산정

2) 내, 외부 데이터 수집

- 데이터의 종류
    - 내부 데이터는 조직 내부의 서비스 시스템, 네트워크 및 서버 장비, 마케팅 관련 시스템 등으로부터 생성되는 데이터
    - 외부데이터는 다양한 소셜 데이터, 특정 기관 데이터, M2M 데이터, LOD 등올 나눌 수 있음
- 데이터의 수집 주기
    - 내부 데이터는 조직 내부에서 습득할 수 있는 데이터로 실시간으로 수집하여 분석할 수 있도록 함
    - 외부 데이터는 일괄 수집으로 끝날지, 일정 주기로 데이터를 수집할지를 결정하여 수집 데이터 관리 정책을 정해야 함
- 데이터의 수집 방법
    - 내부 데이터는 분석에 적합한 정형화된 형식으로 수집되기 때문에 가공에 많은 노력을 기울이지 않아도 됨
    - 외부 데이터는 분석 목표에 맞는 데이터를 탐색, 수집하고, 분석 목표에 맞게 수집 데이터를 변환하는 노력이 필요

3) 데이터 수집 기술

- 데이터 유형별 데이터 수집 기술 : 수집 시스템 사양 설계를 위해 수집 데이터 유형을 파악하여 그에 맞는 수집 기술을 선정 가능
    - 정형 데이터
        - ETL : 수집 대상 데이터를 추출 및 가공하여 데이터 웨어하우스에 저장하는 기술
        - FTP : TCP/IP나 UDP 프로토콜을 통해 원격지 시스템으로부터 파일을 송수신하는 기술
        - API : 솔루션 제조사 및 3rd party 소프트웨어로 제공되는 도구로, 시스템 간 연동을 통해 실시간으로 데이터를 수신할 수 있도록 기능을 제공하는 인터페이스
        - DBToDB : 데이터베이스 관리시스템 간 데이터를 동기화 또는 전송하는 방법
        - 스쿱 : 관계형 데이터베이스와 하둡 간 데이터를 전송하는 방법
    - 비정형 데이터
        - 크롤링 : 인터넷상에서 제공되는 다양한 웹 사이트로부터 소셜 네트워크 정보, 뉴스, 게시판 등으로부터 웹 문서 및 정보를 수집하는 기술
        - RSS : 블로그, 뉴스, 쇼핑몰 등의 웹 사이트에 게시된 새로운 글을 공유하기 위해 XML 기반으로 정보를 배포하는 프로토콜
        - Open API : 응용 프로그램을 통해 실시간으로 데이터를 수신할 수 있도록 공개된 API
        - 척와 : 분산 시스템으로부터 데이터를 수집, 하둡 파일 시스템에 저장 실시간으로 분석할 수 있는 기능 제공
        - 카프카 : 대용량 실시간 로그처리를 위한 분산 스트리밍 플랫폼 기술
    - 반정형 데이터
        - 플럼 : 분산 환경에서 대량의 로그 데이터를 수집 전송하고 분석하는 기능 제공
        - 스크라이브 : 다수의 수집 대상 서버로부터 실시간으로 데이터를 수집, 분산 시스템에 데이터를 저장하는 기능 제공
        - 센싱 : 센서로부터 수집 및 생성된 데이터를 네트워크를 통해 활용하여 수집하는 기능 제공
        - 스트리밍 : 네트워크를 통해 센서 데이터 및 오디오, 비디오 등의 미디어 데이터를 실시간으로 수집하는 기술
- ETL
    - 하나 이상의 데이터 소스로부터 데이터 웨어하우스,데이터 마트, 데이터 통합, 데이터 이동 등 다양한 응용시스템을 위한 데이터 구축에 필요한 핵심 기술
    - ETL은 추출, 변환, 적재의 3단계 프로세스로 구성
- FTP
    - 대량의 파일(데이터)를 네트워크를 통해 주고받을 때 사용되는 파일 전송 서비스
    - 인터넷을 통한 파일 송수신 만을 위해 만들어진 프로토콜이기 때문에 동작 방식이 단순하고 직관적이며, 파일을 빠른 속도로 한꺼번에 주고받을 수 있음
    - 인터넷 프로토콜인 TCP / IP 위에서 동작
    - 서버와 클라이언트를 먼저 연결하고 이후에 데이터 파일 전송
    - FTP 서비스를 제공하는 서버와 접속하는 클라이언트 사이에 두 개의 연결을 생성 (데이터 제어 연결과 데이터 전송 연결)
    - 사용자 계정 및 암호 등의 정보나 파일 전송 명령 및 결과 등은 데이터 제어 연결에서, 이후 실제 파일 송수신 작업(올리기, 내려받기)은 데이터 전송 연결에서 처리
    - 데이터 제어 연결을 위해서 21번 포트, 데이터 전송 연결을 위해서 20번 포트 사용
- 정형 데이터 수집을 위한 아파치 스쿱 기술
    - 관계형 데이터 스토어 간에 대량 데이터를 효과적으로 전송하기 위해 구현된 도구
    - 커넥터를 사용하여 MySQL, Oracle, MS SQL 등 관계형 데이터베이스의 데이터를 하둡 파일시스템으로 데이터 수집
    - 관계형 데이터베이스에서 가져온 데이터들을 하둡 맵리듀스로 변환하고, 변환된 데이터들을 다시 관계형 데이터베이스로 내보낼 수 있음
    - 데이터 가져오기/내보내기 과정을 맵리듀스를 통해 처리하기 때문에 병렬처리가 가능하고 장애에도 강한 특징
    - 모든 적재 과정을 자동화하고 병렬처리 방식으로 작업
- 로그/센서 데이터 수집을 위한 아파치 플럼 기술
    - 아파치 플럼은 대용량의 로그 데이터를 효과적으로 수집, 집계, 이동시키는 신뢰성 있는 분산 서비스를 제공하는 솔루션
    - 스트리밍 데이터 흐름에 기반을 둔 간단하고 유연한 구조
    - 플럼에서 하나의 에이전트는 소스, 채널, 싱크로 구성 : 소스는 웹서버, 로그데이터서버 등 원시데이터소스와 연결되며, 소스로부터 들어오는 데이터는 큐의 구조를 갖는 채널로 들어간 후, 싱크를 통해 목표 시스템으로 전달
    - 플럼은 로그 데이터 수집과 네트워크 트래픽 데이터, 소셜 미디어 데이터, 이메일 메시지 등 대량의 이벤트 데이터 전송을 위해 사용
- 웹 및 소셜 데이터 수집을 위한 스크래피 기술
    - 웹사이트를 크롤링하고 구조화된 데이터를 수집하는 도구
    - API를 이용하여 데이터를 추출할 수 있어, 범용 웹크롤러로 사용될 수 있음
    - 파이썬 기반의 프레임워크로 스크랩 과정이 단순하며 한 번에 여러 페이지를 불러오기 수월

### 02. 데이터 유형 및 속성 파악

1) 데이터 수집 세부 계획 작성 : 데이터 유형, 위치, 크기, 보관방식, 수집주기, 확보비용, 데이터 이관 절차를 조사하여 세부 계획서 작성

2) 데이터 위치 및 비용 : 수집 데이터의 원천에 따라 내부 데이터와 외부 데이터로 구분하고 여러 요소를 고려하여 비용 산정

3) 수집되는 데이터 형태

- HTML
    - 웹 페이지를 만들 때 사용되는 문서 형식
    - 텍스트, 태그, 스크립트로 구성
- XML
    - 데이터를 표현하기 위해서 태그를 사용하는 언어
    - 엘리먼트, 속성, 처리명령, 엔티티, 주석, CDATA 섹션으로 구성
- JSON : 자바스크립트를 위해 객체 형식으로 자료를 표현하는 문서 형식이며, 경량의 데이터 교환 방식

4) 데이터 저장 방식

- 파일 시스템 : 데이터를 읽고, 쓰고, 찾기 위해 일정한 규칙으로 파일에 이름을 명명하고 파일의 위치를 지정하는 체계
- 관계형 데이터베이스 : 데이터의 종류나 성격에 따라 여러 개의 칼럼을 포함하는 정형화된 테이블로 구성된 데이터 항목들의 집합체
- 분산처리 데이터베이스 : 데이터의 집합이 여러 물리적 위치에 분산 배치되어 저장되는 데이터베이스

5) 데이터 적절성 검증

- 데이터 누락 검정 : 수집 데이터 세트의 누락, 결측 여부를 판단하여 누락 발생시 재수집
- 소스 데이터와 비교 : 수집 데이터와 소스 데이터의 사이즈 및 개수를 비교 검증
- 데이터의 정확성 점검 : 유효하지 않는 데이터 존재여부 점검
- 보안 사항 점검 : 수집 데이터의 개인정보 유무 등 보안 사항의 점검 필요
- 저작권 점검 : 데이터의 저작권 등 법률적 검토 수행
- 대량 트래픽 발생 여부 : 네트워크 및 시스템에 트래픽을 발생시키는 데이터 여부 검증

### 03. 분석 마스터 플랜과 로드맵 설정

1) 데이터 변환 : 데이터를 하나의 표현 형식에서 다른 형식으로 변형하는 과정

- 데이터 변환 방식의 종류
    - 비정형 데이터를 정형 데이터 형태로 저장하는 방식(관계형 데이터베이스)
    - 수집 데이터를 분산파일시스템으로 저장하는 방식(HDFS 등)
    - 주제별, 시계열적으로 저장하는 방식(데이터 웨어하우스)
    - 키-값 형태로 저장하는 방식(NoSQL)
- 데이터 변환 수행 자료
    - 데이터 수집 계획서
    - 수집 솔루션 메뉴얼
    - 데이터 변환 솔루션
    - 하둡 오퍼레이션 메뉴얼
    - 소프트웨어 아키텍처 개념도

2) 데이터베이스 구조 설계

- 수집 데이터를 저장하기 위한 데이터베이스 구조 설계
    - 수집 데이터를 바로 HDFS에 저장하여 데이터 분석 가능
    - 수집 데이터를 루비, 파이썬 등으로 데이터 변환 과정을 거쳐 데이터베이스에 저장하기도 함
- DBMS 구축 여부 결정
    - 수집 대상을 확인하고, 필요 데이터의 속성을 파악하여 DBMS 구축 여부 결정
    - 수집 데이터의 특성에 따라 저장 데이터베이스 생성 여부 결정 : 수집 데이터가 정형 데이터일 경우는 수집 솔루션을 거쳐 바로 데이터베이스에 저장하지만, 그렇지 않은 경우 저장하고자 하는 데이터베이스의 종류를 선택하고 데이터에 맞게 모델링
    - 저장 데이터베이스는 분석이 쉬운 RDBMS를 보편적으로 사용
- 저장 데이터베이스 결정 : 다양한 사용, 비상용, 오픈소스 DBMS를 검토
- DBMS 설치 : 선택한 DBMS 설치하고, 정상적인 설치 여부 확인
- 테이블 구조 설계
    - 필요 데이터의 속성을 구체적으로 팡가
    - 테이블 구조를 설계하여 테이블 생성

3) 비정형/반정형 데이터의 변환 : 데이터 전처리나 후처리가 수행되기 전에 비정형/반정형 데이터를 구조적 형태로 전환하여 저장하는 과정

- 수집 데이터의 속성 구조 파악
    - 수집할 데이터 파악
    - 수집할 데이터 구조를 정의하고 적절한 변수명으로 구분
- 데이터 수집 절차에 대한 수행 코드 정의
    - 추출하고자 하는 정보들의 위치와 정보 구조 파악
    - 필요 데이터 추출
- 데이터 저장 프로그램 작성 : 생성된 데이터베이스 테이블에 수집 데이터를 저장하는 프로그램 작성
- 데이터베이스에 저장 : 데이터베이스 테이블로 수집 데이터 저장

4) 융합 데이터베이스 설계

- 데이터의 유형과 의미를 파악하여 활용 목적별 융합 DB 설계
- 활용 업무데이터 요구사항을 분석하고, 데이터 표준화 활동 및 모델링 과정을 수행해야 함
- 요구사항 분석
    - 업무 활용 목적과 방향을 파악하여 어떤 데이터의 속성들이 필요한지 파악
    - 필요한 데이터 항목, 개인정보 또는 민감정보 포함 여부 식별
- 데이터 표준화와 모델링 수행
    - 표준 코드, 표준 용어, 데이터 도메인(데이터값이 공통으로 갖는 형식과 값의 영역) 등을 정의
    - 개념적 설계 수행 : 저장된 데이터를 엔티티와 애트리뷰트로 추출하여, 앤티티 간의 관계를 정의하고 ER 다이어그램을 그림
    - 논리적 설계 수행 : 작성된 ER 다이어그램을 기반으로 매핑하여 관계형 스키마를 만들어 냄

5) 고려사항

- 비정형, 반정형 데이터를 데이터 분석의 용이성을 위해 정형화된 데이터베이스로 변환함에 집중
- 수집 데이터의 속성 구조를 정확히 파악하여야 툴을 이용한 데이터를 쉽게 저장 가능
- 융합 DB 구성은 활용 업무 목적을 정확히 판단하는 것이 중요하고, 융합 DB는 쉽게 자동화 구축될 수 있도록 설계


### 04. 데이터 비식별화

1) 비식별화 개요

- 개인정보 비식별화는 개인정보를 식별할 수 있는 값들을 몇 가지 정해진 규칙으로 대체하거나 사람의 판단에 따라 가공하여 개인을 알아볼 수 없도록 하는 조치
- 정보주체를 알아볼 수 없도록 비식별 조치를 적정하게 한 비식별 정보는 개인 정보가 아닌 것으로 추정되며, 따라서 빅데이터 분석 등에 활용 가능
- 식별자와 속성자
    - 식별자는 개인 또는 개인과 관련한 사물에 고유하게 부여된 값 또는 이름
    - 데이터셋에 포함된 식별자는 원칙적으로 삭제조치하며, 데이터 이용 목적상 필요한 식별자는 비식별 조치 후 활용
    - 속성자는 개인과 관련된 정보로서 다른 정보와 쉽게 결합하는 경우 특정 개인을 알아볼 수도 잇는 정보
    - 데이터셋에 포함된 속성자도 데이터 이용 목적과 관련이 없는 경우에는 원칙적으로 삭제하며, 데이터 이용 목적과 관련이 있을 경우 가명처리, 총계처리 등의 기법을 활용하여 비식별 조치
- 비식별 조치 방법
    - 가명처리, 총계처리, 데이터 삭제, 데이터 범주화, 데이터 마스킹 등 여러 가지 기법을 단독 또는 복합적으로 활용
    - 각각의 기법에는 이를 구현할 수 있는 다양한 세부기술이 있으며, 데이터 이용 목적과 기별법 장, 단점 등을 고려하여 적절한 기법, 세부기술을 선택, 활용

2) 가명처리 : 개인 식별이 가능한 데이터를 직접적으로 식별할 수 없는 다른 값으로 대체하는 기법

- 휴리스틱 가명화
    - 식별자에 해당하는 값들을 몇 가지 정해진 규칙으로 대체하거나 사람의 판단에 따라 가공하여 자세한 개인정보를 숨기는 방법
    - 식별자의 분포를 고려하거나 수집된 자료의 사전 분석을 하지 않고도 모든 데이터를 동일한 방법으로 가공하기 때문에 사용자가 쉽게 이해하고 활용 가능
    - 활용할 수 있는 대체 변수에 한계가 있으며, 다른 값으로 대체하는 일정한 규칙이 노출되는 취약점이 있어 규칙 수립 시 개인을 쉽게 식별할 수 없도록 세심한 고려 필요
    - 적용정보 : 성명, 사용자 ID, 소속(직장)명, 기관번호, 주소, 신용등급, 휴대전화번호, 우편번호, 이메일 주소 등
- 암호화
    - 정보 가공시 일정한 규칙의 알고리즘을 적용하여 암호화함으로써 개인정보 대체하는 방법
    - 통상적으로 다시 복호가 가능하도록 복호화 키를 가지고 있어서 이에 대한 보안방안 필요
    - 일방향 암호화를 사용하는 경우는 이론상 복호화가 원천적으로 불가능
    - 적용정보 : 주민등록번호, 여권번호, 의료보험번호, 외국인등록번호, 사용자 ID, 신용카드번호, 생체정보 등
- 교환 방법
    - 기존의 데이터베이스의 레코드를 사전에 정해진 외부의 변수(항목)값과 연계하여 교환
    - 적용정보 : 사용자 ID, 요양기관번호, 기관번호, 나이, 성별, 신체정보(신장, 혈액형 등), 소득, 휴대전화번호, 주소 등

3) 총계처리

- 통계 값(전체 혹은 부분)을 적용하여 특정 개인을 식별할 수 없도록 함
- 개인과 직접 관련된 날짜 정보(생일, 자격 취득일), 기타 고유 특징(신체정보, 진료기록, 병력정보, 특정소비기록 등 민감한 정보)을 주요 대상
- 데이터 전체 또는 부분을 집계(총합, 평균 등)
- 부분 총계 : 데이터셋 내 일정부분 레코드만 총계 처리하며, 다른 데이터 값에 비하여 오차 범위가 큰 항목을 통계값(평균 등)으로 변환
- 라운딩
    - 집계 처리된 값에 대하여 라운딩(올림, 내림, 반올림) 기준을 적용하여 최종 집계 처리하는 방법
    - 일반적으로 세세한 정보보다는 전체 통계정보가 필요한 경우 많이 사용
- 재배열
    - 기존 정보 값은 유지하면서 개인이 식별되지 않도록 데이터를 재배열하는 방법
    - 개인의 정보를 타인의 정보와 뒤섞어서 전체 정보에 대한 손상 없이 특정 정보가 해당 개인과 연결되지 않도록 함
    - 적용정보 : 나이, 신장, 소득, 질병, 신용등급, 학력 등

4) 데이터 삭제 : 개인을 식별할 수 있는 정보(이름, 전화번호, 주소, 생년월일, 사진, 고유식별정보, 생체정보, 기타)를 주요 대상으로 함

- 식별자 (부분) 삭제
    - 원본 데이터에서 식별자를 단순 삭제하는 방법과 일부만 삭제하는 방법
    - 남아 있는 정보 그 자체로도 분석의 유효성을 가져야 함과 동시에 개인을 식별할 수 없어야 하며, 인터넷 등에 공개되어 있는 정보 등과 결합하였을 경우에도 개인을 식별할 수 없어야 함
- 레코드 삭제
    - 다른 정보와 뚜렷하게 구별되는 레코드 전체를 삭제하는 방법
    - 통계분석에 있어서 전체 평균에 비하여 오차범위를 벗어나는 자료를 제거할 때에도 사용 가능
- 식별요소 전부삭제
    - 식별자뿐만 아니라 잠재적으로 개인을 식별할 수 있는 속성자까지 전부 삭제하여 프라이버시 침해 위험을 줄이는 방법
    - 개인정보 유출 가능성을 최대한 줄일 수 있지만 데이터 활용에 필요한 정보까지 사전에 모두 없어지기 때문에 데이터의 유용성이 낮아지는 문제 발생

5) 데이터 범주화 : 특정 정보를 해당 그룹의 대푯값 또는 구간값으로 변환(범주화)하여 개인 식별 방지

- 감추기
    - 명확한 값을 숨기기 위하여 데이터의 평균 또는 범주 값으로 변환하는 방식
    - 다만 특수한 성질을 지닌 개인으로 구성된 단체 데이터의 평균이나 범주 값은 그 집단에 속한 개인의 정보를 쉽게 추론 가능
- 랜덤 라운딩 : 수치 데이터를 임의의 수 기준으로 올림 또는 내림하는 기법으로 수치 데이터 이외의 경우에도 확장 적용 가능
- 범위 방법
    - 수치 데이터를 임의의 수 기준의 범위로 설정하는 기법
    - 해당 값의 범위 또는 구간으로 표현
- 제어 라운딩
    - 랜덤 라운딩 방법에서 어떠한 특정 값을 변경할 경우 행과 열의 합이 일치하지 않는 단점 해결을 위해 행과 열이 맞지 않는 것을 제어하여 일치시키는 기법
    - 컴퓨터 프로그램으로 구현하기 어렵고 복잡한 통계표에는 적용하기 어려우며, 해결할 수 있는 방법이 존재하지 않을 수 있어 아직 현장에서는 잘 사용하지 않음

6) 데이터 마스킹 : 데이터의 전부 또는 일부분을 대체 값(공백, 노이즈 등)으로 변환

- 임의 잡음 추가
    - 개인 식별이 가능한 정보에 임의의 숫자 등 잡음을 추가(더하기 또는 곱하기) 하는 방법
    - 지정된 평균과 분산의 범위 내에서 잡음이 추가되므로 원 자료의 유용성을 해치지 않으나, 잡음 값은 데이터 값과는 무관하기 때문에, 유효한 데이터로 활용하기 곤란
- 공백과 대체 : 특정 항목의 일부 또는 전부를 공백 또는 대체문자로 바꾸는 기법

7) 적정성 평가

- 개인정보 비식별 조치가 충분하지 않은 경우 공개 정보 등 다른 정보와의 결합. 다양한 추론 기법 등을 통해 개인이 식별될 우려가 있으므로, 개인정보 보호책임자 책임 하에 외부전문가가 참여하는 '비식별 조치 적정성 평가단'을 구성, 개인식별 가능성에 대한 엄격한 평가 필요
- 적정성 평가 시 프라이버시 보호 모델 중 최소한의 수단으로 k-익명성을 활용하며, 필요시 추가적인 평가모델 활용
- k-익명성
    - 공개된 데이터에 대한 연결공격 등 취약점을 방어하기 위해 제안된 개인정보보호 모델로 비식별화 조치를 위한 최소한의 기준으로 사용
    - k-익명성은 주어진 데이터 집합에서 같은 값이 적어도 k개 이상 존재하도록 하여 쉽게 다른 정보로 결합할 수 없도록 함
- l-다양성
    - k-익명성에 대한 두 가지 공격, 즉 동질성 공격 및 배경지식에 의한 공격을 방어하기 위한 모델로, 주어진 데이터 집합에서 함께 비식별되는 레코드들은 적어도 l개의 서로 다른 정보를 가지도록 함
    - 비식별 조치 과정에서 충분한 다양한 서로 다른 정보를 갖도록 동질 집합을 구성함으로써 다양성의 부족으로 인한 공격에 방어가 가능하고, 배경지식으로 인한 공격에도 일정 수준의 방어능력을 가질 수 있음
- t-근접성
    - l-다양성의 취약점(쏠림 공격, 유사성 공격)을 보완하기 위한 모델로 값의 의미를 고려하는 모델
    - t-근접성은 동질 집합에서 특정 정보의 분포와 전체 데이터 집합에서 정보의 분포가 t 이하의 차이를 보여야 하며, 각 동질 집합에서 '특정 정보의 분포'가 전체 데이터집합의 분포와 비교하여 너무 특이하지 않도록 함
    - t-접근성은 정보의 분포를 조정하여 정보가 특정 값으로 쏠리거나 유사한 값들이 뭉치는 경우를 방지하는 방법
    - t 수치가 0에 가까울수록 전체 데이터의 분포와 특정 데이터 구간의 분포 유사성이 강해지기 때문에 그 익명성의 방어가 더 강해지는 경향
    - 익명성 강화를 위해 특정 데이터들을 재배치해도 전체 속성자들의 값 자체에는 변화가 없기 때문에 일반적인 경우에 정보 소신의 문제가 크지 않음

### 05. 데이터 품질 검증

1) 데이터 품질 관리

- 정의 : 비즈니스 목표에 부합한 데이터 분석을 위해 가치성, 정확성, 유용성 있는 데이터를 확보하고, 신뢰성 있는 데이터를 유지하는 데 필요한 관리 활동
- 중요성
    - 분석 결과의 신뢰성은 분석 데이터의 신뢰성과 직접 연계
    - 빅데이터의 특성을 반영한 데이터 품질 관리 체계를 구축하여 효과적인 분석결과 도출

2) 데이터 품질

- 정형 데이터 품질 기준
    - 완전성 : 필수항목에 누락이 없어야 함
    - 유일성 : 데이터 항목은 유일해야 하며 중복되어서는 안됨
    - 유효성 : 데이터 항목은 정해진 데이터 유효범위 및 도메인을 충족
    - 일관성 : 데이터가 지켜야할 구조, 값, 표현되는 형태가 일관되게 정의되고, 서로 일치
    - 정확성 : 실세계에 존재하는 객체의 표현 값이 정확히 반영
- 비정형 데이터 품질 기준 : 비정형 데이터 자체에 대한 품질 기준은 컨텐츠 유형에 따라 다소 다름

3) 데이터 품질 진단 기법

- 정형 데이터 품질 진단 : 데이터 프로파일링 기법을 통해 진단 가능
- 비정형 데이터 품질 진단 : 품질 세부 기준을 정하여 항목별 체크리스트를 작성하여 진단

4) 데이터 품질 검증 수행

- 수집 데이터 품질 보증 체계를 수립하여 품질 점검 수행 후 품질검증 결과서 작성
- 품질 점검 수행 과정에서 데이터 오류수정이 용이하지 않을 경우 데이터를 재수집

## SECTION 02. 데이터 적재 및 저장

### 01. 데이터 적재

1) 데이터 적재 도구

### 02. 데이터 확보 계획

1) 데이터 확보를 위한 사전 검토사항

- 필요 데이터의 정의
    - 분석 목적에 맞는 데이터를 정의하고, 필요한 데이터를 확보할 수 있는지 확인하여야 하며, 확보할 수 없다면 대안을 함께 고려
    - 기업 내부 및 외부 공공기관이나 협력관계의 타 기업 담당자, 전문가 등 이해관계자들과 확보 가능한 데이터의 목록과 기대효과 등을 작성
- 보유 데이터의 현황 파악 : 사전에 정의한 데이터의 존재 여부와 분석 품질을 보장할 만큼 데이터 품질이 우수한지, 충분한 양이 존재하는지 확인
- 분석 데이터의 유형
    - 분석 데이터 확보를 위해 수집 대상 데이터의 유형을 고려
    - 어떤 데이터를 어떤 기법을 이용하여 분석할 것인지 수립된 계획에 따라 데이터의 유형을 선택하고 변수 정의
- 편향되지 않고 충분한 양의 데이터 규모
    - 데이터 분석 기법에 따라 훈련 데이터센, 검증 데이터셋, 테스트 데이터셋 필요
    - 신뢰성 높은 데이터 분석 모형 개발과 정확한 데이터 분석을 위해 3가지 데이터 세트로 나누어 사용할 만큼 충분한 데이터 확보되어야 함
- 내부 데이터의 사용
    - 필요 데이터에 대한 데이터 목록(변수 명칭, 설명, 형태, 기간, 용량, 권한 등) 작성
    - 필요 데이터에 대한 관련 법률이나 보안적인 요소들을 확인하고, 개인정보일 경우 비식별 조치방안 함께 고려
- 외부 데이터의 수집
    - 필요 데이터에 대한 데이터 목록을 데이터를 보유한 기업의 이름과 데이터 제공 방법(Open API, 복제 등)까지 고려하여 작성
    - 필요 데이터의 수집이 관련 법률이나 제도상 제약이 없는지 검토

2) 분석에 필요한 변수 정의 : 데이터 분석 요건에 따라 도출된 활용 시나리오에 적합한 데이터의 유형 및 분석 변수 정의

- 데이터 수집 기획
    - 데이터 수집 기법을 활용하여 필요 데이터를 배치 자동화로 수집
        - 데이터 수집 타깃 시스템 또는 사이트 선별
        - 수집 대상 화면, 텍스트를 위해 인덱스 생성 기획
        - 대상 시스템별 데이터 수집을 위한 크롤러를 준비하고 저장소 기획
        - 크롤링 주기, 대상 범위를 확정하고 데이터 수집 기획
    - 데이터 거래소, 공공 데이터에 적재된 분야별 데이터를 분류하고 선별
        - 검색한 공공 데이터 중 분석 대상이 되는 도메인의 우선순위 정의
        - 필요한 데이터를 다운로드받아 저장할 수 있도록 계획
        - 저장한 데이터를 NoSQL 데이터에 적재하고 정제할 수 잇도록 설계
- 분석 변수 정의
    - 빅데이터의 특징을 고려하여 분석 변수 생성 기획 : 상관관계 분석을 위한 데이터 연속성 범주 등을 고려하여 분석 변수 정의
    - 분석 변수 유형과 형성 알고리즘을 이용하여 분석 유형 도출 : 변수의 분포를 구별하는 정도에 따라 순수도 또는 불순도에 의해서 측정 구간별 순수도를 가장 높이는 분석 변수 도출

3) 분석 변수 생성 프로세스 정의 : 분석 대상에 대해 객관적으로 인식하고 논리적 인과관계 분석 및 데이터 간 상관관계 분석을 위한 분석 변수 생성 프로세스 정의

- 객관적 사실 기반의 문제 접근 : 명확한 문제 인식을 위하여 분석적이고 가정에 의한 접근 방법과 함께 무엇이 문제인지를 파악하여 객관적 관찰 데이터 유형 식별
- 데이터의 상관 분석 : 빅데이터 분석 대상의 연관성 분석을 통해 데이터 집합 간 통계적 관련성을 분석할 수 있는 변수를 생성하고 변수의 척도를 분류
- 프로토타입을 통한 분석 변수 접근 : 의미 있는 분석 변수를 생성하기 위하여 프로토타이핑 접근법을 통해 결과를 확인하며, 반복적으로 개선하여 필요한 데이터를 식별하고 구체화하여 비정형 데이터가 갖는 문제 해소

4) 생성된 분석 변수의 정제를 위한 점검항목 정의 : 분석 기획 단계에서 도출된 문제 인식, 해결을 위한 개념적 대안 설계를 통해 도출된 데이터에 대해 가용성을 평가하고 점검항목 정의

- 분석 변수 점검의 필요성
    - 데이터의 가용성과 적정성이 부족할 경우 문제 해결 및 활용 시나리오 적용을 통해 가치 있는 결과를 도출하기 어려움
    - 실행 전 분석 변수를 논리적 지표에 따라 점검
- 분석 변수 점검항목 정의
    - 데이터 수집
        - 데이터 적정성 : 문제 해결에 적절한 분석 변수인가?
        - 데이터 가용성 : 수집 가능한 데이터인가?
        - 대체 분석 데이터 유무 : 수집 불가능한 데이터인 경우 간접적으로 연관성 있는 데이터로 대체 가능한가?
    - 데이터 적합성
        - 데이터 중복 : 중복이나 노이즈 제거, 데이터값 존재 유무 등 기초 데이터 클렌징 수행 가능한가?
        - 분석 변수별 범위 : 분석 변수별 측정될 수 있는 min/max를 확인하였는가?
        - 분석 변수별 연관성 : 수집된 데이터 간 충분 간격으로 연관성이 있는가?
        - 데이터 내구성 : 데이터 노이즈, 왜곡이 발생하였을 때 예측 성능을 보장할 수 있는가?
    - 특징 변수
        - 특징 변수 사용 : 분석 변수 중 바로 특징 변수로 사용할 수 있는 가능성이 있는가?
        - 변수 간 결합 가능 여부 : 분석 변수를 결합하여 교차 검증을 할 수 있는가?
    - 타당성
        - 편익/비용 검증 : 분석 비용관 분석 후 결과가 추가적 매출, 수익 등에 기여할 수 있는가?
        - 기술적 타당성 : 다양한 분석 툴을 활용할 수 있는 분석 변수를 도출하였는가?
    
5) 생성된 분석 변수의 전처리 방법 수립 : 데이터 정제를 위한 점검항목 정의 후 이에 맞게 논리적 모형 설계를 위한 데이터 전처리 방법 수립

- 데이터 전처리 수행
    - 다양한 비즈니스 도메인에서 추출한 정형, 반정형, 비정형 데이터를 분석 및 처리에 적합한 데이터 형태로 조작
    - 데이터 정제, 통합, 축소, 변환을 반복적으로 수행하여 분석 변수로 활용하는 방안 수립 가능

|처리 기법|내용|
|데이터 정제|결측값을 채우거나 이상치를 제거하는 과정을 통해 데이터의 신뢰도를 높이는 작업|
|데이터 통합|다수의 정제된 데이터를 통합하여 표현하는 작업|
|데이터 축소|데이터 집합의 크기는 더 작지만 분석 결과는 같은 데이터 집합으로 만드는 작업|
|데이터 변환|데이터 마이닝의 효율을 높이기 위한 변환 및 변형 작업|

- 빅데이터 분석 프로세스 수행
    - 다양한 업무와 도메인이 포함되어 있어 완전히 자동화하여 처리하는 것은 어려움
    - 데이터 전처리 과정은 정제와 통합을 통해 약 60~80% 처리

6) 생성 변수의 검증 방안 수립

- 분석 변수의 데이터 검증 방안 수립
    - 모든 개별 데이터에 대한 타당성 보장보다는 빅데이터 개념 및 특성 측면에서 관리되어야 하는 항목과 수준에 대해 품질 검증 정의
        - 대량 데이터
            - 데이터 사용자 오류는 무시
            - 데이터 타당성에 치명적인 예외 상황만 탐지
        - 정밀 데이터
            - 개별 데이터에 대한 타당성 검증은 환경 및 상황에 따라 판단
            - 데이터 전체가 나타내는 의미를 중심으로 검증 기준 정의
        - 데이터 출처 불명확 : 명확한 목적이나 사전 통제 없이 생성된 데이터에 대한 별도 품질 기준 정의
    - 빅데이터 품질 관리 및 검증은 정확성보다는 데이터의 양이 충분한지에 대한 충분성 개념하에 조직의 비즈니스 영역 및 목적에 따라 검증
        - 정확성 : 데이터 사용 목적에 따라 데이터 정확성의 기준 상이하게 적용
        - 완전성 : 필요한 데이터인지 식별하는 수준으로 품질 요소 적용
        - 적시성
            - 소멸성이 강한 데이터에 대한 품질 기준 판단
            - 웹로그 데이터, 트윗 데이터 등 지속적으로 생성 소멸하는 데이터에 대한 품질 기준 수립
        - 일관성 : 동일한 데이터의 경우에도 사용 목적에 따라 데이터의 의미가 달라지기 때문에 분석 요건에 따른 검증 요소 적용
- 데이터 검증 체계 수립
    - 수집한 데이터의 출처가 명확한지 검증
    - 중복된 데이터가 존재하는지, 정보 활용에 컴플라이언스 이슈가 없는지 데이터 관리 대상 선별 검증
    - 데이터의 다양성이 확보되었는지, 데이터셋이 충분한지 검증
    - 주요 품질 지표의 조건을 만족하는지, 분석, 검증, 테스트 데이터가 분리되어 있는지 주요 품질 지표를 분석 및 검증

### 03. 분석 절차와 작업 계획

1) 분석 절차 : 데이터 분석의 시발점이 되는 문제 인식에서부터 시작하여 데이터를 확보하고 분석하여 결과를 도출 및 제시하는 단계까지의 일반적인 과정을 정형화한 프로세스

- 분석 절차의 특징
    - 데이터 분석을 수행하기 위한 기본적인 과정 명시
    - 분석 방법론을 구성하는 최소 요건
    - 상황에 따라 단계를 추가할 수도 있으며 생략 가능
- 일반적인 분석 절차
    - 문제 인식
        - 문제를 인식하고 분석 목적을 명확하게 정의
        - 분석 주제는 가설 형태 또는 결과 해석을 중심으로 할 수 있음
    - 연구조사
        - 문제 해결을 위한 각종 문헌을 조사하고 내용을 바탕으로 문제에 대한 해결방안 정의
        - 중요한 요인이나 변수들을 파악
    - 모형화
        - 복잡한 문제를 논리적이면서도 단순화하는 과정
        - 많은 변수가 포함된 현실 문제를 특징적 변수로 정의
        - 문제를 변수들 간의 관계로 정의
    - 데이터 수집
        - 데이터 수집 또는 변수를 측정하는 과정
        - 기존 데이터 활용이 불가능한 경우 추가적인 데이터 수집 고려
    - 데이터 분석
        - 수집된 데이터로부터 인사이트를 발굴
        - 수집된 데이터로부터 변수들간의 관계 분석
    - 분석 결과 제시
        - 변수들 간 인과관계나 상관관계를 포함한 분석 결과를 제시하고 공유
        - 표, 그림, 차트, 그래프 등을 활용하여 시각화
- 분석 절차 적용 시 고려사항
    - 문제에 대한 구체적 정의가 가능하고, 필요 데이터를 보유하고 있으며, 분석역량을 갖추고 있다면 통계 기반의 전통적 데이터 분석 수행
    - 문제에 대한 구체적 정의가 없다면 데이터 마이닝 기반으로 데이터를 분석하여 인사이트를 발굴하거나 일단 데이터 분석을 시도한 후 겨로가를 확인해 가면서 반복적으로 개선 결과 도출 가능

2) 작업 계획 : 분석 절차에 따라 데이터 분석 업무를 수행하기 위한 전반적인 작업 내용들을 세부적으로 정의하는 과정

- 분석 작업 계획 수립
    - 프로젝트 소요비용 배분
        - 주어진 시스템 및 데이터 환경을 고려하여 현실성 있는 계획이 되도록 프로젝트 일정 수립
        - 사전에 작성해 놓은 데이터 분석목표정의서의 내용이 모두 반영될 수 있도록 함
    - 프로젝트 작업분할구조 수립 : 데이터 분석목표정의서와 프로젝트 소요비용 배분 계획을 참고하여 데이터 분석 절차에 맞게 수립
    - 프로젝트 업무 분장 계획 및 배분
        - 배분된 인건비를 기준으로 단계별 인원 투입 계획을 수립하고 역할별로 작성해야 하는 필수 산출물을 정의
        - 프로젝트 유관부서 리더들과 프로젝트 참여 인원을 중심으로 프로젝트 평가위원회를 구성
        - 상황에 따라 외부 자문 위원을 참여
- 분석 작업 계획 수립을 위한 작업분할구조(WBS)
    - 데이터 분석과제 정의
        - 데이터 분석목표정의서를 기준으로 프로젝트 전체 일정에 맞춰 사전에 준비
        - 각 단계별 필요 산출물과 보고서 작성 시기, 세부 일정 등 정리
    - 데이터 준비 및 탐색
        - 데이터 엔지니어가 데이터를 수집하고 정리하는 일정을 수립
        - 데이터 분석가가 분석에 필요한 데이터들로부터 변수 후보를 탐색하고 최종 산출물을 도출하는 일정 수립
        - 데이터 분석 가설을 세우고 유의미한 검정을 수행하는 일정을 포함
    - 데이터 분석 모델링 및 검증
        - 실험방법 및 절차를 구분하고 검증하는 내용과 수행일정을 상세하게 수립
        - 데이터 분석 모델링 작업이 1회 이상 수행되므로 검증일정 고려하여 세부 일정 수립
    - 산출물 정리
        - 데이터 분석 단계별 산출물을 정리, 모델링 과정에서 개발된 분석 스크립트를 최종 산출물로 정리
        - 전체 일정에서 산출물 정리 과정을 반드시 포함
        
3) 분석목표 정의서 : 문제의 개선방향에 맞는 현실적인 분석목표를 수립하여 필요한 데이터에 대한 정보나 분석 타당성 검토 및 성과측정 방법 등을 정리한 정의서

- 분석목표정의서 구성요소
    - 원천 데이터 조사
        - 데이터 정보 : 데이터 축적 기간, 획득 주기, 테이블 스키마, 메타 데이터를 확인
        - 데이터 수집 난이도
            - 데이터 수집 및 정제 과정, 시기와 방법 확인
            - 데이터 수집 난이도가 높을 경우 데이터 활용 재고
    - 분석 방안 및 적용 가능성 판단
        - 개선 목표와 현시점의 분석 목표 간 차이를 고려하여 분석 목표를 조정하거나 상황에 따라 우선순위 조정
        - 분석 목표에 부합한 데이터 분석 기법이 있더라도 현재 적합한 분석 환경이 구축되지 않았다면 분석 목표 조정
    - 성과평가 기준
        - 정성적 평가
            - 분석 기법이나 기술의 활용 가능성 평가
            - 신규 데이터나 외부 데이터의 활용 가능성 평가
            - 세분화나 군집화를 통해 집단 선정
            - 이 외 관련 시스템별로 정성적 요소 평가
        - 정량적 평가
            - 기존 방법 대비 효과의 증감 비율 평가
            - 유효한 가설의 수나 목표 대비 증감 비율 평가
            - 데이터 모형의 정확도를 측정하여 평가
            - 기타 분석 특성에 따른 자체 KPI에 의한 성과 측정
- 분석목표정의서 작성 방법
    - 분석 목적을 설정하고 이를 달성하기 위한 세부 목표 수립
    - 필요한 데이터를 정의하고, 분석 방법과 데이터 수집 및 분석 난이도, 수행 주기, 분석 결과에 대한 검증 기준 설계
    - 도메인 이슈 도출을 통한 개선 방향을 토대로 목표 수준 정리

### 04. 분석 프로젝트 관리

1) 분석 프로젝트 : 과제 형태로 도출된 분석 기회를 프로젝트화하여 그 가치를 증명하기 위한 수단

- 특징
    - 데이터 영역과 비즈니스 영역에 대한 이해와 더불어 지속적인 반복이 요구되는 분석 프로세스의 특성을 이해하여 프로젝트 관리방안 수립
    - 지속적인 개선 및 변경을 염두에 두고 프로젝트 기한 내에 가능한 최선의 결과를 도출할 수 있도록 프로젝트 구성원들과 협업 필요
- 추가적 속성 : 데이터를 다루면서 분석 모형을 생성하는 프로젝트 특성상 아래 표의 추가적인 중점 관리 영역 고려
    - 데이터 크기 : 데이터가 지속적으로 생성되어 증가하는 점 고려
    - 데이터 복잡도
        - 정형, 비정형 데이터와 다양한 시스템에 산재되어 있는 원천 데이터들을 통합하는 진행 필요
        - 데이터에 잘 적용될 수 있는 분석 모형의 선정 등을 사전에 고려
    - 속도
        - 분석 결과가 도출되어 이를 활용하는 시나리오 측면에서의 속도까지 고려
        - 프로젝트 수행 시 분석 모형의 성능과 속도를 고려한 개발과 테스트 수행 고려
    - 분석 모형의 복잡도
        - 분석 모형의 정확도와 복잡도는 Trade off 관계
        - 분석 모형이 복잡할수록 정확도는 상승하지만 해석이 어려워지므로 이에 대한 기준을 정의하고 최적 모형을 탐색
    - 정확도와 정밀도
        - 분석 결과를 활용하는 측면에서는 Accuracy가 중요
        - 분석 모형의 안정성 측면에서는 Precision이 중요
        - Accuracy와 Precision은 Trade off 경우가 많음
- 분석가의 역할 : 분석가는 데이터 영역과 비즈니스 영역의 중간에서 현황을 이해하고 분석 모형을 통한 조율을 수행하는 조정자의 역할과 분석 프로젝트 관리 역할 수행

2) 분석 프로젝트 관리

- 효율적인 데이터 분석 수행을 위한 필요성 : 범위, 일정, 품질, 이슈 및 리스크, 의사소통 등 영역별로 고려해야 하는 요소가 많아 체계적 관리가 필요
- 분석 프로젝트의 관리 방안
    - 분석 프로젝트는 데이터 분석이 갖는 기본 특성(5V)을 살려 프로젝트 관리 지침을 만들어 기본 가이드로 활용
    - 프로젝트 관리 영역에 대한 주요한 사항들은 체크포인트 형태로 관리되어야 함

3) 분석 프로젝트의 영역별 주요 관리 항목

- 범위 관리
    - 분석 기획 단계에서 명시한 프로젝트의 범위는 분석을 수행하면서 데이터의 형태와 양 또는 적용되는 모형의 알고리즘에 따라 빈번하게 변경되곤 하며 이것은 프로젝트를 지연시키는 중대한 사유가 됨
    - 분석의 최종 결과물이 분석 보고서 형태인지 시스템인지에 따라 투입되는 자원과 범위가 크게 달라지므로 사전에 충분히 고려
- 일정 관리
    - 분석 프로젝트는 초기에 의도했던 모형이나 결과가 쉽게 나오는 경우가 흔치 않으며, 지속적으로 반복하는 과정에서 많은 시간 소모
    - 분석 결과의 품질을 보장한다는 전제하에 Time Boxing 기법으로 일정을 관리하는 것이 필요
- 원가 관리
    - 외부 데이터를 활용하여 데이터 분석을 수행하는 경우 데이터 구입 및 수집을 위해 많은 비용이 소모될 수 있으므로 사전에 충분한 조사 필요
    - 프로젝트를 수행하는 과정에서 목표한 결과를 달성하기 위해 오픈 소스 도구를 사용하지 않고 고가의 사용 도구를 사용하게 될 경우 비용 증가
- 품질 관리
    - 분석 프로젝트의 수행 결과에 대한 품질목표를 사전협의를 통해 수립하고 통제 하여야 함
    - 프로젝트 품질은 품질관리계획과 품질통제 및 품질보증으로 구성되어 있으며, 이를 잘 나누어 수행하여야 함
- 통합 관리 : 프로젝트 관리 프로세스들을 통합적으로 운영될 수 있도록 관리하여야 함
- 조달 관리
    - 상황에 따라 분석 프로젝트 목적에 적합한 범위 내에서 외부에 아웃소싱을 수행
    - PoC와 같이 지속성이 보장되지 않은 프로젝트는 인프라 구매보다 클라우드와 같은 대여방식을 고려해 볼 필요가 있음
- 인적자원 관리
    - 분석 프로젝트는 인적자원과 데이터가 핵심이므로, 프로젝트 수행 전 전문인력 확보와 고용유지 방안을 검토하여야 함
    - 전문인력의 효율적인 운영을 위해 핵심인재의 전문분야와 보유역량 및 수준 등을 관리하고, 프로젝트별 투입 시점과 피로도 등을 종합적으로 관리
- 위험 관리 : 분석 프로젝트 진행 과정에서 발생할 수 있는 위험을 식별하고, 위험을 분석하여 대응방안 수립
- 의사소통 관리 : 프로젝트의 원활한 진행을 위한 다양한 의사소통 채널과 모든 이해관계자가 분석 결과를 공유할 수 있도록 시각화와 같은 방안 마련
- 이해관계자 관리 : 분석 프로젝트에 영향을 미치는 이해관계자와 참여하는 데이터, 분석, 비즈니스, 시스템 등의 전문가들을 잘 관리하여야 함