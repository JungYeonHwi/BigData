# 데이터 분석 계획

# 목차
## [SECTION 01. 분석 방안 수립](#section-01-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B0%9C%EC%9A%94-%EB%B0%8F-%ED%99%9C%EC%9A%A9)
[01. 데이터와 정보](#%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D-%EA%B8%B0%ED%9A%8D)
[02. 데이터베이스](#02-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4)
[03. 빅데이터 개요](#03-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B0%9C%EC%9A%94)
[04. 빅데이터의 가치](#04-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9D%98-%EA%B0%80%EC%B9%98)
[05. 데이터 산업의 이해](#05-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%82%B0%EC%97%85%EC%9D%98-%EC%9D%B4%ED%95%B4)
## [SECTION 02. 빅데이터 기술 및 제도](#section-02-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B8%B0%EC%88%A0-%EB%B0%8F-%EC%A0%9C%EB%8F%84)
[01. 빅데이터 플랫폼](#01-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%94%8C%EB%9E%AB%ED%8F%BC)
[02. 빅데이터 처리기술](#02-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%B2%98%EB%A6%AC%EA%B8%B0%EC%88%A0)
[03. 빅데이터와 인공지능](#03-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%99%80-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5)
[04. 개인정보 개요](#04-%EA%B0%9C%EC%9D%B8%EC%A0%95%EB%B3%B4-%EA%B0%9C%EC%9A%94)
[05. 개인정보 법 · 제도](#05-%EA%B0%9C%EC%9D%B8%EC%A0%95%EB%B3%B4-%EB%B2%95--%EC%A0%9C%EB%8F%84)
[06. 개인정보 비식별화](#06-%EA%B0%9C%EC%9D%B8%EC%A0%95%EB%B3%B4-%EB%B9%84%EC%8B%9D%EB%B3%84%ED%99%94)
[07. 개인정보 활용](#07-%EA%B0%9C%EC%9D%B8%EC%A0%95%EB%B3%B4-%ED%99%9C%EC%9A%A9)

## SECTION 01. 분석 방안 수립

### 01. 데이터 분석

1) 데이터 분석 : 대용량의 데이터 집합으로부터 유용한 정보를 찾고 결과를 예측하기 위해 목적에 따라 분석기술과 방법론을 기반으로 정형·비정형 대용량 데이터를 구축, 탐색, 분석하고 시각화를 수행하는 업무

2) 데이터 분석의 현황

- 대다수의 기업들은 빅데이터가 갖고 있는 무한한 비즈니스 잠재력을 규명하는 초기 프로젝트에 머무르고 있음
- 빅데이터를 활용하기 위한 장애물은 비용보다 베디어 분석을 수행하기 위한 분석적 방법과 성관에 대한 이해의 부족

3) 데이터 분석의 지향점

- 전략적 통찰이 없는 데이터 분석 배제
    - 단순하게 데이터 분석을 자주, 많이 수행하는 것이 경쟁우위를 가져다주는 것은 아님
    - 분석은 경쟁의 본질에 영향을 미치고 기업의 경쟁전략을 이끌어 가므로, 경쟁의 본질을 제대로 바라보지 못한 분석은 불필요한 결과를 만들어 냄
- 일차원적인 데이터 분석 지양 : 대부분의 기업들은 업계 내부의 문제에만 중점을 두고 있으며, 주로 부서 단위로 관리되기에 전체 비즈니스 관점의 핵심적인 영향을 기대하기 어려움
- 전략 도출을 위한 가치 기반 데이터 분석 지향
    - 사업 성과를 견인하는 요소들과 차별화를 꾀할 기회에 대해 전략적 인사이트를 주는 가치 기반 분석 단계로 나아가야 함
    - 사업과 관련 트렌드에 대한 청사진을 그리고, 인구통계학적 변화나 사회경제적 트렌드 및 고객 니즈의 변화 등을 고려하여 분석을 수행

4) 데이터 분석에 대한 회의록

- 솔루션을 도입한 후 어떻게 활용하여 가치를 창출할 수 있을지 다시 또 과제를 수행해야 하는 상황이 반복되어 고가의 솔루션을 방치
- 현재 소개되고 있는 빅데이터 분석 성공사례들의 대다수가 기존 데이터 분석 프로젝트를 재포장한 경우

5) 데이터 분석 시 고려 사항

- 데이터 분석은 규모가 아니라 어떤 시각과 통찰을 얻을 수 있는가의 문제
- 전략과 비즈니스의 핵심 가치에 집중하고 관련된 분석 평가지표를 개발하여 시장과 고객 변화에 효과적으로 대응하는 것이 중요

### 02. 데이터 분석 기획

1) 데이터 분석 기획

- 분석 기획은 실제 분석을 수행하기에 앞서 분석을 수행할 과제의 정의 및 의도했던 결과를 도출할 수 있도록 이를 적절하게 관리할 수 있는 방안을 사전에 계획하는 작업
- 어떠한 목표를 달성하기 위해 어떠한 데이터를 가지고 어떤 방식으로 수행할 것인가에 대한 일련의 계획 수립

2) 분석 기획의 특징

- 분석 대상과 방법에 따른 분류 : 분석 주체와 방법에 대한 특성상 4가지 유형을 넘나들며 분석을 하고 결과를 도출하는 과정 반복
- 목표 시점에 따른 분류
    - 단기적 접근 방식 (과제 중심적 접근)
        - 당연한 과제를 빠르게 해결하기 위한 목적
        - 명확한 해결을 위해 Quick-Win 방식으로 분석
    - 중장기적 접근 방식 (마스터 플랜 접근)
        - 지속적인 분석 문화를 내재화하기 위한 목적
        - 전사적으로 장기적 관점에서 과제를 도출하여 수행
    - 혼합 방식 (분석 기획 시 적합)
        - 마스터 플랜을 수립하고 장기적 관점에서 접근하는 것이 바람직
        - 분석의 가치를 증명하고 이해관계자들의 동의를 얻기 위해 과제를 빠르게 해결하여 그 가치를 조기에 체험

3) 분석 기획 시 필요역령

- 분석 기획을 위한 기본적인 소망 : 분석 기획은 도메인 지식과 정보기술, 수학 및 통계학적 지식이라는 3가지 역량에 대한 균형 잡힌 시각을 갖고서 분석의 방향성과 계획을 수립하는 것
- 프로젝트 관리 역량과 리더십 : 분석 기획 시 기본적인 3가지 소양과 함께 프로젝트 관리 역량과 분석 프로젝트를 잘 이끌어 갈 리더십 중요

4) 분석 기획 시 고려사항

- 사용 가능한 데이터 확인
    - 데이터 확보 가능 여부, 데이터의 유형 등은 미리 확인
    - 데이터의 유형에 따라 적용 가능한 솔루션이나 분석 방법론이 달라짐
- 적합한 사례 탐색
    - 기존에 잘 구현되어 활용되고 있는 유사 분석 시나리오나 솔루션이 있다면 이를 최대한 활용하는 것이 유리
    - 분석 결과를 활용할 사용자의 측면에서 공감대를 얻을 수 있으며, 분석 수행이 원활하게 될 수 있도록 도와줌
- 분석 수행 시 발생 가능한 요소 고려
    - 분석 결과의 정확도를 높이기 위하여 기간과 투입 자원 증가가 불가피하며, 이로 인한 비용 상승을 충분히 고려
    - 분석 결과를 실제 환경에서도 성능에 문제없이 적용할 수 있도록 충분히 고려

### 03. 분석 마스터 플랜과 로드맵 설정

1) 분석 마스터 플랜 : 분석 관제를 수행함에 있어 그 과제의 목적이나 목표에 따라 전체적인 방향성을 제시하는 기본계획

- 분석 마스터 플랜 수립 절차
    - 분석 마스터 플랜 시 일반적인 정보전략계획 방법론을 활용할 수 있다. 다만 데이터 분석 기획의 특성을 고려하여 수행
    - 과제 도출 방법을 활용하여 데이터 분석 과제들을 빠짐없이 정의
    - 분석 과제의 중요도와 난이도 등을 고려하여 우선순위 결정
    - 단기와 중장기로 나누어 분석 로드맵을 수립
- 정보전략계획(ISP)
    - 정보기술 및 시스템을 전략적으로 활용하기 위한 중장기 마스터 플랜을 수립하는 절차
    - 조직 내·외부의 환경을 충분히 분석하여 새로운 기회나 문제점 도출
    - 사용자의 요구사항을 확인하여 시스템 구축 우선순위를 결정

2) 분석 과제 우선순위 평가기준

- IT 프로젝트의 과제 우선순위 평가기준 : 전략적 중요도, 실행 용이성 등 기업에서 고려하는 중요 가치 기준에 따라 다양한 관점으로 과제 우선수누이 기준을 정의하여 평가

|평가관점|평가요소|내용|
|---|---|---|
|전략적 중요도|전략적 필요성|비즈니스 목표나 업무에 얼마나 밀접하게 연관되어 있는지 측정|
|전략적 중요도|시급성|사용자 요구사항 반영이나 업무능률을 향상시키기 위해 얼마나 시급하게 수행되어야 하는지 측정|
|실행 용이성|투자 용이성|과제를 수행하는 데 필요한 비용이나 투자예산의 확보 가능성 정도를 측정|
|실행 용이성|기술 용이성|과제에 적용할 기술의 안정성 검증 정도와 유지보수가 용이한지 측정|

- 데이터 분석 프로젝트의 우선순위 평가기준 : 기존 IT 프로젝트와는 다른 기준으로 우선순위 평가 기준을 정의하여야 하며, 과제를 수행하고자 하는 기업이 처한 상황에 따라 그 기준이 달라질 수 있음

|ROI 요소|특징|내용|
|---|---|---|
|투자비용 요소|데이터 크기|데이터 규모, 데이터 양|
|투자비용 요소|데이터 형태|데이터 종류, 데이터 유형|
|투자비용 요소|데이터 속도|데이터 생산속도, 데이터 처리속도|
|비즈니스 효과|새로운 가치|분석 결과 활용을 통한 획득 가치, 비즈니스 실행을 통한 획득 가치|

- 분석 ROI 요소를 고려한 과제 우선순위 평가기준 : 조직의 상황에 따라 난이도 조율
     - 분석 준비도와 성숙도 진단 결과를 활용해 조직의 분석 수준 파악
     - 파악된 수준을 기초로 하여 적용 범위와 수행 방법별 난이도 조정

|평가관점|평가요소|내용|ROI 요소|
|---|---|---|---|
|시급성(중요)|전략적 중요도|목표가치|현재의 관점에 전략적 가치를 둘 것인지 판단, 중장기적 관점에 전략적인 가치를 둘 것인지 판단|비즈니스 효과|
|난이도|데이터 획득 비용, 데이터 가공 비용, 데이터 저장 비용, 분석 적용 비용, 분석 수준|비용과 범위 측면에서 적용하기 쉬운 과제인지 판단, 과제 범위를 PoC 또는 처음부터 크게 할 것인지 판단, 내부 데이터를 활용하고 외부 데이터까지 확대할지 판단|투자비용 요소|

3) 분석 과제 우선순위 선정 및 조정

- 포트폴리오 사분면 분석 기법 활용 : 난이도와 식브성을 기준으로 분석 과제 유형을 분류하여 4분면에 배치
- 매트릭스 내 분석 과제 우선순위 선정
    - 가장 우선적으로 분석 과제 적용이 필요한 영역은 3사분면
    - 우선순위가 낮은 영역은 2사분면
    - 적용 우선순위 기준을 시급성에 둘 경우 : 3->4->1->2 영역
    - 적용 우선순위 기준을 난이도에 둘 경우 : 3->1->4->2 영역
- 매트릭스 내 분석 과제 우선순위 조정
    - 시급성이 높고 난이도가 높은 1사분면은 의사결정을 통해 적용 우선순위 조정 가능
    - 데이터 양과 특성, 분석 범위 등에 따라 난이도를 조율하여 적용 우선순위를 조정 가능
- 분석 과제 우선순위 조정 시 고려사항
    - 기술적 요소에 따른 적용 우선순위 조정
        - 대용량 데이터 분석은 데이터 저장, 처리, 분석을 위한 새로운 기술 요소들로 인하여 운영중인 시스템에 영향을 줄 수 있음
        - 기존 시스템에 미치는 영향을 최소화하여 적용하거나 운영중인 시스템과 별도로 시행하여 난이도 조율을 통한 우선순위 조정 가능
    - 분석 범위에 따른 우선순위 조정
        - 분석 과제의 전체 범위를 한 번에 일괄적으로 적용하여 추진 가능
        - 분석 과제 중 일부만 PoC로 진행하고 평가 후에 범위를 확대 가능

4) 분석 로드맵 설정 : 분석 로드맵은 마스터 플랜에서 정의한 목표를 기반으로 분석 과제를 수행하기 위해 필요한 기준 등을 담아 만든 종합적인 계획

- 분석 로드맵 수립 절차
    - 최종적인 실행 우선순위를 결정하여 단계적 구현 로드맵 수립
    - 단계별로 추진하고자 하는 목표를 명확하게 저으이
    - 추진 과제별 선행 관계를 고려하여 단계별 추진 내용 정렬
- 세부적인 일정계획 수립
    - 반복적인 정련과정을 통해 프로젝트의 완성도를 높여 나감
    - 데이터 수집 및 확보와 분석 데이터 준비 단계는 순차적으로 진행하고 모델링 단계는 반복적으로 수행
    - 주로 순차형과 반복형을 혼합하여 사용

### 04. 분석 문제 정의

1) 분석 문제 정의 개요

- 분석 과제 도출 : 분석 과제는 해결해야 할 다양한 문제들을 데이터 분석 문제로 변환하여 분석 프로젝트로 수행할 수 있는 과제정의서 형태로 도출
- 대표적인 분석 과제 도출 방법 : 문제가 먼저 주어지고 이에 대한 해법을 찾아가는 하향식 접근 방식과 데이터를 기반으로 문제의 재정의 및 해결방안을 탐색하는 상향식 접근 방식
- 최적인 의사결정을 위한 혼합방식
    - 동적인 환경에서 발산과 수렴 단계를 반복적으로 수행하며 상호 보완을 통해 분석의 가치를 극대화
    - 상향식 접근 방식의 발산 단계 : 가능한 옵션을 도출
    - 하향식 접근 방식의 수렴 단계 : 도출된 옵션을 분석하고 검증
- 분석 과제 정의
    - 분석 과제 정의서는 다양한 방식으로 도출한 분석 과제들을 명확하게 정의하여 상세하게 작성
        - 필요한 데이터, 데이터 수집과 분석 난이도, 분석 방법과 수행 주기, 상세 분석 과정, 분석 결과에 대한 검증 책임자 등을 포함
        - 분석 데이터는 조직 내부뿐만 아니라 외부 데이터도 포함, 데이터 유형이나 종류를 가리지 않고 범위를 확장하여 고려
    - 분석 과제 정의서는 분석 프로젝트를 수행하는 이해관계자가 프로젝트의 방향을 설정하고 성공 여부를 판단할 수 있는 자료로 사용 : 분석 과제 정의서는 향후 프로젝트 수행계획의 입력물로 사용

2) 하향식 접근 방식 : 문제가 주어지고 이에 대한 해법을 찾기 위하여 각 과정이 체계적으로 단계화되어 수행하는 방식

- 문제 탐색 단계
    - 개별적으로 인지하고 있는 문제를 단순하게 나열하는 것보다 전체적인 관점의 기준 모델을 활용하여 누락 없이 문제를 도출하고 식별
    - 현재 데이터의 소유 여부와 데이터가 없는 경우 해결방안 등에 대한 세부적인 내용보다 문제를 해결하여 발생하는 가치에 중심을 두어야 함
        - 비즈니스 모델 기반 문제 탐색
            - 해당 기업이 사업 모델을 도식화한 비즈니스 모델 캔버스의 블록을 단순화하여 기회를 추가로 도출
            - 새로운 관점의 접근으로 새로운 유형의 분석 기회와 주체를 발굴
        - 외부 참조 모델 기반 문제 탐색
            - 유사 또는 동종의 환경에서 기존에 수행한 분석 사례와 벤치마킹
            - 제공되는 산업별, 업무 서비스별 분석테마 후보 그룹을 통해 빠르고 쉬운 방식으로 접근
        - 분석 유즈케이스 정의
            - 도출한 분석 기회들을 구체적인 과제로 만들기 전에 분석 유즈케이스로 정의하여야 함
            - 문제에 대한 상세 설명과 기대효과를 명시하여 향후 데이터 분석 문제로의 전환 및 적합성 평가에 활용
- 문제 정의 단계
    - 식별된 비즈니스 문제를 데이터적인 문제로 변환하여 정의
    - 필요한 데이터와 기법을 정의하기 위한 데이터 분석 문제로 변환
    - 분석 수행자 외 문제 해결 시 효율을 얻을 최종 사용자 관점에서 정의
- 해결방안 탐색 단계
    - 정의된 데이터 분석 문제를 해결하기 위한 다양한 방안들 모색
    - 기법 및 시스템과 분석 역량 보유 여부에 따라 세분화 가능
- 타당성 평가 단계 : 도출된 분석 문제, 가설에 대한 대안을 과제화하기 위한 타당성 분석
    - 경제적 타당성
        - 비용 대비 편익 분석 관점의 접근 필요
        - 항목은 데이터, 시스템, 인력, 유지보수 등 분석 비용
        - 편익은 분석 결과를 적용하여 발생 가능한 실질적 비용 절감, 추가적 매출과 수익 등의 경제적 가치로 산출
    - 데이터 및 기술적 타당성
        - 데이터 분석 시 데이터, 분석 시스템 환경 분석역량 필요
        - 기술적 타당성 분석 시 역량 확보 방안의 사전수립 필요
        - 비즈니스 분석가, 데이터 분석가, 시스템 엔지니어 등과 협업

3) 하향식 접근 방식의 문제 탐색 방법

- 비즈니스 모델 캔버스를 활용한 과제 발굴 : 해당 기업의 사업 모델을 도식화한 비즈니스 모델 캔버스의 9가지 블록을 단순화하여 문제 발굴을 3개의 단위로, 이를 관리하는 2개의 영역으로 도출
    - 업무 단위 : 제품이나 서비스를 생산하기 위한 내부 프로세스 및 주요 자원과 관련하여 주제 도출
    - 제품 단위 : 생산 및 제공하는 제품이나 서비스를 개선하기 위한 관련 주제들 도출
    - 고객 단위 : 제품이나 서비스를 제공받는 사용자나 고객 도는 이를 제공하는 채널 관점에서 관련 주제들 도출
    - 규제와 감사 영역 : 제품 생산과 전달 과정에서 발생하는 규제나 보안 관점에서 관련 주제들 도출
    - 지원 인프라 영역 : 분석을 수행하는 시스템 영역과 이를 운영 및 관리하는 인력의 관점에서 관련 주제들 도출
- 분석 기회 발굴의 범위 확장 : 새로운 문제의 발굴이나 장기적인 접근을 위해서는 환경과 경쟁 구도의 변화 및 역량의 재해석을 통한 혁신 관점의 분석 기회 추가 도출 필요

|관점|영역|내용|
|---|---|---|
|거시적 관점|사회 영역|고객영역을 확장하여 전체 시장을 사회, 문화, 구조적 트렌드 변화에 기반하여 분석 기회 도출|
|거시적 관점|기술 영역|과학, 기술, 의학 등 최신 기술의 등장변화에 따른 역량 내재화와 제품 및 서비스 개발에 대한 분석 기회 도출|
|거시적 관점|경제 영역|산업과 금융 전반의 변동성과 경제 구조 변화 동향에 따른 시장 흐름을 파악하고 이에 대한 분석 기회 도출|
|거시적 관점|환겨 영역|환경과 관련된 정부, 사회단체, 시민사회의 관심과 규제 동향을 파악하고 이에 대한 분석 기회 도출|
|거시적 관점|정치 영역|주요 정책방향, 정세, 지정학적 동향 등의 거시적인 흐름을 토대로 하여 분석 기회 도출|
|경쟁자 확대 관점|대체재 영역|현재 생산하고 있는 제품 또는 서비스의 대체재를 파악하고 이를 고려한 분석 기회 도출|
|경쟁자 확대 관점|경쟁자 영역|현재 생산하고 있는 제품이나 서비스의 주요 경쟁자에 대한 동향을 파악하여 이를 고려한 분석 기회 도출|
|경쟁자 확대 관점|신규 진입자 영역|향후 시장에서 파괴적인 역할을 수행할 수 있는 신규 진입자에 대한 동향을 파악하여 이를 고려한 분석 기회 도출|
|시장의 니즈 탐색 관점|고객 영역|고객의 구매 동향 및 고객의 컨텍스트를 더욱 깊게 이해하여 제품 또는 서비스의 개선에 필요한 분석 기회 도출|
|시장의 니즈 탐색 관점|채널 영역|자체 채널뿐만 아니라 최종 고객에게 상품이나 서비스를 전달 가능한 모든 경로를 파악하여 경로별 채널 분석 기회를 확대하여 탐색|
|시장의 니즈 탐색 관점|영향자들 영역|주주, 투자자, 협의 및 기타 이해관계자의 주요 관심사항에 대하여 파악하고 분석 기회 탐색|
|역량의 재해석 관점|내부 역량 영역|지식, 기술, 스킬 등 노하우와 인프라적인 유형 자산에 대해서도 폭넓게 재해석하고 해당 영역에서 분석 기회 탐색|
|역량의 재해석 관점|파트너와 네트워크 영역|관계사와 공급사 등의 역량을 활용해 수행할 수 있는 기능을 파악해 보고 이에 대한 분석 기회를 추가적으로 도출|

4) 상향식 접근 방식 : 문제의 정의 자체가 어려운 경우 데이터를 기반으로 문제의 재정의 및 해결방안을 탐색하고 이를 지속적으록 개선하는 방식

- 상향식 접근 방식의 특징
    - 다양한 데이터 분석을 통해 왜 그러한 일이 발생하는지 역으로 추적하면서 문제를 도출하거나 재정의할 수 있는 방식
    - 데이터를 활용하여 생각지도 못했던 인사이트 도출 미 시행착오를 통한 개선 가능
- 상향식 접근 방식의 등장 배경
    - 기존 하향식 접근 방식의 한계를 극복하기 위해 등장
        - 하향식 접근 방식은 솔루션 도출은 유효하지만 새로운 문제 탐색을 어려움
        - 하향식 접근 방식은 복잡하고 다양한 환경에서 발생한 문제에는 부적합
    - 논리적 단계별 접근법은 문제의 구조가 분명하고 이에 대한 해결책을 도출하기 위한 데이터가 분석가나 의사결정자에게 주어져 있음을 가정하고 있음
- 상향식 접근기반 전통적 분석 사고 극복방안
    - 디자인 사고 접근법
        - 현장 관찰과 감정이입, 대상 관점으로의 전환 수행
        - 통상적으로는 분석적으로 사물을 인식하려는 Why를 강조하거나, 답을 미리 내는 것이 아니라 사물을 있는 그대로 인식하려는 What 관점으로 접근
        - 객관적으로 존재하는 데이터 자체를 관찰하고 실제 행동을 옮김으로써 대상을 좀 더 잘 이해하는 방식으로 접근
    - 비지도학습 방법에 의한 수행
        - 목표값을 사전에 학습하거나 정의하지 않고 데이터 자체만을 가지고 결과물 도출
        - 새로운 유형의 인사이트를 도출하기에 유용한 방식
    - 빅데이터 환경에서의 분석
        - 통계적 분석환경에서는 인과관계 분석을 위해 가설을 설정하고 이를 검증하기 위해 모집으로부터 표본을 추출하여 가설검증
        - 빅데이터 분석환경에서는 인과관계, 상관관계, 연관분석을 통하여 다양한 문제 해결 가능
- 상향식 접근 방식의 문제 해결 방법
    - 프로토타이핑 접근법 : 일단 먼저 분석을 시도해 보고 그 결과를 확인하면서 반복적으로 개선해 나가는 방식
        - 사용자가 요구사항이나 데이터를 정확히 정의하기 어렵고 원천 데이터라도 명확하지 않을 때 주로 사용
        - 완전하지는 않지만 신속하게 해결책이나 모형을 제시하여 이를 바탕으로 문제를 더 명확하게 인식하고 필요한 데이터를 식별하여 구체화

### 05. 데이터 분석 방안

1) 분석 방법론

- 분석 방법론
    - 데이터 분석 시 품질확보를 위하여 단계별로 수행해야 하는 활동, 작업, 산출물 정의
    - 프로젝트는 한 개인의 역량이나 조직의 우연한 성공에 의해서는 안 되고 일정 품질 수준 이상의 산출물과 프로젝트의 성공 가능성 제시해야 함
- 분석 방법론의 구성요건
    - 상세한 절차
    - 방법
    - 도구와 기법
    - 템플릿과 산출물
    - 어느 정도의 지식만 있으면 활용 가능한 수준의 난이도
- 분석 방법론의 생산과정(선순환 과정)
    - 형식화
        - 개인의 암묵지가 조직의 형식지로 발전
        - 분석가의 경험을 바탕으로 정리하여 문서화
    - 체계화
        - 문서화한 최적화된 형식지로 전개됨으로써 방법론 생성됨
        - 문서에는 절차나 활동 및 작업, 산출물, 도구 등을 정의
    - 내재화
        - 개인에게 전파되고 활용되어 암묵지로 발전됨
        - 전파된 방법론을 학습하고 활용하여 내재화

2) 계층적 프로세스 모델 구성 : 분석 방법론은 일반적으로 계층적 프로세스 모델 형태로 구성 가능하며, 단계, 태스크, 스텝 3계층으로 구성됨

- 최상위 계층(단계)
    - 프로세스 그룹을 통하여 완성된 단계별 산출물 생성
    - 각 단계는 기준선으로 설정되어 관리되어야 하며 버전관리 등을 통하여 통제
- 중간 계층 (태스크)
    - 각 태스크는 단계를 구성하는 단위 활동
    - 물리적 또는 논리적 단위로 품질검토 가능
- 최하위 계층 (스텝)
    - WBS의 워크패키지
    - 입력자료, 처리 및 도구, 출력자료로 구성된 단위 프로세스

3) 소프트웨어개발생명주기 활용

- 분석 방법론은 소프트웨어 공학의 소프트웨어개발생명주기를 활용하여 구성 : 소프트웨어개발생명주기는 소프트웨어에 대해 요구분석과 설계, 구현과정을 거쳐 설치, 운영과 유지보수, 그리고 폐기할 때까지의 전 과 정을 가시적으로 표현
    - 계획(요구명세)
        - 고객의 요구사항 명세
        - 타당성 조사 및 소프트웨어의 기능과 제약조건 정의하는 명세서 작성
        - 요구사항은 일반적으로 모호하고 불완전하며 모순
    - 요구분석 : 대상이 되는 문제 영역과 사용자가 원하는 Task 이해
    - 설계 : 분석모형을 가지고 이를 세분화함으로써 구현될 수 있는 형태로 전환
    - 구현 : 실행 가능한 코드 생성
    - 시험 : 발생 가능한 실행 프로그램의 오류 발견, 수정
    - 유지보수 : 인수가 완료된 후 일어나는 모든 개발 활동
- 폭포수 모형 : 고전적 Life Cycle Paradigm으로 분석, 설계, 개발, 구현, 시험 및 유지보수 과정을 순차적으로 접근하는 방법
    - 소프트웨어 개발을 단계적, 순차적, 체계적 접근 방식으로 수행
    - 개념 정립에서 구현가지 하향식 접근 방법 사용
    - 전 단계의 산출물은 다음 단계의 기초
- 프로토타입 모형 : 사용자의 요구사항을 충분히 분석할 목적으로 시스템의 일부분을 일시적으로 간략히 구현한 다음 다시 요구사항을 반영하는 과정을 반복하는 개발모형
    - 실험적 프로토타입 : 요구분석의 어려움을 해결하기 위해 실제 개발될 소프트웨어의 일부분을 직접 개발함으로써 의사소통의 도구로 활용
    - 진화적 프로토타입 : 프로토타입을 요구분석의 도구로만 활용하는 것이 아니라, 이미 개발된 프로토타입을 지속적으로 발전시켜 최종 소프트웨어로 발전
    - 요구 분석의 어려움 해결을 통해 사용자의 참여 유도
    - 요구사항 도출과 이해에 있어 사용자와의 커뮤니케이션 수단으로 활용 가능
    - 사용자 자신이 원하는 것이 무엇인지 구체적으로 잘 모르는 경우 간단한 시제품으로 개발 가능
    - 개발 타당성을 검토하는 수단으로 활용 가능
- 나선형 모형 : 시스템을 개발하면서 생기는 위험을 최소화하기 위해 나선을 돌면서 점진적으로 완벽한 시스템으로 개발하는 모형
    - 프로젝트의 완전성 및 위험 감소와 유지보수 용이
    - 관리가 중요하나 매우 어렵고 개발시간이 장기화될 가능성이 있음
- 반복적 모형 : 사용자의 요구사항 일부분 혹은 제품의 일부분을 반복적으로 개발하여 최종 시스템으로 완성하는 모형
    - 증분형 모형 : 사용자 요구사항 제품의 일부분을 반복적으로 개발하며 대상범위를 확대해 나아가서 최종제품을 완성하는 방법
    - 진화형 모형 : 시스템이 가지는 여러 구성요소의 핵심부분을 개발한 후 각 구성요소를 지속적으로 발전시켜 나가는 방법

|유형|설명|장점|단점|
|---|---|---|---|
|폭포수 모형|검토 및 승인을 거쳐 순차적, 하향식으로 개발 진행|이해하기 쉽고 관리 용이, 다음 단계 진행 전에 결과 검증|요구사항 도출 어려움, 설계 및 코딩과 테스트 지연, 문제점 발견이 늦어짐|
|프로토타입 모형|시스템의 핵심적인 기능을 먼저 만들어 평가한 후 구현|요구사항 도출과 시스템 이해 용이, 의사소통 향상|사용자의 오해(완제품)가 발생 쉬움, 폐기되는 프로토타입 존재|
|나선형 모형|폭포수 모형과 프로토타입의 모형의 장점에 위험분석 추가|점증적으로 개발 시 실패 위험을 감소시킬 수 있음, 테스트가 용이하고 피드백이 있음|관리가 복잡|
|반복적 모형|시스템을 여러 번 나누어 릴리즈|

- 소프트웨어게빌생명주기 모형 선정 기준
    - 프로젝트의 규모와 성격
    - 개발에 사용되는 방법과 도구
    - 개발에 소요되는 시간과 비용
    - 개발과정에서의 통제수단과 소프트웨어 산출물 인도 방식

4) KDD 분석 방법론 : 통계적인 패턴이나 지식을 탐색하는 데 활용할 수 있도록 체계적으로 정리한 프로파일링 기술 기반의 데이터 마이닝 프로세스

- KDD 분석 방법론의 9가지 프로세스
    - 1. 분석 대상 비즈니스 도메인의 이해
    - 2. 분석 대상 데이터셋 선택과 생성
    - 3. 데이터에 포함되어 있는 잠음과 이상값 등을 제거하는 정제작업이나 선처리
    - 4. 분석 목적에 맞는 변수를 찾고 필요시 데이터의 차원을 축소하는 데이터 변경
    - 5. 분석 목적에 맞는 데이터 마이닝 기법 선택
    - 6. 분석 목적에 맞는 데이터 마이닝 알고리즘 선택
    - 7. 데이터 마이닝 시행
    - 8. 데이터 마이닝 결과에 대한 해석
    - 9. 데이터 마이닝에서 발견된 지식 활용

- KDD 분석 방법론의 분석 절차 : 데이터 분석은 데이터셋 선택, 데이터 전처리, 데이터 변환, 데이터 마이닝, 데이터 마이닝 결과 평가 총 5단계에 걸쳐 진행
    - 데이터셋 선택
        - 분석대상 비즈니스 도메인에 대한 이해 및 프로젝트 목표의 정확한 설정 선행
        - 데이터베이스 또는 원시 데이터에서 분석에 필요한 데이터를 선택
        - 필요시에 목표 데이터를 추가적으로 구성하여 활용
    - 데이터 전처리
        - 잡음과 이상값, 결측치를 식별하고 필요시 제거하거나 대체
        - 데이터가 추가적으로 필요한 경우 데이터셋 선택 절차부터 다시 실행
    - 데이터 변환
        - 분석 목적에 맞는 변수를 선택하거나 데이터의 차원 축소 등 수행
        - 학습용 데이터와 검증용 데이터로 데이터 분리
    - 데이터 마이닝
        - 분석 목적에 맞는 데이터 마이닝 기법 및 알고리즘 선택하여 분석 수행
        - 필요시 데이터 전처리와 데이터 변환 절차를 추가로 실행하여 데이터 분석 결과의 품질을 높일 수 있음
    - 데이터 마이닝 결과 평가
        - 분석 결과에 대한 해석과 평가 및 분석 목적과의 일치성 확인
        - 발견된 지식을 업무에 활용하기 위한 방안 모색
        - 필요한 경우 데이터셋 선택부터 데이터 마이닝 절차가지 반복하여 수행

5) CRISP-DM 분석 방법론 : 계층적 프로세스 모델로써 4계층으로 구성된 데이터 마이닝 프로세스

## SECTION 02. 빅데이터 기술 및 제도

### 01. 빅데이터 플랫폼

1) 정의 : 빅데이터 수집부터 저장, 처리, 분석 등 전 과정을 통합적으로 제공하여 그 기술들을 잘 사용할 수 있도록 준비한 환경

2) 빅데이터 플랫폼의 등장 배경

- 비즈니스 요구사항 변화
- 데이터 규모의 처리 복잡도 증가
- 데이터 구조의 변화와 신속성 요구
- 데이터 분석 유연성 증대

2) 빅데이터 플랫폼의 기능

- 컴퓨팅 부하 발생
- 저장 부하 발생
- 네트워크 부하 발생

3) 빅데이터 플랫폼의 조건 : 서비스 사용자와 제공자 어느 한쪽에 치우쳐서는 안 되며 모두가 만족할 수 잇는 환경을 제공해야 함

### 02. 빅데이터 처리기술

1) 빅데이터 처리과정과 요소기술

- 생성
    - 데이터베이스나 파일 관리 시스템과 같은 내부 데이터가 존재
    - 인터넷으로 연결된 외부로부터 생성된 파일이나 데이터 존재
- 수집
    - 크롤링을 통해 데이터 원천으로부터 데이터 검색하여 수정
    - ETL을 통해 소스 데이터로부터 추출, 변환, 적재
    - 단순한 수집이 아니라 검색 및 수집, 변환 과정 모두 포함
    - 로그 수집기나, 센서 네트워크 및 Open API 등을 활용
- 저장(공유)
    - 저렴한 비용으로 데이터를 쉽고 빠르게 많이 저장
    - 정형 데이터뿐만 아니라 반정형, 비정형 데이터도 포함
    - 병렬 DBMS나 하둡, NoSQL 등 다양한 기술 사용
    - 시스템 간의 데이터를 서로 공유
- 처리
    - 데이터를 효과적으로 처리하는 기술이 필요한 단계
    - 분산 병렬 및 인 메모리 방식으로 실시간 처리
    - 대표적으로 학둡의 맵리듀스를 활용
- 분석
    - 데이터를 신속하고 정확하게 분석하여 비즈니스에 기여
    - 특정 분야 및 목적의 특성에 맞는 분석 기법 선택이 중요
    - 통계분석, 데이터 마이닝, 텍스트 마이닝, 기계학습 방법 등
- 시각화
    - 처리 및 분석 결과를 표, 그래프 등을 이용해 쉽게 표현하고 탐색이나 해석에 활용
    - 정보 시각화 기술, 시각화 도구, 편집 기술, 실시간 자료 시각화 기술로 구성

2) 빅데이터 수집

- 크롤링 : 무수히 많은 컴퓨터에 분산 저장되어 있는 문서를 수집하여 검색 대상의 색인으로 포함시키는 기술
- 로그 수집기 : 조직 내부에 있는 웹 서버나 시스템의 로그를 수집하는 소프트웨어
- 센서 네트워크 : 유비쿼터스 컴퓨팅 구현을 위한 초경량 저전력의 많은 센서들로 구성된 유뮤선 네트워크
- RSS Reader/Open API : 데이터의 생산, 공유, 참여할 수 있는 환경인 웹 2.0을 구현하는 기술
- ETL 프로세스 : 데이터의 추출, 변환, 적재의 약어, 다양한 원천 데이터를 취합해 추출하고 공통된 형식으로 변환하여 적재하는 과정
    - 데이터 추출 : 원천 데이터로부터 적재하고자 하는 데이터 추출
    - 데이터 변환
        - 추출한 데이터를 변환하고 균질화하여 정제
        - 정제한 데이터를 적재하고자 하는 데이터 웨어하우스 구조에 맞게 변환
        - 통합하는 제약 조건 및 비즈니스 규칙에 따라 필터링이나 확인 작업
    - 데이터 적재 : 변환된 데이터를 데이터 웨어하우스에 적재

3) 빅데이터 저장

- NoSQL(Not-only SQL)
    - 전통적인 관계형 데이터베이스와는 다르게 데이터 모델을 단순화하여 설계된 비관계형 데이터베이스로 SQL을 사용하지 않는 DBMS와 데이터 저장장치
    - 기존의 RDBMS 트랜잭션 속성인 원자성, 일관성, 독립성, 지속성을 포기
    - 데이터 업데이트가 즉각적으로 가능한 데이터 저장소
    - Cloudata, Hbase, Cassandra, MongoDB 등
- 공유 데이터 시스템
    - 일관성, 가용성, 분할 내성 중에서 최대 두 개의 속성만 보유 (CAP 이론)
    - 분할 내성을 취하고 일관성과 가용성 중 하나를 포기하여 일관성과 가용성을 모두 취하는 기존 RDBMS보다 높은 성능과 확장성을 제공
- 병렬 데이터베이스 관리 시스템
    - 다수의 마이크로프로세서를 사용하여 여러 디스크에 질의, 갱신, 입출력 등 데이터베이스 처리를 동시에 수행하는 시스템
    - 확장성을 제공하기 위해 작은 단위의 동작으로 트랜잭션 적용 필요
    - VoltDB, SAP HANA, Vertica, Greenplum, Netezza가 대표적
- 분산 파일 시스템
    - 네트워크로 공유하는 여러 호스트의 파일에 접근할 수 있는 파일 시스템
    - 데이터를 분산하여 저장하면 데이터 추출 및 가공 시 빠르게 처리
    - GFS, HDFS, 아마존 S3 파일 시스템이 대표적
- 네트워크 저장 시스템
    - 이기종 데이터 저장 장치를 하나의 데이터 서버에 연결하여 총괄적으로 데이터를 저장 및 관리하는 시스템
    - SAN, NAS가 대표적

4) 빅데이터 처리

- 분산 시스템과 병렬 시스템
    - 분산 시스템
        - 네트워크상에 분산되어 있는 컴퓨터를 단일 시스템인 것처럼 구동하는 기술
        - 분산 시스템에 속한 각 노드는 독립된 시스템
        - 독립 컴퓨터의 집합으로 만들었으나 마치 단일 시스템인 것처럼 수행되어야 함
    - 병렬 시스템
        - 문제 해결을 위해 CPU 등의 자원을 데이터 버스나 지역 통신 시스템 등으로 연결하여 구동하는 기술
        - 분할된 작업을 동시에 처리하여 계산 속도를 빠르게 함
    - 용어는 구분되어 사용되기도 하지만 서로 중첩되는 부분이 많아 실제 시스템에서도 이 둘을 명확히 구분하기 어려움
    - 두 개념을 아우르는 분산 병령 컴퓨팅이라는 용어를 사용
- 분산 병렬 컴퓨팅 : 다수의 독립된 컴퓨팅 자원을 네트워크상에 연결하여 이를 제어하는 미들웨어를 이용해 하나의 시스템으로 동작하게 하는 기술
- 분산 병렬 컴퓨팅 시 고려사항
    - 전체 작업의 배분 문제 : 전체 작업을 잘 쪼개어 여러 개의 작은 작업으로 나눠야 함|
    - 각 프로세서에서 계산된 중간 결과물을 프로세서 간 주고받는 문제
        - 효율적인 통신은 성능과 직결
        - 보통 단일 시스템은 전체 작업을 노드의 수만큼 균등하게 나눔
        - 이중 시스템은 컴퓨팅 능력에 따라 전체 작업을 배분
        - 노드 간의 통신을 최소화하는 기법 등이 반영되면 자원을 좀 더 효율적으로 사용할 수 있어 성능 향상에 도움
    - 서로 다른 프로세서 간 동기화 문제
        - 데이터 병렬 처리에서 동기적 방법을 사용할 경우 프로세서는 특정 계산이 끝나거나 특정 데이터를 넘겨받을 때까지 반드시 대기
        - 동기적 방법의 경우 송신자는 수신자에게서 데이터를 받았다는 응답이 올 때까지 대기
        - 비동기적 방법에서는 결과 메시지를 보낸 즉시 다음 작업을 계속할 수 있음
        - 비동기적 방법의 경우 프로세서는 기다릴 필요가 없지만, 계산 과정이 적합한지는 확인

- 하둡
    - 분선 처리 환경에서 대용량 데이터 처리 및 분석을 지원하는 오픈 소스 소프트웨어 프레임워크
    - 야후에서 최초로 개발, 지금은 아파치 소프트웨어 재단에서 프로젝트로 관리
    - 하둡 분산파일시스템인 HDFS와 분산칼럼기반 데이터베이스인 Hbase, 분산 컴퓨팅 지원 프레임워크인 맵리듀스로 구성
    - 분산파일시스템을 통해 수 천대의 장비에 대용량 파일을 나우어 저장할 수 있는 기능을 제공 : 분산파일시스템에 저장된 대용량의 데이터들을 맵리듀스를 이용하여 실시간으로 처리 및 분석 가능
    - 하둡의 부족한 기능을 보완하는 하둡 에코시스템이 등장하여 다양한 솔류선을 제공
- 아파치 스파크
    - 실시간 분산형 컴퓨팅 플랫폼으로 In-Memory 방식으로 처리를 하며 하둡보다는 처리속도가 빠름
    - 스칼라 언어로 개발되었지만 스칼라뿐만 아니라 Java, R, Python을 지원
- 맵리듀스
    - 구글에서 개발한 방대한 양의 데이터를 신속하게 처리하는 프로그래밍 모델로 효과적인 병렬 및 분산 처리 지원
    - 런타임에서의 입력 데이터 분할, 작업 스케줄링, 노드 고장, 노드 간의 데이터 전송 작업이 맵리듀스 처리 성능에 많은 영향을 미침

5) 빅데이터 분석

- 데이터 분석 방법의 분류
    - 탐구 용인 분석(EFA) : 데이터 간 상호 관계를 파악하여 데이터를 분석하는 방법
    - 확인 요인 분석(CFA) : 관찰된 변수들의 집합 요소 구조를 파악하기 위한 통계적 기법을 통해 데이터를 분석하는 방법
- 데이터 분석 방법
    - 분류 : 미리 알려진 클래스들로 구분되는 학습 데이터셋을 학습시켜 새로 추가되는 데이터가 속할 만한 데이터 셋을 찾는 지도학습 방법
    - 군집화 : 특성이 비슷한 데이터를 하나의 그룹으로 분류하는 방법으로, 분류와 달리 학습 데이터셋을 이용하지 않는 비지도학습 방법
    - 인공지능
        - 분야에서 인간의 학습을 모델링한 방법
        - 의사결정트리 등 기호적 학습과 신경망이나 유전 알고리즘 등 비기호적 학습, 베이지안이나 은닉 마코프 등 확률적 학습 등 다양한 기법
    - 텍스트 마이닝
        - 자연어 처리 기술을 이용해 인간의 언어로 쓰인 비정형 텍스트에서 유용한 정보를 추출하거나 다른 데이터와의 연견솽을 파악하기 위한 방법
        - 분류나 군집화 등 빅데이터에 숨겨진 의미 있는 정보를 발견하는 데 사용|
    - 웹 마이닝 : 인터넷을 통해 수집한 정보를 데이터 마이닝 방법으로 분석하는 응용분야
    - 오피니언 마이닝 : 온라인의 다양한 뉴스와 소셜 미디어 코멘트 또는 사용자가 만든 콘텐츠에서 표현된 의견을 추출, 분류, 이해하는 응용분야
    - 리얼리티 마이닝
        - 휴대폰 등 기기를 사용하여 인간관계와 행동 양태 등을 추론하는 응용분야
        - 통화량, 통화 위치, 통화 상태, 통화 대상, 통화 내용 등을 분석하여 사용자의 인간관계나 행동 특성을 찾아냄
    - 소셜 네트워크 분석 : 수학의 그래프 이론을 바탕으로 소셜 네트워크 서비스에서 네트워크 연결 구조와 강도를 분석하여 사용자의 명성 및 영향력을 측정하는 방법
    - 감성 분석
        - 문장의 의미를 파악하여 글의 내용에 긍정 또는 부정, 좋음 또는 나쁨을 분류하거나 만족 또는 불만족 강도를 지수화하는 방법
        - 도출된 지수를 이용하여 고객의 감성 트렌드를 시계열로 분석하고, 고객의 감성 변화에 기업들이 신속하게 대응 및 부정적인 의견의 확산을 방지하는 데 활용|

### 03. 빅데이터와 인공지능

1) 인공지능 (AI)

- 인공지능의 정의
    - 인공지능은 기계를 지능화하는 노력이며, 지능화란 객체가 환경에서 적절히, 그리고 예지력을 갖고 작동하도록 하는 것
    - 인고지능은 합리적 행동 수행자, 어떤 행동이 최적의 결과를 낳을 수 있도록 하는 의사결정 능력을 갖춘 에이전트를 구축하는 것
    - 인공지능은 설정한 목표를 극대화하는 행동을 제시하는 의사결정 로직
- 딥러닝의 특징
    - 딥러닝은 제프리 힌튼의 노력으로 함수추정 방법으로써의 신경망 관점에서 정보를 압축, 가공, 재현하는 알고리즘으로 일반화하면서 인공지능의 핵심 동인
    - 깊은 구조에 의해 엄청난 양의 데이터를 학습할 수 있는 특징 : 딥러닝의 학습을 위한 데이터의 확보는 곧 우수한 인공지능 개발과 깊은 관련성
- 기계 학습의 종류
    - 지도학습
        - 학습 데이터로부터 하나의 함수를 유추해내기 위한 방법
        - 지도 학습기가 하는 작업은 훈련 데이터로부터 주어진 데이터에 대해 예측하고자 하는 값을 올바로 추측해 내는 것
    - 비지도학습
        - 데이터가 어떻게 구성되었느지를 알아내는 문제의 범주에 속함
        - 지도학습 혹은 강화학습과는 달리 입력값에 대한 목표치가 주어지지 않음
        - 통계의 밀도 추정과 깊은 연관성, 데이터의 주요 특징을 요약하고 설명할 수 있음
    - 준지도학습
        - 목표값이 표시된 데이터와 표시되지 않은 데이터를 모두 학습에 사용하는 것
        - 많은 기계학습 연구자들이 목표값이 없는 데이터에 적은 양의 목표값을 포함한 데이터를 사용할 경우 학습 정확도에 있어서 상당히 좋아짐
    - 강화학습
        - 행동심리학에서 영감을 받았으며, 선택 가능한 행동들 중 보상을 최대화하는 행동 혹은 순서를 선택하는 방법
        - 강화학습의 초점은 학습 과정에서의 성능, 이는 탐색과 이용의 균형을 맞춤으로써 제고
- 기계학습 방법에 따른 인공지능 응용분야

|학습 종류|방법|응용 영역|
|---|---|---|
|지도학습|분류모형|이미지 인식, 음성 인식, 신용평가 및 사기검출, 불량예측 및 원인발굴|
|지도학습|회귀모형|시세/가격/주가 예측, 강우량 예측 등|
|비지도학습|군집분석|텍스트 토픽 분석, 고객 세그멘테이션|
|비지도학습|오토인코더|이상징후 탐지, 노이즈 제거, 텍스트 벡터화|
|비지도학습|생성적 적대 신경망|시뮬레이션 데이터 생성, 누락 데이터 생성, 패션 데이터 생성 등|
|강화학습|강화학습|게임 플레이어 생성, 로봇 학습 알고리즘, 공급망 최적화 등|

2) 인공지능 데이터 학습의 진화

- 전이학습
    - 인간의 응용력과 같이 유사 분야에 학습된 딥러닝 모형을 다른 문제를 해결하기 위해 사용하고자 할 때 적은 양의 데이터로도 좋은 결과를 얻을 수 있음
    - 주로 이미지, 언어, 텍스트 인식과 같이 지도학습 중 분류모형인 인식 문제에 활용 가능 : 인식 문제의 경우 데이터 표준화가 가능하여 사전학습모형 입력형식에 맞출 수 있음
- 전이학습 기반 사전학습모형 : 학습 데이터에 의한 인지능력을 갖춘 딥러닝 모형에 추가적인 데이터를 학습시키는 방식
    - 데이터 학습량에 따라 점차 발전하는 것도 중요하지만, 응용력을 갖추는 것도 필수적
    - 상대적으로 적은 양의 데이터로도 제한된 문제에 인공지능 적용이 가능 : 이미 학습된 사전학습모형도 데이터를 함축한 초보적 인공지능으로서 충분한 가치를 지닌 새로운 의미의 데이터
- BERT : 2018년 구글에서 발표한 언어인식 사전학습 모형
    - 학보된 언어 데이터의 추가 학습을 통한 신속한 학습 가능
    - 다층의 임베딩 구조를 통해 1억2천 개가 넘는 파라미터로 구성된 획기적인 모형
    - 256개까지의 문자가 입력되어 768차원 숫자 벡터가 생성되는 방식
    - 언어 인식뿐 아니라 번역, 챗봇의 Q&A 엔진으로 활용 가능

3) 빅데이터와 인공지능의 관계

- 인공지능을 위한 학습 데이터 확보
    - 학습 데이터 측면을 고려한 양질의 데이터 확보는 결국 성공적인 인공지능 구현과 직결
    - 딥러닝은 깊은 구조를 통해 무한한 모수 추정이 필요한 만큼 많은 양의 데이터 필요
    - 인공지능 학습에 활용될 수 있는 데이터로 가공이 필요, 학습의 가이드를 제공해 주는 애노테이션 작업이 필수적
- 학습 데이터의 애노테이션 작업
    - 많은 데이터 확보 후 애노테이션을 통해 학습이 가능한 데이터로 가공하는 작업 필요
    - 작업의 특성상 많은 수작업이 동반되며, 이로 인해 인공지능 사업은 노동집약적이라는 인식을 만들어 냄
- 애노테이션 작업을 위한 도구로써의 인공지능
    - 인공지능 시장이 확장되며 애노테이션 작업을 전문으로 하는 기업의 수 증가
        - 경쟁으로 인해 학습용 데이터에 대한 보안 및 애노테이션 결과에 대한 품질 요구수준이 높아짐
        - 기업들은 데이터 업로드 및 애노테이션 도구, 작업 모니터링을 위한 플랫폼을 제공 시작
    - 현재 자동으로 애노테이션을 수행해 주는 인공지능 기반의 애노테이션 도구를 제공하는 서비스로 진화 중

4) 인공지능의 기술동향

- 기계학습 프레임워크 보급 확대
    - 구글브레인이 개발한 텐서플로우는 파이썬 기반 딥러닝 라이브러리로 여러 CPU 및 GPU와 플랫폼에서 사용 가능
    - 케라스는 딥러닝 신경망 구축을 위한 단순화된 인터페이스를 가진 라이브러리이며, 몇 줄의 코드만으로 딥러닝 모형 개발이 가능
- 생성적 적대 신경망
    - GNA은 두 개의 인공신경망으로 구성된 딥러닝 이미지 생성 알고리즘
    - 생성자가 가짜 사례를 생성하면 감별자가 진위를 판별하도록 구성한 후 이들이 적대적 관계 속에서 공방전을 반복하도록 함 : 가짜 사례의 정밀도를 점점 더 진짜 사례와 구별하기 어려운 수준으로 높이는 방식으로 작동
    - 주로 새로운 합성 이미지를 생성하는 분석에 많이 적용되어 왔으나, 점차 다른 분야에 응용하는 사례가 늘고 있음
- 오토인코더
    - 라벨이 설정되어 있지 않은 학습 데이터로부터 더욱 효율적인 코드로 표현하도록 학습하는 신경망
    - 입력 데이터의 차원을 줄여 모형을 단순화시키기 위해 활용
- 설명 가능한 인공지능
    - 기존의 기계학습은 정확한 예측을 할 수 있도록 하는 방향으로 개발되어 짐
    - 기존 기계학습의 완성된 모형은 내부 구조가 매우 복자합하고 의미를 이해하기 어려워 일종의 블랙박스 모형
- 기계학습 자동화 : 기계학습의 전체 과정을 자동화
    - 데이터 전처리, 변수 생성, 변수 선택, 알고리즘 선택, 하이퍼파라미터 최적화 등의 기능을 수행
    - 기계학습 모형 개발 과정의 생산성을 높이며 비전문가들의 활용을 용이하게 할 것으로 기대

5) 인공지능의 한계점과 발전방향

- 국내시장의 한계
    - 국내에서 축적한 머신러닝 및 인공지능과 관련한 수학, 통계학적 이해도는 낮은 수준
    - 인공지능 개발을 위한 데이터 확보 및 그 중요성에 대한 인식이 부족함
- 인공지능의 미래
    - 딥러닝의 재학습 및 전이학습 특성을 활용한 사전학습모형이 새로운 데이터 경제의 모습이 됨
    - 마스킹이나 라벨링 등의 애노테이션 작업을 통해 학습용 데이터를 가공하는 산업 확산
    - 복잡한 BERT의 학습을 위한 구글의 클라우드 서비스와 같은 확장된 개념의 데이터 경제로 파생될 것으로 보임

### 04. 개인정보 개요

1) 개인정보의 정의와 판단기준

- 개인정보의 정의
    - 살아 있는 개인에 관한 정보로서 개인을 알아볼 수 있는 정보
    - 해당 정보만으로는 특정 개인을 알아볼 수 없더라도 다른 정보와 쉽게 결합하여 알아볼 수 있는 정보를 포함
- 개인정보의 판단기준
    - '생존하는' '개인에 관한' 정보
    - '정보'의 내용, 형태 등은 제한 없음
    - 개인을 '알아볼 수 있는' 정보여야 함 : 다른 정보와 '쉽게 결합하여' 개인을 알아볼 수 있는 정보도 포함

2) 개인정보의 처리와 활용
    
- 개인정보의 이전 : 개인정보가 다른 사람에게 이전되거나 공동으로 처리하게 하는 것
- 개인정보의 처리 위탁 : 개인정보처리자의 업무를 처리할 목적으로 제3자에게 이전되는 것
- 개인정보의 제3자 제공 : 해당 정보를 제공받는 자의 고유한 업무를 처리할 목적 및 이익을 위하여 개인정보가 이전되는 것

3) 개인정보의 보호

- 개인정보의 보호조치
    - 조직 내부의 정보보안 방침과 개인정보보호법에 위배되지 않도록 개인정보보호 가이드라인을 점검
    - 데이터를 외부에 공개하는 경우 가이드라인에서 정한 규칙을 준수하는지 반드시 확인
    - 가이드라인에 명시되지 않은 경우 관계기관이나 조직 내부의 법무가이드를 받은 후 적절한 범위 안에서 데이터를 활용
    - 개인정보 보호를 위해 주기적인 패스워드 변경, 시스템 패스워드 관리 보안 강화, 의심스러운 메일 열람 금지, 정기적인 보안교육 참여 등을 요구
    - 백신의 설치 및 최신버전으로 유지하고, 개인정보를 과하게 요구하는 사이트의 가업을 자제

### 05. 개인정보 법 · 제도

1) 개인정보보호법

- 개인정보보호법의 개요
    - 당사자의 동의 없는 개인정보 수집 및 활용하거나 제3자에게 제공하는 것을 금지하는 등 개인정보보호를 강화한 내용을 담아 제정한 법률
    - 상대방의 동의 없이 개인정보를 제3자에게 제공하면 5년 이하의 징역이나 5000만원 이하의 벌금에 처할 수 있음
- 개인정보의 범위(제2조 제1호)
    - 어떤 정보가 개인정보에 해당하는지는 그 정보가 특정 개인을 알아볼 수 있게하는 다른 정보와 쉽게 결합할 수 있는가에 따라 결정
    - 법원은 그 정보 자체로는 누구의 정보인지를 알 수 없더라도 다른 정보와 결합 가능성을 비교적 넓게 인정하여 개인정보에 해당한다 판단
- 개인정보의 처리 위탁
    - 일정한 내용을 기재한 문서에 의하여 업무 위탁이 이루어져야 함
    - 위탁하는 업무의 내용과 수탁자를 정보주체엑 알려야 하는바, 개인정보처리방침에 해당 내용을 추가하여 공개하거나, 사업장 등의 보기 쉬운 장소에 게시하는 방법 등을 시행
    - 수탁자에 대한 교육 및 감독 의무를 부담
    - 수탁자가 위탁 받은 업무와 관련하여 개인정보를 처리하는 과정에서 개인정보보호법을 위반하여 발생한 손해배생책임에 대하여는 수탁자를 개인정보처리자의 소속 직원으로 봄
    - 손해가 발생한 경우 정보주체의 손해배상 청구에 대해 위탁자가 책임
- 개인정보의 제3자 제공 : 정보주체로부터 개인정보 제3자 제공 동의를 받아야 함

2) 정보통신망 이용촉진 및 정보보호 등에 관한 법률(정보통신망법)

- 정보통신망법의 개요
    - 정보통신망의 개발과 보급 등 이용 촉진과 함께 통신망을 통해 활용되고 있는 정보보호에 관해 규정한 법률
    - 이용자의 동의를 받지 ㅇ낳고 개인정보를 수집하거나 제3자에게 개인정보를 제공한 경우, 법정대리인의 동의 없이 만 14세 미만의 아동의 개인정보를 수집한 경우, 악성프로그램을 전달 또는 유포한 경우 등은 5년 이하의 징역 또는 5000만 원 이하의 벌금
- 개인정보의 처리 위탁
    - 원칙적으로는 개인정보 처리위탁을 받는 자, 개인정보 처리위탁을 하는 업무의 내용을 이용자에게 알리고 동의 받아야 함
    - 단, 정보통신서비스 제공자 등은 정보통신서비스의 제공에 관한 계약을 이행하고 이용자의 편의 증진 등을 위하여 필요한 경우에는 고지절차와 동의절차를 거치지 ㅇ낳고, 이용자에게 이에 관해 알리거나 개인정보 처리방침 등에 이를 공개할 수 있음
    - 만일 제3자에게 데이터 분석을 위탁할 경우, 해당 서비스가 정보통신서비스 제공에 관한 계약을 이행하고 이용자의 편의 증진을 위한 것인지 검토해야 함

3) 신용정보의 이용 및 보호에 관한 법률(신용정보보호법)

- 신용정보보호법의 개요
    - 개인신용정보를 신용정보회사 등에게 제공하고자 하는 경우에 해당 개인으로부터 서면 똔느 공인전자서명이 있는 전자문서에 의한 동의 등을 얻어야 함
    - 신용정보주체는 신용정보회사 등이 본인에 관한 신용정보를 제공하는 때에는 제공받은 자, 그 이용 목적, 제공한 본인정보의 주요 내용 등을 통보하도록 요구하거나 인터넷을 통하여 조회할 수 있도록 요구 가능
    - 신용정보회사 등이 보유하고 있는 본인정보의 제공 또는 열람을 청구할 수 있고, 사실과 다른 경우에는 정정을 청구 가능
- 개인정보의 범위(제2조 제1호 및 제2호, 제 34조 제1항)
    - '신용정보' : 금융거래 등 상거래에 있어서 거래 상대방의 신용을 판단할 때 필요한 정보로서 다음 각 목의 정보를 말함
        - 특정 신용정보주체를 식별할 수 있는 정보
        - 신용정보주체의 거래내용을 판단할 수 있는 정보
        - 신용정보주체의 신용도를 판단할 수 있는 정보
        - 신용정보주체의 신용거래능력을 판단할 수 있는 정보
        - 그 밖에 위처럼 유사한 정보
- 개인신용정보 : 금융거래 등 상거래에 있어서 거래 상대방에 대한 신용도·신용거래능력 등의 판단을 위해 필요로 하는 정보로 정의하고, 그 세부 사항은 대통령령으로 정함
- 개인신용정보의 처리 위탁
    - 신용정보회사 등은 그 업무 범위에서 의뢰인의 동의를 받아 다른 신용정보회사에 신용정보의 수집·조사를 위탁 가능
    - 신용정보회사, 신용정보집중기간, 은행, 금융지주회사, 금융투자업자, 보험회사 등은 신용정보 처리 위탁 시 금융위원회에 보고해야 하며, 이에 관한 구체적 사항은 '금융회사의 정보처리 업무 위탁에 과한 규정'에 따름
    - 특정 신용정보주체를 식별할 수 있는 정보는 암호화하거나 봉함 등의 보호조치를 하여야 하며, 신용정보가 분실·도난·유출·변조 또는 웨손당하지 않도록 수탁자를 연 1회 이상 교육해야 함
    - 위탁계약의 이행에 필요한 경우로서 수집된 신용정보의 처리를 위탁하기 위하여 제공하는 경우 정보주체의 동의를 받지 않아도 됨
- 개인신용정보의 제3자 제공
    - 개인신용정보를 타인에게 제공하려는 경우 정보주체에 서비스 제공을 위하여 필수적 동의 사항과 그 밖의 선택적 동의 사항을 구분하여 설명한 후 각각 동의를 받도록 함
    - 기타 개인정보 제공 시 개인정보보호법이 적용

4) 2020년 데이터 3법의 주요 개정 내용

- 개인정보보호법 주요 개정 내용
    - 개인정보 관련 개념을 개인정보, 가명정보, 익명정보로 구분
    - 가명정보를 통계 작성 연구, 공익적 기록보존 목적을 처리할 수 있도록 허용
    - 가명정보 이용 시 안전장치 및 통제 수단 마련
    - 분산된 개인정보보호 감독기관을 개인정보보호위원회로 일원화
    - 개인정보보호위원회는 국무총리 소속 중앙행정기관으로 격상
- 정보통신망법 주요 개정 내용
    - 개인정보보호 관련 사항을 개인정보보호법으로 이관
    - 온라인상 개인정보보호 관련 규제 및 감독 주체를 개인정보보호위원회로 변경
- 신용정보보호법 주요 개정 내용
    - 가명정보 내용을 도입해 빅데이터 분석 및 이용의 법적 근거 마련
    - 가명정보는 통계작성, 연구, 공익적 기록보존 등을 위해 신용정보 주체의 동의 없이 이용, 제공 가능

### 06. 개인정보 비식별화

1) 개인정보 비식별화의 개요

- 비식별 정보 : 정보의 집합물에 대해 '개인정보 비식별 조치 가이드라인'에 따라 적정하게 '비식별 조치'된 정보를 말함
- 비식별 조치 : 정보의 집합물에서 개인을 식별할 수 있는 요소를 전부 또는 일부 삭제하거나 대체 등의 방법을 통해 개인을 알아볼 수 없도록 하는 조치
- 비식별 정보의 활용
    - 비식별 정보는 개인정보가 아닌 정보로 추정되므로 정보주체로부터의 별도 동의없이 해당 정보를 이용하거나 제3자에게 제공할 수 있음
    - 다만, 불특정 다수에게 공개되는 경우에는 다른 정보를 보유하고 있는 누군가에 의해 해당 정보주체가 식별될 가능성이 있으므로 비식별 정보의 공개는 원칙적으로 금지
- 비식별 정보의 보호
    - 비식별 정보는 개인정보가 아닌 것으로 추정되지만, 새로운 결합 기술이 나타나거나 결합 가능한 정보가 증가하는 경우에는 정보주체가 '재식별'될 가능성이 있음
    - 비식별 정보를 처리하는 자(비식별 정보를 제공받은 자 포함)가 해당 정보를 이용하는 과정에서 재식별하게 된 경우에는 해당 정보를 즉시 처리중지하고 파기하여야 함
    - 비식별 정보라고 하더라도 필수적인 관리적·기술적 보호조치는 이행해야 함

2) 개인정보 비식별화 조치 가이드라인

- 개인정보 비식별화 조치 가이드라인의 추진배경
    - 정부 3.0 및 빅데이터 활용 확산에 따른 데이터 활용가치가 증대
    - 개인정보 보호 강화에 대한 사회적 요구가 지속
    - '보호와 활용'을 동시에 모색하는 세계적 정책변화에 적극 대응 필요
- 개인정보 비식별화 조치 가이드라인의 단계별 조치사항

|단계|조치사항|데이터|
|----|---|---|
|사전 검토|개인정보에 해당하는지 여부를 검토한 후, 개인정보가 아닌 것이 명백한 경우 법적 규제 없이 자유롭게 활용|개인정보, 식별정보|
|비식별 조치|정보 집합물(데이터 셋)에서 개인을 식별할 수 있는 요소를 전부 또는 일부 삭제하거나 대체하는 등의 방법을 활용, 개인을 알아볼 수 없도록 하는 조치|가명, 총계, 삭제, 범주화, 마스킹|
|적정성 평가|다른 정보와 쉽게 결합하여 개인을 식별할 수 있는지를 '비식별 조치 적정성 평간단'을 통해 평가|k-익명성, i-다양성, i-근접성|
|사후 관리|비식별 정보 안전조치, 재식별 가능성 모니터링 등 비식별 정보 활용 과정에서 재식별 방지를 위해 필요한 조치 수행|관리적/기술적 보호조치

- 개인정보 비식별화 조치 가이드라인의 조치방법
    - 가명 처리
        - 개인정보 중 주요 식별 요소를 달느 값으로 대체하는 방법
        - 값을 대체 시 규칙이 노출되어 역으로 쉽게 식별할 수 없도록 주의
    - 총계 처리
        - 데이터의 초합 값을 보여 주고 개별 값을 보여 주지 않는 방법
        - 특정 속성을 지닌 개인으로 구성된 단체의 속성 정보를 공개하는 것을 그 집단에 속한 개인의 정보를 공개하는 것이므로 주의
    - 데이터 사게 : 데이터 공유나 개방 목적에 딸는 데이터 셋에 구성된 값 중 필요 없는 값 또는 개인식별에 중요한 값을 삭제하는 방법
    - 데이터 범주화 : 데이터의 값을 범주의 값으로 변환하여 값을 숨기는 방법
    - 데이터 마스킹
        - 개인을 식별하는 데 기여할 화귤ㄹ이 높은 주요 식별자를 보이지 않도록 처리하는 방법
        - 남아 있는 정보만으로 개인을 식별할 수 없어야 하며, 공개된 다른 정보와 결합하더라도 특정 개인을 식별할 수 없어야 함

### 07. 개인정보 활용

1) 데이터 수집의 위기 요인과 통제 방안

- 사생활 침해의 위기 발생
    - M2M 시대가 되면서 정보를 수집하는 센서들의 수가 증가
    - 개인정보의 가치가 커짐에 따라 많은 사업자들이 개인정보 습득에 더 많은 자원 투입
    - 특정 데이터가 본래 목적 외로 가공되어 2차, 3차 목적으로 활용될 가능성이 커짐
    - 위험의 범위가 사생활 침해 수준을 넘어 사회, 경제적 위험으로 더 확대될 수 있음
- 동의에서 책임으로 강화하여 통제
    - 개인정보는 본래의 1차적 목적 외에도 2차, 3차적 목적으로 가공, 유통, 활용 : 개인정보의 활용에 대해 개인이 매번 동의하는 것은 매우 어려운 일, 경제적으로도 비효율적
    - 개인정보 사용으로 발생하는 피해에 대해서는 개인정보 사용자가 책임을 지게 함
    - 개인정보를 사용하는 주체가 익명화 기술 같은 더 적극적인 보호 장치를 마련하게 하는 효과가 있을 것으로 기대

2) 데이터 활용의 위기 요인과 통제 방안

- 책임원칙 훼손으로 위기 발생
    - 빅데이터의 분석 결과에 따라 특정한 행위를 할 가능성이 높다는 이유만으로 특정인이 처벌받는 것은 민주주의 사회 원칙 훼손
    - 특정인이 특정한 사회, 경제적 특성을 가진 집단에 속한다는 이유만으로 그의 신용도와 무관하게 대출이 거절되는 상황은 잘못된 클러스터링의 피해
- 결과 기반 책임 원칙을 고수하여 통제
    - 기존의 책임 원칙을 더 강화
    - 예측 결과에 의해 불이익을 당할 가능성을 최소화하는 방안 마련 필요
    - 제도 마련과 함께 알고리즘의 기술적 완성도를 더 높여야 함

3) 데이터 처리의 위기 요인과 통제 방안

- 데이터 오용으로 위기 발생
    - 빅데이터는 과거에 일어났던 일로 인해 기록된 데이터에 의존 : 빅데이터를 기반으로 미래를 예측하는 것은 어느 정도 정확도를 가질 수 있지만 항상 맞는 것은 아님
    - 빅데이터 사용자가 데이터를 과신할 때 큰 문제가 발생할 가능성이 높음 : 잘못된 지표를 사용하는 것은 오히려 과거 경험에 의존하는 것보다 더 잘못된 결론을 도출할 수 있음
- 알고리즘 접근을 허용하여 통제
    - 알고리즘에 대한 접근권한을 부여받아 직접 검증할 수 있도록 함
    - 알고리즘에 대한 객과적인 인증방안을 마련 및 도입
    - 알고리즘의 부당함을 반증할 수 있는 방법을 제시해 줄 것을 요청
    - 공개해 준 알고리즘을 해석해 줄 알고리즈미스트와 같은 전문가 영입 : 알고리즈미스트는 컴퓨터, 수학, 통계학, 비즈니스 등의 다양한 지식 필요