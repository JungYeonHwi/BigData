# 분석기법 적용

## SECTION 01. 분석기법

### 01. 분석기법 개요

1) 학습 유형에 따른 데이터 분석 모델

- 지도학습
    - 정답이 있는 데이터를 활용해 데이터를 학습시키는 방법으로 입력값이 주어질 때 정답이 무엇인지 알려주면서 컴퓨터를 학습시키는 방법
    - 대표적으로 분류나 회귀로 구분할 수 있음
        - 분류 : 주어진 데이터를 여러 가지 중의 하나로 분류하는 것
        - 회귀 : 주어진 데이터의 특징을 기반으로 새로운 데이터 값을 예측하는 것
- 비지도학습
    - 정답 없는 데이터를 컴퓨터 스스로 학습하여 숨겨진 의미, 패턴을 찾아내고 구조화하는 방법
    - 라벨이 없는 데이터에 대해서 데이터의 특성을 스스로 분석하여 데이터가 어떤 특성의 그룹으로 구성되어 있는지 확인하는데 사용, 빅데이터 분석 프로젝트에서 초기 데이터가 동질적인 소수 집단으로 이루어져 있는지, 이질적인 많은 집단으로 구성되어 있는지 탐색하는 데에도 활용
    - 종류 : 군집분석, 연관성분석, 인공신경망, 오토인코더 등
- 준지도학습
    - 정답이 있는 데이터와 정답이 없는 데이터를 동시에 학습에 사용하는 기법
    - 일반적으로 데이터 라벨링(데이터에 정답을 붙이는 과정)을 하는 데에 훈련된 사람의 손을 거쳐야 하므로 데이터 규모가 클 경우 구축 비용이 기하급수적으로 증가
    - 목표값이 없는 데이터에 적은 양의 목표값을 포함한 데이터를 사용할 경우 비용뿐만 아니라 학습 정확도가 상대적으로 좋아지게 되어 준지도학습을 이용하여 결과를 향상 시키기 가능
    - 레이블이 된 소수의 데이터만으로 부분학습모델을 만들고, 이 모델을 사용해서 나머지 레이블이 없는 데이터에 레이블을 생성한 후 지도학습 수행
    - 종류 : 셀프 트레이닝, GAN
        - 셀프 트레이닝 : 정답이 있는 데이터로 모델을 학습한 뒤 정답이 없는 데이터를 예측하여 이 중에서 가장 확률값이 높은 데이터들만 정답 데이터로 다시 가져가는 방식을 반복하는 것으로 높은 확률 값이 나오는 데에 가중치를 주는 간단한 기법
        - GAN : 생성모델과 판별모델이 존재하여 생성모델에서 데이터 분포 법칙에 따라 데이터를 생성하면 판별 모델에서는 이를 판별하는 방식으로 학습 진행
- 강화학습
    - 주어진 환경에서 보상을 최대화하도록 에이전트를 학습하는 기법
    - 에이전트와 환경의 상태 등이 인공신경망으로 들어가게 되고 에이전트가 행동을 결정하고 환경을 통해 보상 학습하게 됨

2) 데이터 분석 알고리즘 분야

- 업리프트 모델링
    - 마케팅 캠페인에서 많이 사용하는 기법으로 실제로는 추정 모델을 단계별로 적용하는 기법
    - A/B 테스트와 같이 환경이나 조건을 달리 한 후에 적당한 그룹을 선택하여 마케팅이나 신용 관리, 채널, 가격 선택, 고객 이탈 관리 등 다양한 분야에서 사용
- 회귀분석
    - 예측 또는 분류에 사용하는 대중적인 알고리즘
    - 로지스틱 회귀분석은 이진분류에 자주 활용
- 이자율이나 주식 예측 등에 자주 사용되는 시계열 불량 감지와 사기 탐지 등에 사용하는 이상치 감지 기법
- 여러 분석기법들 중 선택하는 기준은 목적과 해석 가능 여부에 따라 달라짐

### 02. 회귀분석

1) 회귀분석

- 독립변수 : 입력값 또는 원인을 설명하는 변수
- 종속변수 : 결과값 또는 효과를 설명하는 변수
- 회귀선(회귀계수) : 독립변수가 주어질 때의 종속변수의 기댓값으로 일반적으로 최소제곱법 이용
- 최소제곱법(최소자승법) : 잔차 제곱의 합이 최소가 되게 하는 직선을 찾는 방법

2) 선형회귀분석 : 종속변수 y와 한 개 이상의 독립변수 x와의 선형 상관성을 파악하는 회귀분석 기법으로 종속변수와 독립변수 모두 연속형 변수

- 단순 선형회귀분석 : 가장 단순한 분석으로 한 개의 종속변수 y와 한 개의 독립변수 x로 두 개의 변수 사이의 관계 분석
- 다중 선형회귀분석 : 하나의 독립변수가 아닌 여러 개의 독립변수를 사용한 회귀분석 기법으로 단순 선형회귀분석이 독립변수를 하나 가지고 있는 선형회귀분석이라면 다중 선형회귀분석은 독립변수가 두 개 이상이고 종속변수가 y 하나인 선형회귀분석

3) 로지스틱 회귀분석

- 단순 로지스틱 회귀분석 : 종속변수가 이항형 문제(범주의 개수가 두 개인 경우)인 회귀분석
- 다중 로지스틱 회귀분석 : 종속변수가 이항형 문제가 아닌 두 개 이상의 범주를 가지게 될 경우의 회귀분석
    - 로지스틱 회귀함수식은 각 모수에 대해 비선형식이며 승산으로 로짓변환을 통해 선형함수로 치환 가능
    - 승산 : 임의의 사건 A가 발생하지 않을 확률 대비 일어날 확률의 비율, P(A)가 1에 가까울수록 발생확률 승산은 올라가며 반대로 P(A)가 0이라면 0
    - 두 개 이상의 범주를 가지는 문제가 대상인 경우에는 다항 로지스틱 회귀 또는 분화 로지스틱 회귀, 복수의 범주이면서 순서가 존재하면 서수 로지스틱 회귀로 다양한 분야에서 분류/예측을 위한 모델들로 활용

4) 회귀분석의 장단점

- 장점 : 크기와 관계없이 계수들에 대한 명료한 해석과 손쉬운 통계적 유의성 검증 가능
- 단점 : 선형적인 관계로 데이터가 구성되어 있어야 적용 가능

### 03. 의사결정나무

1) 의사결정나무

- 의사결정 규칙을 나무 모양으로 나타내어 전체 자료를 몇 개의 소집단으로 분류하거나 예측을 수행하는 기법
- 상위노드로부터 하위노드로 트위구조를 형성하는 매 단계마다 분류 변수와 분류 기준값의 선택이 중요
- 상위노드에서 분류된 각각의 하위노드는 노드 내 동질성이 커지고, 노드 간에는 이질성이 커지는 방향으로 분류 변수와 기준값 선택
- 모형의 크기는 과대적합(또는 과소적합) 되지 않도록 적절히 조절되어야 함
- 의사결정나무 기법을 사용한 분석은 시장조사, 광고조사, 품질관리 등 다양한 분야에서 활용되고 있으며, 타겟 고객분류, 고객 신용분류, 행동 예측 등에 사용

2) 의사결정나무 구성

- 뿌리 마디 : 나무가 시작되는 마디, 부모가 없는 마디로 대상이 되는 모든 자료집합을 포함
- 중간 마디 : 뿌리 마디에서 나온 각 나무줄기 중간에 있는 마디
- 끝 마디 : 각 나무줄기의 끝에 있는 마디, 자식이 없는 마디
- 자식 마디 : 하나의 마디로부터 분리된 2개 이상의 마디
- 부모 마디 : 자식 마디의 상위 마디
- 가지 : 하나의 마디로부터 끝 마디까지 연결된 마디들
- 깊이 : 가장 긴 가지의 크기(마디의 개수)

3) 의사결정나무의 종류

- 분류나무 : 이산형(범주형) 목표변수에 딸느 빈도 기반 분리에 사용
    - 상위 노드에서 가지 분할을 진행할 때 카이제곱 통계량의 p-값, 지니지수, 엔트로피 지수 등이 분리 기준으로 활용
    - 분리 기준을 선택할 때에는 서로 다른 데이터가 섞여 있는 정도인 불순도를 통해 자식 노드가 현재 노드에 비해 불순도가 감소되도록 설정해야 하는데 이 때의 불순도 차이를 정보 획득이라고 함
        - 카이제곱 통계량(불순도 함수) : ((실제도수-기대도수) ^ 2 / 기대도수)의 합
        - 지니 지수 : 특정 집합에서 한 항목을 뽑아 무작위로 라벨 추정 시 틀릴 확률
        - 엔트로피 지수 : 무질서 정도에 대한 측도
- 회귀나무 : 연속형(수치형) 목표변수에 따른 평균/표준편차 기반 분리에 사용
    - 상위노드에서 가지분할을 진행할 대 F-통계량의 p-값, 분산의 감소량 등이 분리 기준으로 활용
    - 분산분석 F-통계량의 p-값 : 등분산성을 검정하여 p-값이 커지면 등분산성이 있음을 뜻하므로 낮은 이질성, 즉 순수도가 높아짐
    - 분산의 감소량 : 분산의 감소량이 최대화가 될수록 낮은 이질성, 순수도가 높아지는 방향으로 가지분할 진행됨

4) 의사결정나무의 분석 과정

- 변수 선택 : 목표변수와 관련된 설명(독립) 변수들을 선택
- 의사결정나무 형성 : 분산목적에 따른 적절한 분리기준과 정지규칙, 평가기준 등으로 의사결정 나무 만듦
- 가지치기 : 부적절한 나뭇가지는 제거, 오퍼피팅을 막고 일반화 성능을 높여줌
- 모형 평가 및 예측 : 이익, 위험, 비용 등을 고려하여 모형을 평가하여 분류 및 예측 수행
    - 정보 획득 : 정보이론에서 순도가 증가하고 불확실성이 감소하는 것, 현재 노드의 불순도와 자식노드의 불순도 차이
    - 재귀적 분가 학습
        - 의사결정나무 분기 전보다 분기 후 각 영여의 정보 획득량이 높아지도록 입력 변수의 영역을 구분하여 사전에 설정한 기준을 만족할 때까지 분기를 반복하는 학습
        - 분기 뒤 순도의 증가, 불확실성이 최대한 감소하는 방향으로 학습 진행
        - 모든 잎(끝마디)의 엔트로피가 0이 될 때까지 반복하는데 새로운 데이터가 제대로 분류되지 못하는 현상을 방지하기 위해 일저단계에서 중지하거나 분기를 재조정하는 가지치기 단계로 넘어감
- 가지치기 : 평가용 데이터를 활용, 부적절한 추론규칙을 가지고 있거나 불필요한 또는 분류오류를 크게 할 위험 있는 마디들 제거
    - 에러감소 가지치기 : 분할/결합 전과 후의 오류를 비교하여 오류가 더 이상 줄어들지 않을 때까지 반복하는 방법
    - 롤 포스트 가지치기 : 나무구조를 뿌리 노드부터 잎 노드까지 경로의 형태로 변환한 뒤 정확도가 낮은 순서대로 제겅하는 방법
    - 분기가 너무 많을 경우 학습데이터가 과적합되어 더 복잡한 트리가 생성되게 되는데 이를 위해 사전 가지치기와 사후 가지치기가 있음
- 타당성 평가 : 이익, 비용, 위험 등을 고려하여 모형을 평가하는 단계로 이익 도표, 위험 도표, 교차 타당성(교차 검증)으로 의사결정나무 평가
- 해석 및 예측 : 의사결정나무 최종 모형에 대한 해석으로 분류 및 예측 모델 결정

5) 의사결정나무의 대표적 알고리즘

- CART : 일반적으로 활요되는 의사결정나무 알고리즘, 불순도 측도로 범주형 또는 이산형일 경우 지니 지수를, 연속형인 경우 분산의 감소량을 이용한 이진분리를 활용
- C4.5 / C5.0 : 범주형/이산형 목표변수에만 활용되며 불순도 측도로 엔트로피 지수를 활용, 범주의 수만큼 분리가 일어나는데 각 마디에서 다지분리 가능
- CHAID : 범주형/이산형 목표변수와 연속형 목표변수에 활요오디며 불순도 측도로 카이제곱 통계량 활용, 가지치기를 하지 않고 적당한 크기에서 성장을 중지하며 분리변수의 범주마다 마디를 형성하는 다지분리 가능
- 랜덤 포레스트 : 부트스트래핑 기반 샘플링을 활용한 의사결정나무 생성 이후 배깅 기반 나무들을 모아 앙상블 학습하여 숲을 형성하는 것
    - 부트스트래핑 : 단순 복원 임의추출법(랜덤 샘플링)으로 크기가 동일한 여러 개의 표본자료 생성
    - 배깅
        - 여러 부트스트랩 자료를 생성하여 학습하는 모델링으로 분류기를 생성한 후 그 결과를 앙상블 하는 방법
        - 추출한 각 샘플별 모델링 학습 뒤 결과들을 집계, 최종 결과를 만들어 내는 방식으로 범주형 데이터인 경우 다수결 투표방식으로, 연속형 데이터인 경우 평균으로 결과 집계
    - 부스팅
        - 가중치를 활용하여 약분류기를 강분류기로 만드는 방법으로 순차적으로 분류 모델들이 틀린 곳에 집중하여 새로운 분류 규칙 생성하는 기법
        - 이전 분류기의 학습 결과를 토대로 다음 분류기의 학습 데이터의 샘풀가중치를 조정해 학습을 진행하는데, 잘 맞춘 약분류기는 가중치를 더하고 잘못 평가한 약분류기의 가중치는 제하여 누적된 약분류기 가중치를 합산하여 최종 학습 모델링
    - 앙상블 학습 : 여러 모델을 학습시켜 결합하는 방식의 학습방법으로 일반화 성능을 향상시켜 과적합 해결

6) 의사결정나무의 장단점

- 장점
    - 연속형, 범주형 변수 모두 적용, 변수 비교가 가능하며 규칙에 대해 이해하기 쉬움
    - 데이터로부터 규칙을 도출하는 데에 유용하므로 DB마케팅, CRM, 시장조사, 기업 부도/환율 예측 등 다양한 분야에서 활용
- 단점
    - 트리구조가 복잡할 시 예측/해석력 떨어짐
    - 데이터 변형에 민감

### 04. 인공신경망(ANN)

1) 인공신경망의 특징

- 인공신경망은 인간의 두뇌 신경세포인 뉴런을 기본으로 한 기계학습 기법으로 하나의 뉴런이 다른 뉴런들과 연결되어 신호를 전달, 처리하는 구조
- 입력데이터가 들어가면서 신호의 강도에 따라 가중치 처리되고 활성화 함수를 통해 출력이 계산되는데 학습을 거쳐 원하는 결과가 나오게끔 가중치가 조정된다는 점이 주요 특징, 신경망 모형은 높은 복잡성으로 입력 자료의 선택에 민감

2) 인공신경망의 발전

- 기존 신경망 다층 퍼셉트론이 가진 문제
    - 사라지는 경사도 : 신경망 층수를 늘릴 때 데이터가 사라져 학습이 잘 되지 않는 현상
    - 과대적합 : 데이터가 많지 않은 경우 특정 학습데이터에만 학습이 잘되어 신규 데이터에 대한 추론처리 성능이 낮아지는 문제
- 딥러닝의 등장
    - 2000년 중, 후반부터 사전학습으로 사라지는 경사도 문제를 해결하고 과대적합을 방지하는 초기화 알고리즘의 발전 및 고의로 데이터를 누락시키는 드롭아웃을 사용하여 해결되는 것이 증명되면서 기존 인공신경망을 뛰어넘은 모델을 리브랜딩의 일환으로 딥러닝으로 부르게 됨
    - 딥러닝은 알고리즘 개선 외에도 풍부한 학습이 가능한 빅데이터 도래 및 신경망을 이용한 학습과 계산에 적합한 그래픽 처리장치 등 하드웨어 발전에 힘입어 21세기 이후 가장 알려진 인공신경망이 됨
    - 딥러닝의 기본구조인 DNN은 은닉층을 2개 이상 가진 학습 구조로 컴퓨터가 스스로 분류 답안을 만들어내며 데이터를 구분, 반복하여 최적의 답안을 결정

3) 인공신경망의 원리

- 지도학습의 경우 하나의 뉴런은 입력 값(X)과 목표 출력 값(Y)이 있을 때 다음 뉴런으로 전달하는데 적절한 출력 값을 생성하기 위해 가중치 W를 곱한 값에 편향을 더하여 이를 조정하면서 학습, 최적화 과정을 거치게 되며 최종적으로 활성화 함수 활용
- 비지도학습, 강화학습 기반 다양한 인공신경망 모델들로 구현
- 인공신경망의 주요 요소
    - 노드 : 신경계 뉴런, 가중치와 입력값으로 활성함수를 통해 다음 노드로 전달
    - 가중치 : 신경계 시냅스, 노드와의 연결계수
    - 활성함수 : 임계값 이용, 노드의 활성화 여부 결정
    - 입력층 : 학습 위한 데이터 입력
    - 은닉층 : 다층 네트워크에서 입력층과 출력층 차이, 데이터를 전파학습
    - 출력층 : 결과값 출력
- 뉴런 간의 연결 방법
    - 층간 연결 : 서로 다른 층에 존재하는 뉴런과 연결
    - 층내 연결 : 동일 층 내의 뉴런과의 연결
    - 순환 연결 : 어떠한 뉴런의 출력이 자기 자신에게 입력되는 연결

4) 학습 : 신경망에는 적용 가능한 가중치와 편향이 있으며 이를 훈련 데이터에 적용하도록 조정하는 과정

- 손실함수
    - 신경망이 출력한 값과 실제 값과의 오차에 대한 함수
    - 손실 함수값이 최소화되도록 하기 위해 가중치와 편향을 찾는 것을 인공신경망의 학습
    - 일반적인 손실 함수로는 평균제곱 오차 또는 교차엔트로피 오차 활용
- 평균제곱 오차(MSE) : 인공신경망의 출력 값과 사용자가 원하는 출력 값 사이의 거리 차이를 오차로 사용하며 각 거리 차이를 제곱하여 합산한 후 평균을 구함
- 교차 엔트로피 오차(CEE) : 분류 부문으로 t값이 원-핫 인코딩 벡터이며, 모델의 출력 값에 자연로그를 적용, 곱함
- 학습 알고리즘
    - 1단계 : 미니배치
        - 훈련 데이터 중 일부를 무작위로 선택한 데이터를 미니배치라고 함
        - 이에 대한 손실함수를 줄이는 것으로 목표 설정
    - 2단계 : 기울기 산출
        - 미니배치의 손실함수 값을 최소화하기 위해 경사법으로 가중치 매개변수의 기울기를 일반적으로 미분을 통해 구함
        - 경사법은 현재 위치에서 기울어진 방향으로 일정 거리만큼 이동, 이동한 자리에서 다시 기울기를 구해서 기울어진 방향으로 이동하는 과정을 반복하는 기법
        - 기울기의 최소값을 찾는 경사 하강법, 경사 상승법, 무작위 미니배치를 통한 확률적 경사 하강법이 있음
    - 3단계 : 매개변수 갱신
        - 가중치 매개변수를 기울기 방향으로 조금씩 업데이트 하면서 1~3단계를 반복
- 오차역전파
    - 가중치 매개변수 기울기를 미분을 통해 진행하는 것은 시간 소모가 크므로 오차를 출력층에서 입력층으로 전달, 연쇄법칙을 활용한 역전파를 통해 가중치와 편향을 계산, 업데이트 함
    - 신경망 각 계층에서의 역전파 처리는 덧셈노드, 곱셈노드의 연산 역전파처리, 활성화 함수인 렐루 계층, 시그모이드 계층, 아핀 계층, Softmax-with-loss 등의 역전파 처리 등이 있음
- 활성(활성화) 함수
    - 입력 신호의 총합을 그대로 사용하지 않고 출력 신호로 변환하는 함수
    - 활성화를 일으킬지를 결정하게 됨
    - 대표적인 함수는 시그모이드와 렐루가 있음
    - 퍼셉트론은 1개 이상의 입력층과 1개 출력층 뉴런으로 구성된 활성화 함수에 따라 출력되는 신경망구조, 다른 퍼셉트론은 은닉층이 1개 이상의 퍼셉트론으로 활성화함수인 계단 함수를 사용하여 0 또는 1을 반환하는 반면에 딥러닝 인공신경망은 시그모이드를 포함한 다른 활성화함수들을 사용하여 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 특징
- 과대적합(과적합) : 기계학습에서 학습 데이털을 과하게 학습하는 것으로 학습데이터에 대해서는 높은 정확도 성능으로 오차가 줄어드나 실제 데이터에 적용할 시에는 성능이 떨어지면서 오차가 증가하는 경우, 일반적으로 훈련데이터가 적은 경우, 매개변수가 많고 표현력이 높은 모델인 경우 등에 발생
    - 해결방안
        - 가중치 감소 : 가중치가 클수록 일종의 패널티를 부과하여 가중치 매개변수 절대값을 감소시켜 과적합의 위험을 줄임, 패널티 역할로 규제가 이용되는데 모델을 강제로 제한한다는 의미
            - Lasso(L1 규제)
            - Ridge(L2 규제)
        - 드롭아웃 : 신경망모델에서 은닉층의 뉴런을 임의로 삭제하면서 학습하는 방법으로 적은 뉴런만으로 훈련한 뒤 테스트 시에 전체 뉴런을 사용하면 정답을 보다 잘 찾을 수 있음
        - 초매개변수 최적화 방법 : 최적의 딥러닝 모델 구현을 위해 학습률이나 배치크기, 훈련 반복 횟수, 가중치 초기화 방법 등 수동으로 딥러닝 모델에 설정하느 변수로 적절한 튜닝으로 최적화된 하이퍼파라미터를 도출하여 과적합을 방지할 수 있음

5) 딥러닝 모델 종류

- CNN(합성곱 신경망 모델)
    - 신경네트워크의 한 종류인 CNN은 사람의 시신경 구조를 모방한 구조로 인접하는 계층의 모든 뉴런과 결합된 완전 연결을 구현한 아핀 계층을 사용하여 모든 입력 데이터들을 동등한 뉴런으로 처리
    - 형상정보를 처리할 수 없었던 과거 모델들과 달리 이미지 형상을 유지할 수 있는 모델로 데이터의 특징, 차원을 추출하여 패턴을 이해하는 방식으로 이미지(벡터)의 특징을 추출하는 과정과 클래스를 분류하는 과정을 통해 진행됨
        - CNN에서 특징을 추출하는 과정은 합성곱 계층과 풀링 계층으로 나뉘어지는데 입력된 데이터를 필터가 순회하며 합성곱을 계산한 뒤 특징지도 생성
        - 특징 지도는 서브샘플링을 통해 차원을 줄여주는 효과를 지니며 필터 크기, 스트라이드, 패딩 적용여부, 최대 풀링 크기에 따라 출려 데이터의 구조가 결정됨
            - 필터 : 이미지 특징을 찾기 위한 정사각형 행렬로 정의된 파라미터로 커널로도 불림
            - 스트라이드 : 필터는 입력 데이터를 일정한 간격인 스트라이드로 순회하면서 특징을 추출하는데, 결과로 특징지도가 만들어짐
            - 패딩 : 합성곱 계층에서 필터와 스트라이드 적용으로 생성된 특징지도는 입력데이터 크기보다 작은데 해당 출력데이터 크기가 줄어드는 것을 사전 방지하고자 입력데이터 주변을 특정값으로 채우는 것을 의미, 입력 데이터에 대한 필터의 크기와 스트라이드 크기에 따라서 특징 지도 크기가 결정됨
    - 합섭곱 계층
        - 합성곱 연산은 2차원의 입력데이터가 들어오면 필터의 윈도우를 일정 간격으로 이동하면서 입력 데이터에 적용, 입력과 필터에서 대응하는 원소끼리 곱한 뒤 총합을 구하면 결과가 출력되고 이를 모든 영역에서 수행하면 합성곱의 연산 출력이 완성됨
        - 필터의 매개변수가 완전연결 신경망의 가중치에 해당됨, 합성곱 신경망을 통해 학습이 반복되면서 필터의 원소값이 매번 갱신되며 편향은 동일하게 항상 하나만 존재
        - 3차원의 합성곱 연산에서 입력 데이터 채널 수와 필터의 채널 수가 같아야 하며 필터크기는 임의로 설정가능하나 모든 채널의 필터 크기가 동일해야 하는 조건을 가짐
    - 풀링 계층
        - 선택적인 요소이며 독립적인 채널별 연산, 입력데이터의 채널수가 변화되지 않도록 2차원 데이터의 세로 및 가로 방향의 공간을 줄이는 연산으로 최대 풀링, 평균 풀링 등이 있음
        - 최대 풀링은 대상영역에서 최댓값을 취하는 연산이며 평균 풀링은 대상 영역의 평균을 계산함
        - 풀링 계층을 이용하는 경우 이미지를 구성하는 요소 변경 시 출력값이 영향을 받는 문제를 최소화하며 이미지 크기 축소를 통해 인공신경망의 매개변수 또한 크게 줄어들어 과적합 및 학습시간 소요를 해결할 수 있음
    - 평탄화 계층을 통해 이미지 형태의 데이터를 배열 형태로 처리한 뒤 완전연결계층을 통해 최종 클래스가 분류됨, CNN 계층은 합성곱 계층-Affine ReLU-풀링 흐름으로 연결되며 출력에 가까운 층에서는 Affine-ReLU 구성을 사용할 수 있으며 마지막 출력 계층에서는 Affine-Softmax 조합 이용
    - CNN은 보편적으로는 이미지 프로세싱에서 활용되는데 수치, 텍스트, 음성, 이미지들의 여러 유형의 데이터 들에서 많은 특징들을 자동으로 학습하여 추출, 분류, 인식 처리에 사용되고 있음
    - 대표적인 CNN 모델로 LeNet이 있으며, VGG, GoogleLeNet, ResNet은 더 깊은 층을 쌓은 합성곱 신경망 기반 심층 신경망 모델(DNN)임
- RNN : 순서를 가진 데이터를 입력하면 단위 간 연결이 시퀀스를 따라 방향성 그래프를 형성하는 신경네트워크 모델로 내부 상태(메모리)를 이용하여 입력 시퀀스를 처리
    - CNN과는 달리 중간층(은닉층)이 순환구조로 동일한 가중치 공유, 가중치와 편향에 대한 오차함수의 미분을 계산하기 위해 확률적 경사하강법 이용
    - RNN은 가중치 업데이터를 위해 과거시점까지 역전파하는 BPTT 활용, 입력 데이터의 순서로 모두 동일 연산을 수행하며 입력 시점마다 가중치가 공유, 계산 기울기는 현재 상태와 이전 상태에 대해 의존적이므로, 순차적 데이터 처리에 유용
- LSTM
    - RNN은 점차 데이터가 소멸해 가는 문제를 발생하는데, 관련 정보와 그 정보를 사용하는 지점 사이 거리가 멀 경우에 역전파 시 기울기가 점차 줄어들어 학습 능력이 떨어짐
    - LSTM은 RNN.의 단점을 보완하기 위해 변형된 알고리즘으로 보통 신경망 대비 4배 이상 파라미터를 보유하여 많은 단계를 거치더라도 오랜 시간동안 데이터를 잘 기억함
    - LSTM은 3가지 게이트로 보완된 구조를 통해 가중치를 곱한 후 활성화 함수를 거치지 않고 컨트롤 게이트를 통해 상황에 맞게 값을 조절함으로써 문제를 해결
    - LSTM은 은닉층 이외 셀이라는 층을 구성하는데 셀은 장기 메모리를 기억하는 셀로 망각 게이트와 입력 게이트를 과거와 현재 상태의 셀로 조합, 과거정보를 얼마나 망각할 지 현재 정보를 얼마나 반영할지를 결정, 해당 메모리값이 활성화 함수를 거치고 출력 게이트를 통해 얼마나 밖으로 표현될지가 결정되면 현재의 은닉층 값이 정해짐
- 오토인코더
    - 대표적 비지도학습 모델로 다차원 데이터를 저차원으로 바꾸고 저차원 데이터를 다시 고차원 데이터로 바꾸면서 특징점을 찾아냄
    - 입력으로 들어온 다차원 데이터를 인코더를 통해 차원을 줄이는 은닉층으로 보내고, 은닉층의 데이터를 디코더를 통해 차원을 늘리는 출력층으로 내보낸 뒤, 출력값을 입력값과 비슷해지도록 만드는 가중치를 찾아냄, 하나의 신경망을 두 개 붙여놓은 형태이며 출력 계층과 입력 계층의 차원은 같음
    - 주로 활용되는 분야는 데이터 압축, 저차원화를 통한 데이터 관찰, 배경잡음 억제
- GAN
    - 학습 데이터 패턴과 유사하게 만드는 생성자 네트워크와 패턴의 진위 여부를 판별하는 판별자 네트워크로 구성되는데 두 네트워크가 서로의 목적을 달성하도록 학습 반복
    - 판별자 네트워크 : 랜덤 노이즈 m개를 생성하여, 생성자 네트워크에 전달하고 변환된 데이터 m개와 진짜 데이터 m개를 획득, 2m개의 데이터를 이용해 판별자 네트워크의 정확도를 최대화하는 방향으로 학습
    - 생성자 네트워크 : 랜덤 노이즈 m개를 재생성하여 생성자가 판별자의 정확도를 최소화하도록 학습
    - 생성자 네트워크에 랜덤 노이즈가 주어지며 출력은 학습 데이터와 유사한 패턴으로 변환하는 함수를 학습, 판별자 네트워크는 생성된 데이터가 학습 데이터에 포함된 진짜인지에 대한 확률 출력, 생성자 네트워크와 판별자 네트워크 모두 데이터 형태에 적합한 네트워크를 선택하며 MLP, CNN, Autoencoder 등 제약없이 사용 가능
    - GAN은 기본적으로 두 모델간의 균형있는 경쟁이 필요하나 한쪽으로 역량이 치우치는 경우 성능이 제약되게 되는데 이를 개선한 모델이 DCGAN, 비지도학습 적용을 위해 기존의 Fully Connected DNN 대신 CNN 기법으로 leaky_RELU 활성화 함수를 적용하여 신경망 구성
    - GAN은 기초연구, 응용산업에 폭넓게 활용되고 있으며 DCCGAN, SRGAN, 스택 GAN, Cycle GAN 등의 종류가 있음

6) 인공신경망의 장단점

- 장점
    - 비선형적 예측 가능
    - 다양한 데이터 유형, 새로운 학습 환경, 불완전한 데이터 입력 등에도 적용 가능
- 단점
    - 데이터가 커질수록 학습시키는 데에 시간 비용이 기하급수적으로 커질 수 있음
    - 모델에 대한 설명가능이 떨어지나 Explainable AI 등 대체안이 연구되고 있음

### 05. 서포트벡터머신(SVM)

1) 서포트벡터머신 : 지도학습 기법으로 고차원 또는 무한 차원의 공간에서 초평면(의 집합)을 찾아 이를 이용하여 분류와 회귀 수행

2) SVM의 주요 요소

- 벡터 : 점들 간 클래스
- 결정영역 : 클래스들을 잘 분류하는 선
- 초평면 : 서로 다른 분류에 속한 데이터들 간 거리를 가장 크게 하는 분류 선
- 서포트벡터 : 두 클래스 사이에 위치한 데이터 포인트들
- 마진 : 서프토벡터를 지나는 초평면 사이의 거리

3) SVM의 핵심적 특징

- 기존 분류기가 '오류율 최소화'를 특징으로 한다면 SVM은 '여백(마진) 최대화'로 일반화 능력의 극대화 추구
- 마진이 가장 큰 초평면을 분류기로 사용할 때, 새로운 자료에 대한 오분류가 가장 낮아짐
- 초평면의 마진은 각 서포트 벡터를 지나는 초평면 사잉의 거리를 의미함, 기하학적 의미는 두 초평면 사이의 거리, 즉 2/||w||라는 것을 알 수 있으며 마진을 최대화해야 하므로 w의 크기가 최소가 되어야 함
- 선형으로 분리가 가능한 경우에는 분리 초평면은 h(x) < 0 인 모든 점들은 -1의 군집으로, h(x) > 0 인 모든 점들은 +1의 군집으로 분류되도록 구해질 수 있음
- 가중치 벡터는 초평면에 직교하며 편향은 초평면의 오프셋을 제공
- SVM은 선형 분류와 더불어 비선형 분류에서도 사용될 수 있음, 비선형 분류를 하기 위해서 주어진 데이터를 고차원 특징 공간으로 사상하는 작업이 필요한데, 이를 효율적으로 하기 위해 커널 트릭을 사용하기도 함

4) SVM의 장단점

- 장점
    - 다양한 라이브러리로 사용하기 쉬우며 분류, 회귀 예측 문제에 동시에 활용 가능
    - 신경망 기법에 비해 적은 데이터로 학습이 가능하며 과대적합, 과소적합 정도가 덜함
- 단점
    - 이진분류만 가능하며 데이터가 많을 시 모델 학습 시간이 오래 소요
    - 각각 분류에 대한 SVM 모델 구축 필요

### 06. 연관성분석

1) 연관성분석

- 둘 이상의 거래, 사건에 포함된 항목들의 관련성을 파악하는 탐색적 데이터 분석 기법으로 컨텐츠 기반 추천의 기본방법론
- 그룹에 대한 특성 분석으로 군집분서고가 병행 가능하며 장바구니 분석으로도 불림

2) 연관규칙 순서

- 데이터 간 규칙 생성 : if 조건절 -> 결과절
- 어떤 규칙이 데이터 특성에 부합되는지 기준 설정
    - 지지도 : 데이터 전체에서 해당 물건을 고객이 구입한 확률
    - 신뢰도 : 어떤 데이터를 구매했을 때 다른 제품이 구매될 조건부 확률 P(A, B) / P(A)
    - 향상도 : 두 물건의 구입 여부가 독립인지 판단하는 개념으로 1이면 상호 독립적인 관계, 1보다 크면 양의 상관관계, 1보다 작으면 음의 상관관계에 있음을 뜻함
- 규칙의 효용성 평가
    - 실제 규칙 생성 : 지지도, 신뢰도, 향상도가 높은 규칙들을 발견하기 위해 모든 경우를 탐색한다면 계산비용이 급증하기 때문에 빈발만 고려하고, 연관규칙을 생성하는 아프리오리 알고리즘을 활용하는 추세

3) 아프리오리 알고리즘 : 모든 항목집합에 대한 지지도를 계산하는 대신, 최소 지지도 이상의 빈발항목집합만을 찾아내서 연관규칙을 계산하는 기법

- 최소지지도 이상의 한 항목집합이 빈발하다면 이 항목집합의 모든 부분집합은 역시 빈발항목집합으로 연관규칙 계산에 포함
- 최소지지도 미만의 항 항목집합이 비빈발하다면 이 항목집합을 포함하는 모든 집합은 비번발항목집합으로 가지치기를 함
- 이후 최소신뢰도 기준을 적용해서 최소 신뢰도에 미달하는 연관규칙은 다시 제거하여 반복작업을 수행, 새로운 연관규칙이 없을 때까지 진행

4) 연관성분석의 장단점

- 장점 : 분석 결과가 이해하기 쉽고 실제 적용하기에 용이
- 단점
    - 품목이 많아질수록 연관성 규칙이 더 많이 발견되나 의미성에 대해 사전 판단 필요
    - 상당 수의 계산과정 필요

### 07. 군집분석

1) 군집분석

- 비지도학습의 일종으로 주어진 각 개체들의 유사성을 분석해서 높은 대상끼리 일반화된 그룹으로 분류하는 기법
- 군집에 속한 개체들의 유사성과 서로 다른 그룹간의 상이성을 분류하여 규칙 내지 결과 없이 주어진 데이터들을 가장 잘 설명하는 그룹 또는 클러스터를 찾을 수 있는 방법

2) 군집분류 시 기본적인 가정

- 하나의 군집 내에 속한 개체들의 특성은 동일
- 군집의 개수 또는 구조와 관계없이 개체간의 거리를 기준으로 분류
- 개별 군집의 특성은 군집에 속한 개체들의 평균값으로 나타냄

3) 군집분석의 척도

- 군집분석의 유사성 계산은 방법에 따라 거리와 유사성으로 구분하는데 거리는 값이 작을수록 두 관찰치가 유사함을 의미하며 유클리드 거리, 맨하탄 거리 등이 있음
- 유사성은 값이 클수록 두 관찰치가 서로 유사함을 뜻하며 코사인 값, 상관계수 등이 있음
- 유클리드 거리 : 2차원 공간에서 두 점간의 거리로 두 점을 잇는 가장 짧은 거리 개념인 피타고라스 정리를 통해 측정하며 민코우스 거리 적용시 L2 거리로도 불림
- 맨하탄 거리 : 택시 거리, 시가지 거리, 민코우스키 거리 적용시 L1 거리로도 통칭되며 사각형 격자, 블록으로 이뤄진 지도에서 출발점에서 도착점까지 가로지르지 않고 도착하는 최단거리 개념
- 민코우스키 거리 : m차원 민코프스키 공간에서의 거리를 뜻하며 m=1일 때 맨하타 ㄴ거리와 같고 m=2일 때 유클리드 거리와 같음
- 마할라노비스 거리 : 일반적인 다변량 데이터에서 두 데이터 간의 거리를 파악하기 위해 서로 다른 의미를 지닌 특징 간의 상관관계를 고려해야 함, 두 특징 간 나타나는 데이터의 방향성과 상관도를 나타낸 공분산 행렬 개념을 적용하여 정규 분포에서 특정 값이 얼마나 평균에서 멀리 있는지를 나타낸 거리
- 자카드 거리 : 비교 대상인 두 개의 객체를 특성들의 집합으로 간주하며 범주형 데이터에서 비유사성을 측정하는 지표

4) 군집분석의 종류

- 계층적 군집분석 : 계층화된 구조로 군집을 형성, 군집 수 명시가 필요하지 않고 덴드로그램을 통해 결과 표현을 시각화
    - 계층적 병합 군집화 : N개의 군집으로 시작하고, 가장 근접하고 유사한 두 개의 군집들이 1개 군집으로 병합, 가장 거리가 짧은 두 개의 군집들이 순차적으로 병합
    - 최단 연결법 : 군집과 군집/데이터 간의 거리 중 최단거리 값을 거리로 산정
    - 최장 연결법 : 군집과 군집/데이터 간의 거리 중 최장거리 값을 거리로 산정
    - 평균 연결법 : 군집과 군집/데이터 간의 거리의 평균거리 값을 거리로 산정
    - Ward 연결법 : 군집 내 편차들의 제곱합을 고려한 군집 내 거리를 기준
- 비계층적 군집분석(분할적 군집)
    -사전 군집 수로 표본을 나누며 레코드들을 정해진 군집에 할당
    - 적은 계산량으로 대규모 DB에서 처리가 유용
    - K-평균 군집 분석 : 군집들 내부의 분산을 최소화하여 각각의 사례를 군집들 중 하나에 할당
    - 밀도 기반 클러스터링(DBSCAN)
        - 개체들의 밀도 계산을 기반으로 밀접하게 분포된 개체들끼리 그루핑
        - 파라미터로 밀도계산 범위와 하나의 그룹으로 묶는 최소 개체수 필요
    - 이상치들은 충분한 고려없이 제외 가능하며 유형 간 밀도 차이가 뚜렷하지 않다면 밀도 기반 클러스터링을 대안으로 추천
    - 확률 분포 기반 클러스터링
        - 전체 데이터의 확률 분포가 가우시안 분포 조합으로 이뤄졌음을 가정하고, 각 분포에 속할 확률이 높은 데이터들 간 군집을 형성하는 방법
        - 개별 데이터가 정규 분포 상에서 어떤 분포에 속할지 더 높은 확률로 배정된 부문으로 군집화
        - 데이터가 정규 분포 조합 가설에 어긋나면 클러스터링이 부적절하게 될 수 있으며, 많은 계산량으로 인해 대용량 데이터 처리가 필요할 때는 적합하지 않음

5) 군집분석의 장단점

- 장점
    - 다양한 데이터 형태에 적용 가능
    - 특정 변수에 대한 정의가 필요하지 않는 적용이 용이한 탐색적 기법
- 단점
    - 초기 군집 수, 관측치간의 거리 등의 결정에 따라 결과가 바뀔 수 있음
    - 사전 주어진 목표가 없으므로 결과 해석이 어려움

## SECTION 02. 고급 분석기법

### 01. 범주형 자료분석

1) 범주형 자료분석의 통계적 정의

- 범주형 자료분석은 변수들이 이산형 변수일 때 주로 사용하는 분석
    - 범주형 변수를 다룰 때에는 일반적으로 그 빈도를 세서 표를 작성하게 됨
    - 만약 두 변수의 범주가 교차되어 있다면 이 표를 분할표라고 함
    - 분할표는 범주형 변수를 요약해서 표현하기에 가장 적당, 분할표는 범주형 변수를 요약해서 표현하기에 가장 적당, 분할표를 통해서 범주별 비교를 하고 분할표를 기반으로 범주형 변수의 독립성, 동질성 검정 등의 카이제곱 검정 수행
    - 분할표는 쉽지만 중요한 개념이며 로지스틱 회귀모형 등으로 대표되는 일반화 선형모형을 해석하는 과정에서도 사용

2) 자료의 분석

- 자료의 형태에 따른 범주형 자료 분석 방법

|독립변수|종속변수|분석방법|
|범주형|범주형|빈도분석, 카이제곱 검정, 로그선형모형|
|연속형|범주형|로지스틱 회귀분석|
|범주형|연속형|T검정(2그룹), 분산분석(2그룹 이상)|
|연속형|연속형|상관분석, 회귀분석|

- 분할표 : 범주형데이터가 각 변수에 따라서 통계표 형태로 정리되어 쓴 것
    - 차원 : 분할표의 구성에 관계된 변수의 수
    - 수준 : 범주형 변수가 가지는 범주의 수
        - 비율의 차이 (D = alpha1 - alpha2) : 범위는 -1~1 사이를 취하며 동질 또는 독립인 경우 D=0
        - 상대적 위험도 (RR = alpha1 / alpha2) : 범위는 0~무한대 사이를 취하며 동질 또는 독립인 경우 RR=1
        - 오즈비 (OR = {alpha1 / 1-alpha2} / {alpha2 / 1-alpha2}) : 범위는 0~무한대 사이를 취하며 동질 또는 독립인 경우 OR=1
- 빈도분석 : 질적자료를 대상으로 빈도와 비율을 계산할 때 쓰임, 데이터에 질적자료와 양적자료가 많을 때 질적자료를 대상으로 오류가 있는지 확인 가능
- 교차분석 또는 카이제곱검정 : 두 범주형 변수가 서로 상관이 있는지 독립인지를 판단하는 통계적 검정방법
- 로지스틱 회귀분석 : 분석하고자 하는 대상들이 두 집단 또는 그 이상의 집단으로 나누어진 경우 개발 관측치들이 어느 집단으로 분류될 수 있는지를 분석할 때 사용
- T 검정 : 독립변수가 범주형(두 개의 집단)이고 종속변수가 연속형인 경우 사용되는 검정방법으로 두 집단간의 평균 비교 등에 사용
- 분산분석 : 독립변수가 범주형(두 개 이상 집단)이고 종속변수가 연속형일 경우 사용되는 검정 방법으로 두 집단간의 분산 비교 등에 사용

### 02. 다변량분석

1) 다변량 분석

- 조사 중인 각 개인 혹은 대상물에 대한 다수의 측정치를 동시에 분석하는 모든 통계적 방법
- 많은 다변량분석 기법은 일변량분석과 이변량분석의 확장형태
- 통계적으로는 종속변수의 관계성을 고려해서 여러 개의 단변량분석을 동시에 수행하는 것

2) 용어

- 종속 기법 : 변수들을 종속변수와 독립변수로 구분하여 독립변수들이 종속변수에 미치는 영향력을 분석하는 기법
- 상호의존적 기법 : 분석할 변수들을 종속변수와 독립변수로 구분하지 않고 전체를 대상으로 하는 분석
- 명목 척도 : 단지 분류만을 위해 사용된 숫자로서 숫자 그 자체는 전혀 의미가 없는 측정단위
- 순위 척도 : 선호되는 순위를 나타낸 숫자로서 숫자 자체는 의미를 가지거나 간격이나 비율이 의미를 가지지 못하는 측정단위
- 등간 척도 : 측정된 숫자 자체와 숫자의 차이는 의미를 가지나 숫자의 비율은 의미를 가지지 못하는 측정단위
- 비율 척도 : 측정된 숫자와 그 간격이 의미를 가질 뿐만 아니라 숫자의 비율마저도 의미를 가지는 가장 높은 측정단위
- 정량적 자료 : 등간척도나 비율척도로 측정된 자료로서 양적자료 또는 모수화된 자료
- 비정량적 자료 : 명목척도나 순위척도로 측정된 자료로서 질적 자료 또는 비모수화된 자료라고도 함
- 변량 : 변수들이 연구자의 실험대상인 표본으로부터 수집한 자료 그대로를 나타내는 반면에 변량은 이러한 변수들을 일종의 통계적인 방법으로 가중치를 주어 변수들의 합의 형태로 나타낸 새로운 변수

3) 다변량분석기법의 종류

- 다중회귀분석
    - 하나의 계량적 종속변수와 하나 이상의 계량적 독립변수 간에 관련성이 있다고 가정되는 연구문제에 적합한 분석기법
    - 다수의 독립변수의 변화에 따른 종속변수의 변화 예측
    - 다중회귀분석을 통해 연구자는 회귀모형의 적합도를 분석할 수 있고, 독립변수들이 종속변수를 설명하는 정도, 종속변수에 대한 독립변수들의 상대적인 기여도를 파악할 수 있음
- 다변량분산분석, 다변량공분산분석
    - 다변량분산분석
        - 두 개 이상의 범주형 독립변수와 다수의 계량적 종속변수 간 관련성을 동시에 알아볼 때 이용되는 통계적 방법으로 일변량분산분석의 확장된 형태
        - 다변량분산분석은 두 개 이상의 계량적 종속변수에 대한 각 집단의 반응치의 분산에 대한 가설을 검증하는데 매우 유용
    - 다변량공분산분석 : 실험에서 통제되지 않은 독립변수들의 종속변수들에 대한 효과를 제거하기 위해 다변량분산분석과 함께 이용되는 방법으로는 그 절차는 이변량부분상관과 비슷
- 정준상관분석
    - 하나의 계량적 종속변수와 다수의 계량적 독립변수 간의 관련성을 조사하는 다중회귀분석을 논리적으로 확대시킨 것
    - 종속변수군과 독립변수군 간의 상관을 가장 크게하는 각 변수군의 선형조합을 찾아내는 일
    - 종속변수군과 독립변수군 간의 상관을 최대화하는 각 변수군의 가중치의 집합을 찾아내는 것
- 요인분석
    - 많은 수의 변수들 간 상호관련성을 분석하고, 이들 변수들을 어떤 공통 요인들로 설명하고자 할 때 이용되는 기법
    - 요인분석은 많은 수의 원래 변수들을 이보다 적은 수의 요인으로 요약하기 위한 분석기법
    - 요인분석은 주로 검사나 측정도구의 개발과정에서 측정도구의 타당성을 파악하기 위한 방법으로 많이 사용
    - 요인분석의 종류로는 연구자가 가설적인 요인을 설정하지 않고 얻어진 자료에 근거하여 경험적으로 요인의 구조를 파악하는 탐색적 요인분석과 연구자가 사전에 요인의 구조를 가설적으로 설정하고 이를 검증하는 확인적 요인분석이 있음
- 군집분석
    - 집단에 과한 사전정보가 전혀 없는 각 표본에 대하여 그 분류체계를 찾을 때, 다시 말해 각 표본을 표본들 간의 유사성에 기초에 한 집단에 분류시키고자 할 때 사용되는 기법으로 판별분석과 달리 군집분석에서는 집단이 사전에 정의되어 있지 않음
    - 단계
        - 첫째로 몇 개의 집단이 존재하는가를 알아보기 위해 각 표본들 간의 유사성 혹은 연관성 조사
        - 둘째로 첫 번째 단계에서 정의된 집단에 어떤 표본을 분류해 넣거나 혹은 그 소속을 예측
        - 두 번째 단계에서는 군집기법에 의해 나탄 그룹들에 대해 판별분석 적용
- 다중판별분석
    - 종속변수가 남/녀와 같이 두 개의 범주로 나누어져 있거나, 상/중/하와 같이 두 개 이상의 범주로 나누어져 있을 경우, 즉 종속변수가 비계량적 변수일 경우 다중판별분석 이용됨
    - 다중회귀분석과 같이 독립변수는 계량적 변수로 이루어져 있음, 판별분석은 각 표본이 여러 개의 범주를 가진 종속변수에 기초한 여러 개의 집단으로 분류될 때 적합함
    - 다중판별분석의 주목적은 집단 간의 차이를 판별하며, 어떤 사례가 여러 개의 계량적 독립변수에 기초하여 특정 집단에 속할 가능성을 예측하는 데 있음
- 다차원척도법
    - 다차원 관측값 또는 개체들 간의 거리 또는 비유사성을 이용하여 개체들을 원래의 차원보다 낮은 차원의 공간상에 위치시켜 개체들 사이의 구조 또는 관계를 쉽게 파악하고자 하는데 목적
    - 차원의 축소와 개체들의 상대적 위치 등을 통해 개체들 사이의 관계를 쉽게 파악하고자 하는데 목적이 있으며, 공간적 배열에 대한 주관적인 해석에 중점
    - 응답자들이 경쟁관광지와 비교하여 자기 지역 관광상품에 대한 이미지를 어떻게 지각하는지를 알 수 있으며, 이를 통해 자기 지역의 차별화 방안을 구체화할 수 있음

### 03. 시계열분석

1) 시계열분석

- 시계열 자료를 분석하고 여러 변수들 간의 인과관계를 분석하는 방법론
- 경제학에서도 매우 많이 쓰이는 방법론으로 계량경제학이나 금융, 거시경제 분석에 사용
- 시계열자료의 구분, 정상성 구분에 다른 분석모형 그리고 회귀분석에 대해서 이해할 수 있어야 함

2) 시계열 자료 : 시간의 흐름에 따라서 관측되는 자료(데이터)를 지칭

- 시계열 자료를 이용하여 미래에 대해 예측 또는 제어하는 것이 주 이용목적
- 이산 시계열 : 관측값들이 이산적인 형태로 분리되어 존재
- 연속 시계열 : 관측값들이 연속적으로 연결된 형태의 자료
- 시차 : 한 관측시점과 다른 관측시점 사이의 간격

3) 시계열자료의 성분

- 불규칙 성분 : 시간에 따른 규칙적인 움직임이 없는 랜덤하게 변화하는 변동성분
- 체계적 성분 : 시간에 따른 규칙이 존재하는 변동성분
    - 추세성분 : 관측 값이 지속적 증가 또는 감소하는 추세 포함
    - 계절성분 : 주기적 성분에 의한 변동을 가지는 형태
    - 순환성분 : 주기적 변화를 가지나 계절적인 것이 아닌 주기가 긴 변동을 가지는 형태
    - 복합성분 : 추세성분과 계절성분을 동시에 가지는 경우
    - 자기상관성 : 시계열 데이터에서 시차값들 사이에 선형관계를 보이는 것
    - 백색잡음 : 자기 상관성이 없는 시계열 데이터를 지칭하며 아무런 패턴이 남아있지 않은 무작위한 움직임(진동)을 보이는 데이터

4) 정상성 : 시계열 데이터가 평균과 분산이 일정한 경우, 일반적으로 시계열 데이터가 정상성을 가지면 분석이 용이한 형태로 볼 수 있음

- 평균이 일정
    - 모든 시점에 대해 평균 일정
    - 시계열 데이터가 평균이 일정하지 않으면 차분을 통해 정상성을 가지도록 함
- 분산이 일정
    - 모든 시점에서 분산 일정
    - 시계열 데이터가 분산이 일정하지 않으면 변환을 통해 정상성을 가지도록 함
- 공분산의 경우도 단지 시차에만 의존하며 특정시점에는 의존하지 않음
- 정상성을 가지는 시계열 자료의 특징
    - 정상시계열은 어떤 시점저에서 평균분산 그리고 특정시차가 일저한 경우의 공분산이 동일
    - 정상시계열은 항상 평균회귀 경향이 있으며 평균 주변의 변동은 대체로 일정학 폭을 가짐
    - 정상성을 가지는 시계열의 경우는 특정기간에서 얻은 정보를 다른 시기에서도 사용이 가능한 정보로 일반화가 가능하지만 아닌 경우는 일반화가 힘듦

5) 시계열자료의 분석 방법

- 단순방법
    - 이동평균법
        - 과거로부터 현재까지 시계열 자료를 대상으로 일정기간(관측기간)을 시계열을 이동하면서 평균을 계산하는 방법
        - 이릍 통해 추세를 파악하며 시계열의 다음기간을 예측하는데 사용됨
        - 간단한 방법으로 추세의 판단이 가능, 특히 데이터가 많고 안정된 패턴을 보이는 경우 추세의 판단의 효용성이 높음
        - 데이터가 뚜렷한 추세가 있거나 불규칙 움직임이 적은 경우는 n을 작게 사용하고 반대의 경우는 n을 늘려서 사용함
    - 지수평활법
        - 이동평균법과 달리 관찰기간의 제한이 없이 모든 시계열 데이터를 사용하며 최근 시계열에 더 많은 가중치를 주며 추세를 찾는 방법
        - 단기간에 발생하는 불규칙 변동을 평활하는데 주로 사용하며 지수평활계수의 효과로 과거 데이터일수록 가중치를 적게 배당하며 구함
        - 중기 이상의 예측에 주로 사용, 단 단순지수평활법의 경우 장기 추세나 계절성이 포함된 시계열 데이터에는 부적합
        - 지수평활계수가 작으면 지엽적 변화에 민가하고, 반대로 크면 지엽적 변화에 둔감해지는 효과
    - 분해법
        - 분해법은 시계열자료의 성분 분류대로 시계열 데이터를 분해하는 방법
        - 시계열이 체계적 성분과 불규칙적 성분으로 이루어져 있다는 가정하에 체계적 성분을 시계열로부터 분리하여 분석/예측을 목적으로 하는 기법
        - 분해법사용의 목적은 시계열 자료를 분해된 성분별로 해석하는 데 있음, 시계열자료로부터 계절적 특성, 추세/순환 성분을 분리하여 시계열의 장기적 추이를 분석하며, 불규칙성분으로부터 불규칙성이 발생한 시점을 찾는 것
        - 계절조정 자료 제공, 체계적 성분 중 계절 성분은 종종 시계열의 장기적 변화를 살피는데 방해가 됨, 많은 공공기관에서 시계열 자료는 원자료에서 계절성분을 뺀 자료 제공, 이러한 자료를 계절조정된 시계열 자료라고 함, 계절조정된 시계열 자료를 만들기 위해서는 우선 원자료로부터 계절 성분을 분리해야 함
- 모형에 의한 벙법
    - 자기회귀모형 : 과거의 패턴이 지속된다면 시계열 데이터 관측치 Xt는 과거 관측치 x(t-1), X(t-2), ..., X(t-p)에 의해 예측할 수 있을 것임, 어느 정도의 멀리 있는 과거 관측치까지 이용할 것인지에 대한 판단이 중요, 백색잡음은 오차항을 의미
    - 자기회귀이동평균모형
    - 자기회귀누적이동평균모형

### 04. 베이즈 기법

1) 베이즈 추론

- 베이즈 추론은 통계적 추론의 한 방법으로, 추론 대상의 사전 확률과 추가적인 정보를 통해 해당 대상의 사후 확률을 추론하는 방법
- 확률론적 의미해석(조건부 확률)
    - 추론의 대상인 사건을 'A'라고 할 때 사전 확률 P(A)가 주어지고 'A'와 관계된 B가 있을 때 조건부 확률 P(B|A)가 주어진다면 조건부 확률식에 의해 P(B|A) = P(A 교집합 B) / P(A)이고 여기서 P(B|A)P(A) = P(A 교집합 B)이므로 사후 확률 P(A|B) = P(A 교집합 B) / P(B) = P(B|A)P(A) / P(B)로 구할 수 있는데 이를 베이즈 정리라고 함
    - 사전 확률 P(A)는 결과가 나타나기 전에 결정되어 있는 원인 A의 확률
    - P(B|A)는 우도 확률로 원인 A가 발생하였다고 했을 때 관계된 B가 발생할 조건부 확률
    - 사후 확률 P(A|B)는 B가 발생 시 조건하에 A가 발생하는 확률
- 베이즈 기법의 개념
    - 베이즈 확률에는 두 가지 관점이 있는데 그 하나는 객관적 관점으로 베이즈 통계의 법칙은 이성적, 보편적으로 증명될 수 있으며 논리의 확장으로 설명될 수 있다는 것, 주관주의 확률 이론의 관점으로 보면 지식의 상태는 개인적인 믿음의 정도로 측정 가능
    - 많은 현대적 기계 학습 방법은 객관적 베이즈 원리에 따라 만들어짐

2) 베이즈 기법 적용

- 머신러닝에서의 베이즈 확률모델을 적용하는 원리는 크게 회귀분석모델과 분류에 적용을 나누어서 정리
- 회귀분석모델에서 베이즈 기법의 적용
    - 선형회귀분석모델 : 머신러닝에서 목표로 하는 것은 독립변수와 종속변수의 관계에 대해 추론하는 것, 즉 추정치와 실제의 차이를 최소화하는 것이 목표
    - 기존 머신러닝의 방법 : 머신러닝은 경사하강법과 같은 알고리즘을 통해 점진적으로 학습하여 매개변수를 찾아감
    - 베이지안 확률론의 적용개념
- 분류에서 베이즈 기법의 적용
    - 나이브 베이즈 분류 : 특성들 사이의 독립을 가정하는 베이즈 정리를 적용한 확률 분류기를 지칭
    - 나이브 베이즈의 특성
        - 분류기를 만들 수 있는 간단한 기술로서 단일 알고리즘을 통한 훈련이 아닌 일반적인 원칙에 근거한 여러 알고리즘들을 이용하여 훈련
        - 모든 나이브 베이즈 분류기는 공통적으로 모든 특성 값이 서로 독립임을 가정
    - 나이브 베이즈의 장점
        - 일부 확률모델에서 나이브 베이즈 분류는 지도 학습 환경에서 매우 효율적으로 훈련될 수 있음
        - 분류에 필요한 파라미터를 추정하기 위한 트레이닝 데이터의 양이 매우 적음
        - 간단한 디자인과 단순한 가정에도 불구하고, 나이브 베이즈 분류는 많은 복잡한 실제 상황에서 잘 작동
    - 나이브 베이즈 분류기의 생성(확률모델)
    - 이벤트 모델
        - 클래스의 사전확률은 클래스간의 동일확률을 가정하여 계산할 수 있고, 또한 트레이닝 셋으로부터 클래스의 확률의 추정치를 계산 가능
        - 특성의 분포에 대한 모수들을 추정하기 위해서는 트레이닝 셋의 특성들을 위한 비모수 모델이나 분포를 가정 또는 생성
        - 특성의 분포에 대한 여러 가정들은 나이브 베이즈 분류 이벤트 모델이라고 불림, 문서 분류에서 발생하는 것과 같은 이산적인 특성은 다항 분포와 베르누이 분포가 그 모델로 인기가 있음, 이러한 가정들로 인해 모델을 선택하는데 있어서 혼동이 발생하는 경우도 종종 발생
    - 나이브 베이즈 분류의 적용 : 분류기반의 머신러닝 적용을 하는데 광범위하게 사용되며 예시로 문서분류 등이 있음

### 05. 딥러닝 분석

1) 딥러닝 분석의 개념

- 인공신경망
    - 기계학습과 인지과학에서 생물학의 신경망을 통해 영감을 얻은 통계학적 학습 알고리즘
    - 시냅스의 결합으로 네트워크를 형성한 인공 뉴런(노드)이 학습을 통해 시냅스의 결합 세기를 변화시켜, 문제 해결 능력을 가지는 모델 전반을 가리킴
    - 인공신경망의 문제점
        - 계산속도의 저하 : 1980년대 컴퓨터의 연산수준이 해당알고리즘 수행의 최적 수준까지 발달하지 못함
        - 초기치의 의존성 : 최초 시작점의 선택에 따라 수렴, 발산, 진동 등 다양한 형태로 결과가 바뀌는 문제 발생
        - 과적합 문제 : 트레이닝 셋에만 최적화되어 실제 테스트와 예상 결과의 괴리가 발생
- 딥러닝
    - 여러 비선형 변환기법의 조합을 통해 높은 수준의 추상화를 시도하는 기계 학습 알고리즘의 집합으로 정의
    - 큰 틀에서 사람의 사고방식을 컴퓨터에게 가르치는 기계학습의 한 분야
    - 인공신경망의 단점 등이 극복되면서 재조명되고 부각된 기계학습
- 딥러닝의 원리 : 기존에는 신명망의 학습수준을 높이기 위해 하나의 은닉층에 은닉노드를 3개가 아니라 10개, 100개 이런 식으로 동일 레이어 내 수직으로 쭉 늘려놓기만 했었는데, 은닉층 자체를 여러 개로 만들어 여러 단계를 거치도록 신경망을 구성하였더니 정확도가 훨씬 향상됨
    - 노드 : 신경계 뉴런, 가중치와 입력값으로 활성함수를 통해 다음 노드로 전달
    - 가중치 : 신경망 시냅스, 노드와의 연결계수
        - 활성함수 : 임계값을 이용, 노드의 활성화 여부를 결정, 생물학적 뉴런에서 입력 신호가 일정 크기 이상일 때만 신호를 전달하는 매커니즘을 모방한 함수
        - 시그모이드, 탄젠트 쌍곡선함수, 정류선형유닛함수 등이 사용

2) 딥러닝 분석 알고리즘

- 심층 신경망
    - 입력층과 출력층 사엥 여러 개의 은닉층들로 이뤄진 인공 신경망
    - 일반적인 인공신경망과 마찬가지로 복잡한 비선형 관계들을 모델링 가능
- 활성곱 신경망
    - 최소한의 전처리를 사용하도록 설계된 다계층 퍼셉트론의 한 종류
    - 하나 또는 여러 개의 합성곱 계층과 그 위에 올려진 일반적인 인공 신경망 계층들로 이루어져 있으며, 가중치와 통합 계층들을 추가로 활용, 이러한 구조 덕분에 CNN은 2차원 구조의 입력 데이털르 충분히 활용 가능
    - CNN은 2차원 구조의 입력 데이터를 충분히 활용 가능, 다른 딥러닝 구조들과 비교해서, CNN은 영상, 음성 분야 모두에서 좋은 성능
    - CNN은 오차 역전파를 통해 훈련 가능, CNN은 다른 순방향 인공신경망 기법들보다 쉽게 훈련되는 편이고 적은 수의 매개변수를 사용한다는 이점이 있음
- 파생 알고리즘
    - 합성곱 심층 신뢰 신경망 : CNN과 심층 신뢰 신경망의 결합으로 만들어진 알고리즘으로 영상과 신호처리분야에서 많이 사용
- 순환 신경망
    - 인공신경망을 구성하는 유닛 사이의 연결이 순환적 구조를 갖는 신경망
    - 순환 신경망은 순방향 신경망과 달리, 임의의 입력을 처리하기 위해 신경망 내부의 메모리 활용 가능
    - 순환 신경망은 필기체 인식과 같은 분야에 활용되고 있고, 높은 인식률을 나타냄
    - 순환 신경망은 시퀀스 데이터를 모델링하기 위해 등장, 순환 신경망이 기존의 뉴럴 네트워크와 다른 점은 '기억'을 갖고 있다는 점인데, 네트워크의 기억은 지금가지의 입력 데이터를 요약한 정보
        - 새로운 입력이 들어올 때마다 네트워크는 자신의 기억을 조금씩 수정, 결국 입력을 모두 처리하고 난 후 네트워크에 남겨진 기억은 시퀀스 전체를 요약하는 정보
        - 사람이 시퀀스를 처리하는 방식과 비슷
    - 파생 알고리즘
- 심층 신뢰 신경망
    - 기계학습에서 사용되는 그래프 생성 모형
    - 딥러닝에서는 잠재변수와 다중계층으로 이루어진 심층 신경망 의미, 계층 간에 연결이 있지만 계층 내의 유닛 간에는 연결이 없다는 특징
    - 선행학습을 통해 초기 가중치를 학습한 후 역전파 혹은 다른 판별 알고리즘을 통해 가중치의 미조정 가능, 이러한 특성은 훈련용 데이터가 적을 때 유용
    - 선행학습된 가중치 초기값은 임의로 설정된 가중치 초기값에 비해 최적의 가중치에 가깝게 되고 이는 미조정 단계의 성능과 속도향상을 가능하게 함

### 06. 비정형 데이터 분석

1) 비정형 데이터

- 데이터 세트가 아닌 하나의 데이터가 수집 데이터로 객체화 되어 있음
- 언어 분석이 가능한 텍스트 데이터나 이미지, 동영상 같은 멀티미디어 데이터가 대표적인 비정형 데이터
- 웹에 존재하는 데이터의 경우 html 형태로 존재하여 반정형 데이터로 구분할 수도 있지만, 특정한 경우 텍스트 마이닝을 통해 데이터를 수집하는 경우도 존재하므로 명확한 구분은 어려움

2) 비정형 데이터 분석

- 빅데이터 환경에서 80% 이상의 데이터가 비정형 데이터이므로 이를 분석하는 것의 중요도는 높음, 분석하는 기법들은 상당히 제한적이며 대부분의 경우가 상식적인 수준의 연관관계 추출에 그칠 가능성이 높음
- 비정형 데이터의 분석의 기본 원리
    - 비정형 데이터의 내용 파악과 비정형 데이터 속 패턴 발견을 위해 데이터 마이낭, 텍스트 분석, 비표준 텍스트 분석 등과 같은 다양한 기법 사용
    - 비정형 데이터를 정련과정을 통해 정형데이터로 만든 후, 분류, 군집화, 회귀 분석, 요약, 이상감지 분석 등의 데이터 마이닝을 통해 의미있는 정보를 발굴 가능
- 데이터 마이닝
    - 대규모로 저장된 데이터 안에서 체계적이고 자동적으로 통계적 규칙이나 패턴을 분서갛여 가치있는 정보를 추출하는 과정
    - 통계학쪽에서 발전한 탐색적자료분석, 가설 검정, 다변량 분석, 시계열 분석, 일반선형모형 등의 방법론과 데이터베이스 쪽에서 발전한 OLAP, 인공지능 진영에서 발전한 SOM, 신경망, 전문가 시스템 등의 기술적인 방법론이 쓰임
    - 적용분야
        - 분야 : 일정한 집단에 대한 특정 정의를 통해 분류 및 구분을 추론
        - 군집화 : 구체적인 특성을 공유하는 군집을 찾음, 군집화는 미리 정의된 특성에 대한 정보를 가지지 않는다는 점에서 분류와 다름
        - 연관성 : 동시에 발생한 사건간의 관계 정의
        - 연속성 : 특정 기간에 걸쳐 발생하는 관계 규명, 기간의 특성을 제외하면 연관성 분석과 유사
        - 예측 : 대용량 데이터집합 내의 패턴을 기반으로 미래 예측
- 텍스트 마이닝 : 전통적인 데이터 마이닝의 한계를 벗어난 방법으로 인간의 언어로 이루어진 비정형 텍스트 데이터들을 자연어 처리방식을 이용하여 대규모 문서에서 정보 추출, 연계성 파악, 분류 및 군집화, 요약 등을 통해 데이터의 숨겨진 의미를 발견하는 기법
    - 자연어 처리
        - 인간의 언어 현상을 컴퓨터와 같은 기계를 이용해서 묘사할 수 있도록 연구하고 이를 구현하는 인공지능의 주요 분야 중 하나
        - 연구대상이 언어이기 때문에 언어 자체를 연구하는 언어학, 언어현상의 내적 기재를 탐구하는 언어인지 과학과 연관
        - 구현을 위해 수학, 통계적 도구를 많이 활용하며 특히 기계학습 도구를 많이 사용하는 대표적인 분야
- 웹 마이닝
    - 데이터 마이닝 기술의 응용분야로 인터넷을 통해 웹자원으로부터 의미있는 패턴, 프로파일, 추세 등을 발견하는 것
    - 데이터의 속성이 반정형이거나 비정혀이고, 링크 구조를 가지고 있기 때문에 전통적인 데이터 마이닝 기술에 추가적인 분석기법이 필요
    - 활용분야에는 정보필터링, 경쟁자와 특허, 그리고 기술개발 등의 감시, 이용도 분석을 위한 웹 액세스 로그의 마이닝, 브라우징 지원 등이 있음
- 오피니언 마이닝
    - 어떤 사안이나 인물, 이슈, 이벤트 등과 관련된 원천 데이터에서 의견이나 평가, 태도, 감정 등과 같은 주관적인 정보를 식별하고 추출하는 것
    - 사람들의 주관적인 의견을 통계, 수치화하여 객관적인 정보로 바꾸는 기술, 어떤 시안이나 인물에 대한 사람들의 의견뿐만 아니라 감정과 태도로 분석하기 때문에 감정 분석이라고도 불림
    - 오피니언 마이닝도 분석 대상이 텍스트이므로 텍스트 마이닝에서 활용하는 자연어 처리 방법을 사용하며, 주된 분석 대상은 포털 게시판, 블록, 쇼핑몰과 같은 대규모의 웹 문서
- 리얼리티 마이닝
    - 사람들이 매일 사용하는 스마트폰 등의 기계나 모션센서 등의 행동에서 비정형 데이터를 추출하는 방법
    - 리얼리티 마이닝에서 수집하고자 하는 데이터는 통화/메시징 등의 커뮤니티케이션 데이터, GPS/WIFI 등의 위치 데이터임, 이를 통해 사회적 행위를 마이닝하고 사용자 행동 모델링이나 라이프 로그도 얻어내는 것을 목표로 함

### 07. 앙상블 분석

1) 정의 : 주어진 자료로부터 여러 개의 학습 모형을 만든 후 학습 모형들을 조합하여 하나의 최종 모형을 만드는 개념

- 약학습기 : 무작위 선정이 아닌 성공확률이 높은, 즉 오차율이 일정 이하인 학습 규칙, 가능성이 있는 다양한 복수의 학습 규칙
- 강학습기 : 약학습기로부터 만들어 내는 강력한 학습 규칙

2) 이해

- 동일한 학습 알고리즘을 이용할 때, 앙상블 분석이 한 개의 단일학습기에 의한 분석보다는 더 나은 분석성능을 이끌어 낼 수 있음
- 앙상블 기법은 다양한 약학습기를 통해 강학습기를 만들어가는 과정

3) 종류

- 보팅
    - 서로 다른 여러 학습 모델을 조합해서 사용
    - 배깅은 같은 알고리즘 내에서 다른 표본 데이터 조합 사용
    - 하드 보팅 : 결과물에 대한 최종 값을 투표햇거 결정하는 방식
    - 소프트 보팅 : 최종 결과물이 나올 확률 값을 다 더해서 최종 결과물에 대한 각각의 확률을 구한 뒤 최종 값을 도출해내는 방법
- 부스팅
    - 가중치를 활용하여 연속적인 약학습기를 생성하고 이를 통해 강학습기를 만드는 방법
    - 다른 앙상블 기법과 가장 다른 점 중 하나는 바로 순차적인 학습을 하며 가중치를 부여해서 오차를 보완해 나간다는 점, 순차적이기 때문에 병렬 처리에 어려움이 있고, 그렇기 때문에 다른 앙상블 대비 학습 시간이 오래 거림
- 배깅
    - 샘플을 여러 번 뽑아 각 모델을 학습시켜 결과물을 집계하는 방법
    - 데이터로부터 부트스트랩을 함, 부트스트랩한 데이터로 모델의 학습을 거침, 학습된 모델의 결과를 집계하여 최종 결과값 구함
        - 범주형 자료는 투표 방식으로 결과를 집계하며, 연속형 자료는 평균으로 집계
        - 범주형 자료일 때, 투표 방식으로 한다는 것은 전체 모델에서 예측한 값 중 가장 많은 값을 최종 예측값으로 선정하는 것
        - 평균으로 집계한다는 것은 말 그 대로 각각의 결정 트리 모델이 예측한 값에 평균을 취해 최종 배깅 모델의 예측값을 결정한다는 것
    - 배깅은 간단하면서도 강력한 방법이며 배깅을 활용한 모델이 바로 랜덤 포레스트

### 08. 비모수 통계

1) 모수의 정의

- 모수는 수학과 통계학에서 어떠한 시스템이나 함수의 특정한 성질을 나타내는 변수, 일반적으로는 theta라고 표현되며, 다른 표시는 각각 독특한 뜻을 지님
- 함수의 수치를 정해진 변역에서 구하거나 시스템의 반응을 결정할 때, 독립변수는 변하지만 매개변수는 일정
- 다른 매개변수를 이용해 함수의 다른 수치를 다시 구하거나 시스템의 다른 반응을 볼 수 있음

2) 비모수 통계의 개념

- 통계학에서 모수에 대한 가정을 전헤로 하지 않고 모집단의 형태에 관계없이 주어진 데이터에서 직접 확률을 계산하여 통계학적 감정을 하는 분석
- 상대적으로 모수 통계는 데이터의 분포를 알거나 모수 등을 안다고 가정하고 통계적 검정 및 추론을 하는 것

3) 비모수 통계법의 사용조건

- 자료가 나타내는 모집단의 형성이 정규분포가 아닐 때
- 자료가 나타내는 현상이 정규분포로 적절히 변환되지 못할 때
- 자료의 표본이 적을 대
- 자룓르이 서로 독립적일 때
- 변인의 척도가 명명척도나 서열척도일 때
    - 명명척도(범주형척도) : 분류를 위한 척도
    - 서열처도 : 명명척도에서 서열(우열)적 정보를 가지는 숫자 포함 척도

4) 비모수 통계의 특징

- 가정을 만족시키지 못한 상태에서 그대로 모수 통계분석을 함으로써 발생할 수 있는 오류를 줄일 수 있음
- 질적척도로 측정된 자료로 분석 가능
- 비교적 신속하고 쉽게 통계량을 구할 수 있으며 결과에 대한 해석 및 이해 또한 쉬움
- 많은 표본을 추출하기 어려운 경우에 사용하기 적합함

5) 비모수적 통계 검정법

- 부호검정 : 관측치들 간에 같다 혹은 크거나 작다라는 주장이 사실인지 아닌지를 검정
- 월콕슨 부호순위 검정 : 크가나 작음을 나타내는 부호 뿐만 아니라 관측치 간 차이의 크기 순위까지를 고려하여 검정
- 만 위트니 검정 : 두 집단 간의 중심위치를 비교하기 위하여 사용하는 검정 방법
- 크루스칼-왈리스 검정 : 3개 이상 집단의 중앙값 차이 검정