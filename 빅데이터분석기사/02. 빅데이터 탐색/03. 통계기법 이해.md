# 데이터 수집 및 저장 계획

## SECTION 01. 데이터 수집 및 전환

### 01. 데이터 수집

1) 데이터 수집 : 데이터 처리 시스템에 들어갈 데이터를 모으는 과정으로 여러 장소에 있는 데이터를 한 곳으로 모으는 것

2) 비즈니스 도메인과 원천 데이터 정보 수집

- 비즈니스 도메인 정보
    - 비즈니스 모델, 비즈니스 용어집, 비즈니스 프로세스로부터 관련 정보를 습득
    - 도메인 전문가 인터뷰를 통해 데이터의 종류, 유형, 특징 정보를 습득
- 원천 데이터 정보 : 데이터 분석에 필요한 대상 원천 데이터의 수집 가능성, 데이터의 보안, 정확성을 탐색하고, 데이터 수집의 난이도, 수집 비용 등 기초 자료 수집 가능
    - 데이터의 수집 가능성 : 원천 데이터 수집의 용이성과 데이터 발생 빈도를 탐색하고, 데이터 활용에 있어서 전처리 및 후처리 비용을 대략 산정
    - 데이터의 보안 : 수집 대상 데이터의 개인정보 포함 여부, 지적 재산권 존재 여부를 판단하여 데이터 분석 시 발생할 수 있는 문제 예방
    - 데이터 정확성 : 데이터 분석 목적에 맞는 적절한 데이터 항목이 존재하고, 적절한 데이터 품질이 확보될 수 있는지 탐색
    - 수집 난이도 : 원천 데이터의 존재 위치, 데이터의 유형, 데이터 수집 용량, 구축비용, 정제 과정의 복잡성을 고려하여 데이터 탐색
    - 수집 비용 : 데이터를 수집하기 위해 발생할 수 있는 데이터 획득 비용을 산정

2) 내, 외부 데이터 수집

- 데이터의 종류
    - 내부 데이터는 조직 내부의 서비스 시스템, 네트워크 및 서버 장비, 마케팅 관련 시스템 등으로부터 생성되는 데이터
    - 외부데이터는 다양한 소셜 데이터, 특정 기관 데이터, M2M 데이터, LOD 등올 나눌 수 있음
- 데이터의 수집 주기
    - 내부 데이터는 조직 내부에서 습득할 수 있는 데이터로 실시간으로 수집하여 분석할 수 있도록 함
    - 외부 데이터는 일괄 수집으로 끝날지, 일정 주기로 데이터를 수집할지를 결정하여 수집 데이터 관리 정책을 정해야 함
- 데이터의 수집 방법
    - 내부 데이터는 분석에 적합한 정형화된 형식으로 수집되기 때문에 가공에 많은 노력을 기울이지 않아도 됨
    - 외부 데이터는 분석 목표에 맞는 데이터를 탐색, 수집하고, 분석 목표에 맞게 수집 데이터를 변환하는 노력이 필요

3) 데이터 수집 기술

- 데이터 유형별 데이터 수집 기술 : 수집 시스템 사양 설계를 위해 수집 데이터 유형을 파악하여 그에 맞는 수집 기술을 선정 가능
    - 정형 데이터
        - ETL : 수집 대상 데이터를 추출 및 가공하여 데이터 웨어하우스에 저장하는 기술
        - FTP : TCP/IP나 UDP 프로토콜을 통해 원격지 시스템으로부터 파일을 송수신하는 기술
        - API : 솔루션 제조사 및 3rd party 소프트웨어로 제공되는 도구로, 시스템 간 연동을 통해 실시간으로 데이터를 수신할 수 있도록 기능을 제공하는 인터페이스
        - DBToDB : 데이터베이스 관리시스템 간 데이터를 동기화 또는 전송하는 방법
        - 스쿱 : 관계형 데이터베이스와 하둡 간 데이터를 전송하는 방법
    - 비정형 데이터
        - 크롤링 : 인터넷상에서 제공되는 다양한 웹 사이트로부터 소셜 네트워크 정보, 뉴스, 게시판 등으로부터 웹 문서 및 정보를 수집하는 기술
        - RSS : 블로그, 뉴스, 쇼핑몰 등의 웹 사이트에 게시된 새로운 글을 공유하기 위해 XML 기반으로 정보를 배포하는 프로토콜
        - Open API : 응용 프로그램을 통해 실시간으로 데이터를 수신할 수 있도록 공개된 API
        - 척와 : 분산 시스템으로부터 데이터를 수집, 하둡 파일 시스템에 저장 실시간으로 분석할 수 있는 기능 제공
        - 카프카 : 대용량 실시간 로그처리를 위한 분산 스트리밍 플랫폼 기술
    - 반정형 데이터
        - 플럼 : 분산 환경에서 대량의 로그 데이터를 수집 전송하고 분석하는 기능 제공
        - 스크라이브 : 다수의 수집 대상 서버로부터 실시간으로 데이터를 수집, 분산 시스템에 데이터를 저장하는 기능 제공
        - 센싱 : 센서로부터 수집 및 생성된 데이터를 네트워크를 통해 활용하여 수집하는 기능 제공
        - 스트리밍 : 네트워크를 통해 센서 데이터 및 오디오, 비디오 등의 미디어 데이터를 실시간으로 수집하는 기술
- ETL
    - 하나 이상의 데이터 소스로부터 데이터 웨어하우스,데이터 마트, 데이터 통합, 데이터 이동 등 다양한 응용시스템을 위한 데이터 구축에 필요한 핵심 기술
    - ETL은 추출, 변환, 적재의 3단계 프로세스로 구성
- FTP
    - 대량의 파일(데이터)를 네트워크를 통해 주고받을 때 사용되는 파일 전송 서비스
    - 인터넷을 통한 파일 송수신 만을 위해 만들어진 프로토콜이기 때문에 동작 방식이 단순하고 직관적이며, 파일을 빠른 속도로 한꺼번에 주고받을 수 있음
    - 인터넷 프로토콜인 TCP / IP 위에서 동작
    - 서버와 클라이언트를 먼저 연결하고 이후에 데이터 파일 전송
    - FTP 서비스를 제공하는 서버와 접속하는 클라이언트 사이에 두 개의 연결을 생성 (데이터 제어 연결과 데이터 전송 연결)
    - 사용자 계정 및 암호 등의 정보나 파일 전송 명령 및 결과 등은 데이터 제어 연결에서, 이후 실제 파일 송수신 작업(올리기, 내려받기)은 데이터 전송 연결에서 처리
    - 데이터 제어 연결을 위해서 21번 포트, 데이터 전송 연결을 위해서 20번 포트 사용
- 정형 데이터 수집을 위한 아파치 스쿱 기술
    - 관계형 데이터 스토어 간에 대량 데이터를 효과적으로 전송하기 위해 구현된 도구
    - 커넥터를 사용하여 MySQL, Oracle, MS SQL 등 관계형 데이터베이스의 데이터를 하둡 파일시스템으로 데이터 수집
    - 관계형 데이터베이스에서 가져온 데이터들을 하둡 맵리듀스로 변환하고, 변환된 데이터들을 다시 관계형 데이터베이스로 내보낼 수 있음
    - 데이터 가져오기/내보내기 과정을 맵리듀스를 통해 처리하기 때문에 병렬처리가 가능하고 장애에도 강한 특징
    - 모든 적재 과정을 자동화하고 병렬처리 방식으로 작업
- 로그/센서 데이터 수집을 위한 아파치 플럼 기술
    - 아파치 플럼은 대용량의 로그 데이터를 효과적으로 수집, 집계, 이동시키는 신뢰성 있는 분산 서비스를 제공하는 솔루션
    - 스트리밍 데이터 흐름에 기반을 둔 간단하고 유연한 구조
    - 플럼에서 하나의 에이전트는 소스, 채널, 싱크로 구성 : 소스는 웹서버, 로그데이터서버 등 원시데이터소스와 연결되며, 소스로부터 들어오는 데이터는 큐의 구조를 갖는 채널로 들어간 후, 싱크를 통해 목표 시스템으로 전달
    - 플럼은 로그 데이터 수집과 네트워크 트래픽 데이터, 소셜 미디어 데이터, 이메일 메시지 등 대량의 이벤트 데이터 전송을 위해 사용
- 웹 및 소셜 데이터 수집을 위한 스크래피 기술
    - 웹사이트를 크롤링하고 구조화된 데이터를 수집하는 도구
    - API를 이용하여 데이터를 추출할 수 있어, 범용 웹크롤러로 사용될 수 있음
    - 파이썬 기반의 프레임워크로 스크랩 과정이 단순하며 한 번에 여러 페이지를 불러오기 수월

### 02. 데이터 유형 및 속성 파악

1) 데이터 수집 세부 계획 작성 : 데이터 유형, 위치, 크기, 보관방식, 수집주기, 확보비용, 데이터 이관 절차를 조사하여 세부 계획서 작성

2) 데이터 위치 및 비용 : 수집 데이터의 원천에 따라 내부 데이터와 외부 데이터로 구분하고 여러 요소를 고려하여 비용 산정

3) 수집되는 데이터 형태

- HTML
    - 웹 페이지를 만들 때 사용되는 문서 형식
    - 텍스트, 태그, 스크립트로 구성
- XML
    - 데이터를 표현하기 위해서 태그를 사용하는 언어
    - 엘리먼트, 속성, 처리명령, 엔티티, 주석, CDATA 섹션으로 구성
- JSON : 자바스크립트를 위해 객체 형식으로 자료를 표현하는 문서 형식이며, 경량의 데이터 교환 방식

4) 데이터 저장 방식

- 파일 시스템 : 데이터를 읽고, 쓰고, 찾기 위해 일정한 규칙으로 파일에 이름을 명명하고 파일의 위치를 지정하는 체계
- 관계형 데이터베이스 : 데이터의 종류나 성격에 따라 여러 개의 칼럼을 포함하는 정형화된 테이블로 구성된 데이터 항목들의 집합체
- 분산처리 데이터베이스 : 데이터의 집합이 여러 물리적 위치에 분산 배치되어 저장되는 데이터베이스

5) 데이터 적절성 검증

- 데이터 누락 검정 : 수집 데이터 세트의 누락, 결측 여부를 판단하여 누락 발생시 재수집
- 소스 데이터와 비교 : 수집 데이터와 소스 데이터의 사이즈 및 개수를 비교 검증
- 데이터의 정확성 점검 : 유효하지 않는 데이터 존재여부 점검
- 보안 사항 점검 : 수집 데이터의 개인정보 유무 등 보안 사항의 점검 필요
- 저작권 점검 : 데이터의 저작권 등 법률적 검토 수행
- 대량 트래픽 발생 여부 : 네트워크 및 시스템에 트래픽을 발생시키는 데이터 여부 검증

### 03. 분석 마스터 플랜과 로드맵 설정

1) 데이터 변환 : 데이터를 하나의 표현 형식에서 다른 형식으로 변형하는 과정

- 데이터 변환 방식의 종류
    - 비정형 데이터를 정형 데이터 형태로 저장하는 방식(관계형 데이터베이스)
    - 수집 데이터를 분산파일시스템으로 저장하는 방식(HDFS 등)
    - 주제별, 시계열적으로 저장하는 방식(데이터 웨어하우스)
    - 키-값 형태로 저장하는 방식(NoSQL)
- 데이터 변환 수행 자료
    - 데이터 수집 계획서
    - 수집 솔루션 메뉴얼
    - 데이터 변환 솔루션
    - 하둡 오퍼레이션 메뉴얼
    - 소프트웨어 아키텍처 개념도

2) 데이터베이스 구조 설계

- 수집 데이터를 저장하기 위한 데이터베이스 구조 설계
    - 수집 데이터를 바로 HDFS에 저장하여 데이터 분석 가능
    - 수집 데이터를 루비, 파이썬 등으로 데이터 변환 과정을 거쳐 데이터베이스에 저장하기도 함
- DBMS 구축 여부 결정
    - 수집 대상을 확인하고, 필요 데이터의 속성을 파악하여 DBMS 구축 여부 결정
    - 수집 데이터의 특성에 따라 저장 데이터베이스 생성 여부 결정 : 수집 데이터가 정형 데이터일 경우는 수집 솔루션을 거쳐 바로 데이터베이스에 저장하지만, 그렇지 않은 경우 저장하고자 하는 데이터베이스의 종류를 선택하고 데이터에 맞게 모델링
    - 저장 데이터베이스는 분석이 쉬운 RDBMS를 보편적으로 사용
- 저장 데이터베이스 결정 : 다양한 사용, 비상용, 오픈소스 DBMS를 검토
- DBMS 설치 : 선택한 DBMS 설치하고, 정상적인 설치 여부 확인
- 테이블 구조 설계
    - 필요 데이터의 속성을 구체적으로 팡가
    - 테이블 구조를 설계하여 테이블 생성

3) 비정형/반정형 데이터의 변환 : 데이터 전처리나 후처리가 수행되기 전에 비정형/반정형 데이터를 구조적 형태로 전환하여 저장하는 과정

- 수집 데이터의 속성 구조 파악
    - 수집할 데이터 파악
    - 수집할 데이터 구조를 정의하고 적절한 변수명으로 구분
- 데이터 수집 절차에 대한 수행 코드 정의
    - 추출하고자 하는 정보들의 위치와 정보 구조 파악
    - 필요 데이터 추출
- 데이터 저장 프로그램 작성 : 생성된 데이터베이스 테이블에 수집 데이터를 저장하는 프로그램 작성
- 데이터베이스에 저장 : 데이터베이스 테이블로 수집 데이터 저장

4) 융합 데이터베이스 설계

- 데이터의 유형과 의미를 파악하여 활용 목적별 융합 DB 설계
- 활용 업무데이터 요구사항을 분석하고, 데이터 표준화 활동 및 모델링 과정을 수행해야 함
- 요구사항 분석
    - 업무 활용 목적과 방향을 파악하여 어떤 데이터의 속성들이 필요한지 파악
    - 필요한 데이터 항목, 개인정보 또는 민감정보 포함 여부 식별
- 데이터 표준화와 모델링 수행
    - 표준 코드, 표준 용어, 데이터 도메인(데이터값이 공통으로 갖는 형식과 값의 영역) 등을 정의
    - 개념적 설계 수행 : 저장된 데이터를 엔티티와 애트리뷰트로 추출하여, 앤티티 간의 관계를 정의하고 ER 다이어그램을 그림
    - 논리적 설계 수행 : 작성된 ER 다이어그램을 기반으로 매핑하여 관계형 스키마를 만들어 냄

5) 고려사항

- 비정형, 반정형 데이터를 데이터 분석의 용이성을 위해 정형화된 데이터베이스로 변환함에 집중
- 수집 데이터의 속성 구조를 정확히 파악하여야 툴을 이용한 데이터를 쉽게 저장 가능
- 융합 DB 구성은 활용 업무 목적을 정확히 판단하는 것이 중요하고, 융합 DB는 쉽게 자동화 구축될 수 있도록 설계


### 04. 데이터 비식별화

1) 비식별화 개요

- 개인정보 비식별화는 개인정보를 식별할 수 있는 값들을 몇 가지 정해진 규칙으로 대체하거나 사람의 판단에 따라 가공하여 개인을 알아볼 수 없도록 하는 조치
- 정보주체를 알아볼 수 없도록 비식별 조치를 적정하게 한 비식별 정보는 개인 정보가 아닌 것으로 추정되며, 따라서 빅데이터 분석 등에 활용 가능
- 식별자와 속성자
    - 식별자는 개인 또는 개인과 관련한 사물에 고유하게 부여된 값 또는 이름
    - 데이터셋에 포함된 식별자는 원칙적으로 삭제조치하며, 데이터 이용 목적상 필요한 식별자는 비식별 조치 후 활용
    - 속성자는 개인과 관련된 정보로서 다른 정보와 쉽게 결합하는 경우 특정 개인을 알아볼 수도 잇는 정보
    - 데이터셋에 포함된 속성자도 데이터 이용 목적과 관련이 없는 경우에는 원칙적으로 삭제하며, 데이터 이용 목적과 관련이 있을 경우 가명처리, 총계처리 등의 기법을 활용하여 비식별 조치
- 비식별 조치 방법
    - 가명처리, 총계처리, 데이터 삭제, 데이터 범주화, 데이터 마스킹 등 여러 가지 기법을 단독 또는 복합적으로 활용
    - 각각의 기법에는 이를 구현할 수 있는 다양한 세부기술이 있으며, 데이터 이용 목적과 기별법 장, 단점 등을 고려하여 적절한 기법, 세부기술을 선택, 활용

2) 가명처리 : 개인 식별이 가능한 데이터를 직접적으로 식별할 수 없는 다른 값으로 대체하는 기법

- 휴리스틱 가명화
    - 식별자에 해당하는 값들을 몇 가지 정해진 규칙으로 대체하거나 사람의 판단에 따라 가공하여 자세한 개인정보를 숨기는 방법
    - 식별자의 분포를 고려하거나 수집된 자료의 사전 분석을 하지 않고도 모든 데이터를 동일한 방법으로 가공하기 때문에 사용자가 쉽게 이해하고 활용 가능
    - 활용할 수 있는 대체 변수에 한계가 있으며, 다른 값으로 대체하는 일정한 규칙이 노출되는 취약점이 있어 규칙 수립 시 개인을 쉽게 식별할 수 없도록 세심한 고려 필요
    - 적용정보 : 성명, 사용자 ID, 소속(직장)명, 기관번호, 주소, 신용등급, 휴대전화번호, 우편번호, 이메일 주소 등
- 암호화
    - 정보 가공시 일정한 규칙의 알고리즘을 적용하여 암호화함으로써 개인정보 대체하는 방법
    - 통상적으로 다시 복호가 가능하도록 복호화 키를 가지고 있어서 이에 대한 보안방안 필요
    - 일방향 암호화를 사용하는 경우는 이론상 복호화가 원천적으로 불가능
    - 적용정보 : 주민등록번호, 여권번호, 의료보험번호, 외국인등록번호, 사용자 ID, 신용카드번호, 생체정보 등
- 교환 방법
    - 기존의 데이터베이스의 레코드를 사전에 정해진 외부의 변수(항목)값과 연계하여 교환
    - 적용정보 : 사용자 ID, 요양기관번호, 기관번호, 나이, 성별, 신체정보(신장, 혈액형 등), 소득, 휴대전화번호, 주소 등

3) 총계처리

- 통계 값(전체 혹은 부분)을 적용하여 특정 개인을 식별할 수 없도록 함
- 개인과 직접 관련된 날짜 정보(생일, 자격 취득일), 기타 고유 특징(신체정보, 진료기록, 병력정보, 특정소비기록 등 민감한 정보)을 주요 대상
- 데이터 전체 또는 부분을 집계(총합, 평균 등)
- 부분 총계 : 데이터셋 내 일정부분 레코드만 총계 처리하며, 다른 데이터 값에 비하여 오차 범위가 큰 항목을 통계값(평균 등)으로 변환
- 라운딩
    - 집계 처리된 값에 대하여 라운딩(올림, 내림, 반올림) 기준을 적용하여 최종 집계 처리하는 방법
    - 일반적으로 세세한 정보보다는 전체 통계정보가 필요한 경우 많이 사용
- 재배열
    - 기존 정보 값은 유지하면서 개인이 식별되지 않도록 데이터를 재배열하는 방법
    - 개인의 정보를 타인의 정보와 뒤섞어서 전체 정보에 대한 손상 없이 특정 정보가 해당 개인과 연결되지 않도록 함
    - 적용정보 : 나이, 신장, 소득, 질병, 신용등급, 학력 등

4) 데이터 삭제 : 개인을 식별할 수 있는 정보(이름, 전화번호, 주소, 생년월일, 사진, 고유식별정보, 생체정보, 기타)를 주요 대상으로 함

- 식별자 (부분) 삭제
    - 원본 데이터에서 식별자를 단순 삭제하는 방법과 일부만 삭제하는 방법
    - 남아 있는 정보 그 자체로도 분석의 유효성을 가져야 함과 동시에 개인을 식별할 수 없어야 하며, 인터넷 등에 공개되어 있는 정보 등과 결합하였을 경우에도 개인을 식별할 수 없어야 함
- 레코드 삭제
    - 다른 정보와 뚜렷하게 구별되는 레코드 전체를 삭제하는 방법
    - 통계분석에 있어서 전체 평균에 비하여 오차범위를 벗어나는 자료를 제거할 때에도 사용 가능
- 식별요소 전부삭제
    - 식별자뿐만 아니라 잠재적으로 개인을 식별할 수 있는 속성자까지 전부 삭제하여 프라이버시 침해 위험을 줄이는 방법
    - 개인정보 유출 가능성을 최대한 줄일 수 있지만 데이터 활용에 필요한 정보까지 사전에 모두 없어지기 때문에 데이터의 유용성이 낮아지는 문제 발생

5) 데이터 범주화 : 특정 정보를 해당 그룹의 대푯값 또는 구간값으로 변환(범주화)하여 개인 식별 방지

- 감추기
    - 명확한 값을 숨기기 위하여 데이터의 평균 또는 범주 값으로 변환하는 방식
    - 다만 특수한 성질을 지닌 개인으로 구성된 단체 데이터의 평균이나 범주 값은 그 집단에 속한 개인의 정보를 쉽게 추론 가능
- 랜덤 라운딩 : 수치 데이터를 임의의 수 기준으로 올림 또는 내림하는 기법으로 수치 데이터 이외의 경우에도 확장 적용 가능
- 범위 방법
    - 수치 데이터를 임의의 수 기준의 범위로 설정하는 기법
    - 해당 값의 범위 또는 구간으로 표현
- 제어 라운딩
    - 랜덤 라운딩 방법에서 어떠한 특정 값을 변경할 경우 행과 열의 합이 일치하지 않는 단점 해결을 위해 행과 열이 맞지 않는 것을 제어하여 일치시키는 기법
    - 컴퓨터 프로그램으로 구현하기 어렵고 복잡한 통계표에는 적용하기 어려우며, 해결할 수 있는 방법이 존재하지 않을 수 있어 아직 현장에서는 잘 사용하지 않음

6) 데이터 마스킹 : 데이터의 전부 또는 일부분을 대체 값(공백, 노이즈 등)으로 변환

- 임의 잡음 추가
    - 개인 식별이 가능한 정보에 임의의 숫자 등 잡음을 추가(더하기 또는 곱하기) 하는 방법
    - 지정된 평균과 분산의 범위 내에서 잡음이 추가되므로 원 자료의 유용성을 해치지 않으나, 잡음 값은 데이터 값과는 무관하기 때문에, 유효한 데이터로 활용하기 곤란
- 공백과 대체 : 특정 항목의 일부 또는 전부를 공백 또는 대체문자로 바꾸는 기법

7) 적정성 평가

- 개인정보 비식별 조치가 충분하지 않은 경우 공개 정보 등 다른 정보와의 결합. 다양한 추론 기법 등을 통해 개인이 식별될 우려가 있으므로, 개인정보 보호책임자 책임 하에 외부전문가가 참여하는 '비식별 조치 적정성 평가단'을 구성, 개인식별 가능성에 대한 엄격한 평가 필요
- 적정성 평가 시 프라이버시 보호 모델 중 최소한의 수단으로 k-익명성을 활용하며, 필요시 추가적인 평가모델 활용
- k-익명성
    - 공개된 데이터에 대한 연결공격 등 취약점을 방어하기 위해 제안된 개인정보보호 모델로 비식별화 조치를 위한 최소한의 기준으로 사용
    - k-익명성은 주어진 데이터 집합에서 같은 값이 적어도 k개 이상 존재하도록 하여 쉽게 다른 정보로 결합할 수 없도록 함
- l-다양성
    - k-익명성에 대한 두 가지 공격, 즉 동질성 공격 및 배경지식에 의한 공격을 방어하기 위한 모델로, 주어진 데이터 집합에서 함께 비식별되는 레코드들은 적어도 l개의 서로 다른 정보를 가지도록 함
    - 비식별 조치 과정에서 충분한 다양한 서로 다른 정보를 갖도록 동질 집합을 구성함으로써 다양성의 부족으로 인한 공격에 방어가 가능하고, 배경지식으로 인한 공격에도 일정 수준의 방어능력을 가질 수 있음
- t-근접성
    - l-다양성의 취약점(쏠림 공격, 유사성 공격)을 보완하기 위한 모델로 값의 의미를 고려하는 모델
    - t-근접성은 동질 집합에서 특정 정보의 분포와 전체 데이터 집합에서 정보의 분포가 t 이하의 차이를 보여야 하며, 각 동질 집합에서 '특정 정보의 분포'가 전체 데이터집합의 분포와 비교하여 너무 특이하지 않도록 함
    - t-접근성은 정보의 분포를 조정하여 정보가 특정 값으로 쏠리거나 유사한 값들이 뭉치는 경우를 방지하는 방법
    - t 수치가 0에 가까울수록 전체 데이터의 분포와 특정 데이터 구간의 분포 유사성이 강해지기 때문에 그 익명성의 방어가 더 강해지는 경향
    - 익명성 강화를 위해 특정 데이터들을 재배치해도 전체 속성자들의 값 자체에는 변화가 없기 때문에 일반적인 경우에 정보 소신의 문제가 크지 않음

### 05. 데이터 품질 검증

1) 데이터 품질 관리

- 정의 : 비즈니스 목표에 부합한 데이터 분석을 위해 가치성, 정확성, 유용성 있는 데이터를 확보하고, 신뢰성 있는 데이터를 유지하는 데 필요한 관리 활동
- 중요성
    - 분석 결과의 신뢰성은 분석 데이터의 신뢰성과 직접 연계
    - 빅데이터의 특성을 반영한 데이터 품질 관리 체계를 구축하여 효과적인 분석결과 도출

2) 데이터 품질

- 정형 데이터 품질 기준
    - 완전성 : 필수항목에 누락이 없어야 함
    - 유일성 : 데이터 항목은 유일해야 하며 중복되어서는 안됨
    - 유효성 : 데이터 항목은 정해진 데이터 유효범위 및 도메인을 충족
    - 일관성 : 데이터가 지켜야할 구조, 값, 표현되는 형태가 일관되게 정의되고, 서로 일치
    - 정확성 : 실세계에 존재하는 객체의 표현 값이 정확히 반영
- 비정형 데이터 품질 기준 : 비정형 데이터 자체에 대한 품질 기준은 컨텐츠 유형에 따라 다소 다름

3) 데이터 품질 진단 기법

- 정형 데이터 품질 진단 : 데이터 프로파일링 기법을 통해 진단 가능
- 비정형 데이터 품질 진단 : 품질 세부 기준을 정하여 항목별 체크리스트를 작성하여 진단

4) 데이터 품질 검증 수행

- 수집 데이터 품질 보증 체계를 수립하여 품질 점검 수행 후 품질검증 결과서 작성
- 품질 점검 수행 과정에서 데이터 오류수정이 용이하지 않을 경우 데이터를 재수집

## SECTION 02. 데이터 적재 및 저장

### 01. 데이터 적재

1) 데이터 적재 도구

- 수집된 데이터는 빅데이터 분석을 위한 저장 시스템에 적재해야 함
- 관계형 데이터베이스, HDFS를 비롯한 분산파일시스템, NoSQL 저장 시스템에 데이터를 적재 가능
- 데이터 수집 도구를 이용한 데이터 적재
    - 플루언티드
        - 트레저 데이터에서 개발된 크로스 플랫폼 오픈소스 데이터 수집 소프트웨어
        - 사용자의 로그를 다양한 형태로 입력받아 JSON 포맷으로 변환 한 뒤 다양한 형태로 출력
    - 플럼
        - 많은 양의 로그 데이터를 효율적으로 수집, 취합, 이동하기 위한 분산형 소프트웨어
        - 로그 데이터 수집과 네트워크 트래픽 데이터, 소셜 미디어 데이터, 이메일 메시지 데이터 등 대량의 이벤트 데이터 전송을 위해 사용
    - 스크라이브
        - 수많은 서버로부터 실시간으로 스트리밍되는 로그 데이터를 집약 시키기 위한 서버
        - 클라이언트 사이드의 수정 없이 스케일링 가능하고 확장이 가능
    - 로그스태시 : 다양한 소스에서 데이터를 수집하여 변환한 후 자주 사용하는 저장소
- NoSQL DBMS가 제공하는 도구를 이용한 데이터 적재
    - 로그 수집기를 이용한 방법처럼 많은 기능을 사용할 수는 없지만, 수집한 데이터가 csv등의 텍스트 데이터라면 mongoimport와 같은 적재 도구를 사용하여 데이터 적재를 수행
    - 로그 수집 도구를 쓰는 방식처럼 데이터 수집 주기 등을 환경설정하여 사용할 수 없음
- 관계형 DBMS의 데이터를 NoSQL DBMS에서 적재
    - 기존의 운영 중이던 관계형 데이터베이스로부터 데이터를 추출하여 NoSQL 데이터베이스로 적재 가능
    - 데이터 변형이 많이 필요하면, 데이터 적재를 위한 프로그램을 작성하여 적재할 수 있고, 큰 변화 없이 적재한다면 SQLtoNoSQLimporter, Mongify 등의 도구를 사용하여 적재 가능

2) 데이터 적재 완료 테스트

- 데이터 적재 내용에 따라 체크리스트를 작성
    - 정형 데이터인 경우에는 테이블의 개숭와 속성의 개수 및 데이터 타입의 일치 여부, 레코드 수 일치 여부가 체크리스트가 될 수 있음
    - 반정형이나 비정형인 경우에는 원천 데이터의 테이블이 목적지 저장시스템에 맞게 생성되었는지, 레코드 수가 일치하는지 등이 체크리스트에 포함될 수 있음
- 데이터 테스트 케이스를 개발
    - 적재된 레코드 수를 확인하는 간단한 것 외에도 원천 데이터 중에 특정 데이터에 대해 샘플링을 해서 목적지 저장시스템에서 조회하는 테스트 케이스를 개발 가능
    - 적재한 대량 데이터의 데이터 타입, 특별히 한글 문자 등의 ASCII 코드가 아닌 문자들이 정상적으로 적재되는지 확인 필요
    - 문자열이 숫자로 이루어지면 문자열로 적재되었는지 또는 숫자인데 문자열로 적재되지는 않았는지 확인
- 체크리스트 검증 및 데이터 테스트 케이스 실행
    - 이전 단계에서 작성된 체크리스트와 테스트 케이스에 대해 검증 실행
    - 검증 결과를 분석하여 데이터 적재 결과 보고서 작성

### 02. 데이터 저장

1) 빅데이터 저장시스템 : 대용량 데이터 집합을 저장하고 관리하는 시스템으로 사용자에게 데이터 제공 신뢰성과 가용성을 보장하는 시스템

- 파일 시스템 저장방식
    - 빅데이터를 확장 가능한 분산 파일의 형태로 저장하는 방식의 대표적인 예는 Apache HDFS, 구글의 GFS 등
    - 파일 시스템 저장방식은 저사양 서버들을 활용하여 대용량, 분산, 데이터 집중형의 어플리케이션을 지원하며 사용자들에게 고성능 fault-tolerance 환경을 제공하도록 구현
- 데이터베이스 저장방식
    - 빅데이터를 저장하는 방식에는 전통적인 관계형 데이터베이스 시스템을 이용하거나 NoSQL 데이터베이스 시스템을 이요하는 방식
    - NoSQL 데이터베이스는 대용량 데이터 저장 측면에서 봤을 때, 관계형 데이터베이스보다 수평적 확장성, 데이터 복제, 간편한 API 제공, 일관성 보장 등의 장점

2) 분산 파일 시스템

- 하둡 분산파일 시스템
    - 하둡은 아파치 진영에서 분산 환경 컴퓨팅을 목표로 시작한 프로젝트로 분산처리를 위한 파일 시스템
    - HDFS는 대용량 파일을 클러스터에 여러 블록으로 분산하여 저장하며, 블록들은 마지막 블록을 제외하고 모두 크기 도일 (기본 크기 64MB)
    - HDFS는 마스터 하나와 여러 개의 슬레이브로 클러스터링 되어 구성
        - 마스터노드는 네임노드라고 하며 슬레이브를 관리하는 메타데이터와 모니터링 시스템을 운영
        - 슬레이브노드는 데이터노드라고 하며 데이터 블록을 분산처리
    - 데이터 손산을 방지하기 위해서 데이터 복제 기법 사용
- 구글 파일 시스템
    - 구글 파일 시스템은 엄청나게 많은 데이터를 보유해야하는 구글의 핵심 데이터 스토리지와 구글 검색 엔진을 위해 최적화된 분산 파일 시스템
    - 마스터, 청크 서버, 클라이언트로 구성
        - 마스터는 GFS 전체의 상태를 관리하고 통제
        - 청크서버는 물리적인 하드디스크의 실제 입출력 처리
        - 클라이언트는 파일을 읽고 쓴느 동작을 요청하는 어플리케이션
    - 파일들은 일반적인 파일 시스템에서의 클러스터들과 섹터들과 비슷하게 64MB로 고정된 크기의 청크들로 나누어서 저장됨
    - 가격이 저렴한 서버에서도 사용되도록 설계되었기 때문에 하드웨어 안정성이나 자료들의 유실에 대해서 고려하여 설계되었고 응답시간이 조금 길더라도 데이터의 높은 처리성능에 중점

3) NoSQL

- 개요
    - NoSQL 데이터베이스는 전통적인 관계형 데이터베이스보다 유연한 데이터의 저장 및 검색을 위한 매커니즘 제공
    - 대규모 데이터를 처리하기 위한 확장성, 가용성 및 높은 성능을 제공하여, 빅데이터 처리와 저장을 위한 플랫폼으로 활용
    - SQL 계열 쿼리를 지원하는 데이터베이스도 있음
- CAP 이론 : 기존 데이터 저장 구조의 한계
    - 2002년 버클리 대학의 에릭 브루어 교수가 발표한 이론
    - 분산 컴퓨팅 환경의 특징을 일관성, 가용성, 지속성 세 가지로 정의 할 수 있는데, 어떤 시스템이든 이 세 가지 특성을 동시에 만족하기 어려움
        - 일관성 : 분산 환경에서 모든 노드가 같은 시점에 같은 데이터를 보여줘야 함
        - 가용성 : 일부 노드가 다운되어도 다른 노드에 영향을 주지 않아야 함
        - 지속성 : 데이터 전송 중에 일부 데이터를 손실하더라도 시스템은 정상 동작해야 함
- NoSQL의 기술적 특성
    - 無 스키마
        - 데이터를 모델링하는 고정된 데이터 스키마 없이 키 값을 이용하여 다양한 형태의 데이터 저장 및 접근 가능
        - 데이터 저장 방식은 크게 열, 값, 문서, 그래프 등의 네 가지를 기반으로 구분
    - 탄력성
        - 시스템 일부에 장애가 발생해도 클라이언트가 시스템에 접근 가능
        - 응용 시스템의 다운 타임이 없도록 하는 동시에 대용량 데이터의 생성 및 갱신
        - 질의에 대응할 수 있도록 시스템 규모와 성능 확장이 용이하며, 입출력의 부하를 분산시키는데에도 용이한 구조
    - 질의 가능 : 수십 대에서 수천 대 규모로 구성된 시스템에서도 데이터의 특성에 맞게 효율적으로 데이터를 검색, 처리할 수 있는 질의 언어, 관련 처리 기술, API를 제공
    - 캐싱
        - 대규모 질의에도 고성능 응답 속도를 제공할 수 있는 메모리 기반 캐싱 기술을 적용하는 것이 중요
        - 개발 및 운영에도 투명하고 일관되게 적용할 수 있는 구조
- NoSQL의 데이터 모델
    - NoSQL의 데이터 저장 방식에 따라 키-값 구조, 칼럼기반 구조, 문서기반 구조로 구분
    - 키-값 데이터베이스
        - 데이터를 키와 그에 해당하는 값의 쌍으로 저장하는 데이터 모델에 기반
        - 단순한 데이터 모델에 기반을 두기 때문에 관계형 데이터베이스보다 확장성이 뛰어나고 질의 응답시간이 빠름
        - 아마존의 Dynamo 데이터베이스가 효시이며, Redis와 같은 in-memory 방식의 오픈소스 데이터베이스가 대표적
    - 열기반 데이터베이스
        - 데이터를 로우가 아닌 칼럼기반으로 저장하고 처리하는 데이터베이스
        - 칼럼과 로우는 확장성을 보장하기 위하여 여러 개의 노드로 분할되어 자장되어 관리
        - 구글의 Bigtable이 칼럼기반 데이터베이스의 효시이며, 이로부터 파생된 Cassandra, HBase, HyperTable 등이 대표적인 칼럼기반 데이터베이스
    - 문서기반 데이터베이스
        - 문서 형식의 정보를 저장, 검색, 관리하기 위한 데이터베이스
        - 키-값 데이터베이스보다, 문서의 내부 구조에 기반을 둔 복잡한 형태의 데이터 저장을 지원하고 이에 딸느 최적화가 가능하다는 장점
        - 대표적으로 MongoDB, SimpleDB, CouchDB

4) 빅데이터 저장시스템 선정을 위한 분석

- 기능성 비교분석
    - 데이터 모델
        - 데이터를 테이블로 정리하고 사용하는데 무리가 없다면, RDBMS가 필요하고 NoSQL을 사용할 필요가 없음
        - MongoDB는 RDBMS보다 한층 유연한 스키마를 활용하는 것이 가능하므로, RDBMS에서 문서 중심의 데이터 모델로 전환하는 데 도움
        - Apache CouchDB는 데이터 저장소에 대한 인터페이스로 RESTful HTTP를 지원하므로 상대적으로 웹기반 시스템을 구축하는 데 장점이 있는 document 데이터베이스 시스템
        - 단순 key-value 쌍을 저장하여 대규모 사용자와 부하 분산을 위한 안정적인 분산 저장소가 필요한 경우, Dyname나 Redis 선택
        - Casssandra, HyperTable, HBase는 column family 중심의 데이터베이스 시스템으로 극단적인 확장성을 보장할 수 있다는 장점
    - 확장성
        - 확장성에는 HBase, Cassandr, HyperTable과 같은 column-oriented 데이터베이스가 가장 뛰어남
        - Redis와 같은 인메모리 방식의 데이터베이스와 MongoDB, CouchDB와 같은 documnet 데이터베이스가 약간 뒤쳐지는 것으로 알려져 있음
    - 트랜잭션 일관성
        - 트랜잭션의 일관성은 데이터 수정, 삭제 등의 작업이 빈번하게 일어나는 환경에서는 중요도가 높지만, 배치중심의 하둡 기반 분석환경에서는 중요도가 높지 않음
        - 트랜잭션의 일관성이 중요한 분야에서는 RDBMS를 선택해야 하며, 그렇지 않을 때에만 NoSQL을 선택
    - 질의 지원
        - MongoDB는 SQL과 유사한 문법에 기반을 두어 쉽게 학습할 수 있는 우수한 질의 인터페이스 지원
        - CouchDB도 MongoDB에 비해 뒤처지지 않으며, 뷰 개념을 이해하고 활용하면 간편
        - key-value 데이터베이스의 대표 격인 Redis도 풍부한 질의기능 제공
        - HBase나 HyperTable은 자체 질의지원 기능은 제공하지 않으나 Hive를 통해 SQL과 유사한 형태의 질의기능을 사용할 수 있다는 장점
    - 접근성
        - MongoDB를 접근하기 위한 드라이버는 아주 다양하며, 현존하는 주류 라이브러리용 드라이버를 대부분 지원
        - CouchDB는 웹 통신을 지원하는 프로그래밍 언어라면 다 사용 가능
        - 기타 Redis, HBase, HyperTable, Cassandr는 대부분의 프로그래밍 언어에서 연결이 가능하도록 언어 바인딩 지원
- 분석방식 및 환경
    - 빅데이터 저장 방식은 빅데이터를 파일 시스템 형식으로 저장하는 방식, NoSQL 저장시스템을 사용하는 방식이나, RDBMS에 기반을 둔 데이터 웨어하우스 방식
    - 필요로 하는 분석 및 검색결과가 상시로 온라인 형식으로 필요한지, 분석가를 통해 별도의 프로세스를 거쳐 제공받는지 등을 구분하여 저장 방식과 환경 선택
- 분석대상 데이터 유형 : 분석대상이 되는 데이터 유형이 기업 내외부에서 발생하는 기업 데이터인가, IoT 환경에서 발생하는 데이터인가 혹은 기타 다양한 과학 분야나 바이오, 의학 분야에서 취급되는 데이터이냐에 따라 데이터의 volume, velocity, variety, veracity 등을 고려하여 빅데이터 저장시스템 선택
- 기존 시스템과의 연계
    - 저장 대상 데이터의 유형이 대부분 테이블로 정의될 수 있는 형태이고 기존에 RDBMS 기반의 데이터 웨어하우스가 도입된 형태라면 기존 시스템을 그대로 활용하여 저장
    - 기존에 HDFS만을 활용하여 빅데이터 저장시스템이 구축되어 있으나, SQL-like 분석환경을 구축하고자 한다면 HBase를 추가 도입하는 것 권장
    - 빅데이터 분석 애플리케이션 키-값 쌍 처리 위주로 시스템이 구현되어 있다면 Redis 등을 도입하는 것이 좋음
    - IoT 데이터처럼, 다양한 데이터가 지속해서 실시간 발생하는 것을 수집하는 시스템 환경이라면 key-value, 데이터베이스나 확장성이 중요한 요소라면 Cassandra와 같이 확장성이 보장된 칼럼 기반 데이터베이스를 선정하는 것이 좋음
    
5) 데이터 밠생 유형 및 특성

- 대용량 실시간 서비스 데이터 개요
    - 대상 데이터의 용량, 실시간 여부, 정형, 비정형 등 유형 및 요건을 파악하여 빅데이터 저장 계획 수립에 반영
    - 일반적으로 실시간으로 처리해야 하는 데이터를 스트리밍 데이터로 통칭하는데 대용량의 특성과 무중단 서비스를 보장하는 저장 체계를 구축
    - 실시간 데이터 처리를 위해 사용되는 시스템으로는 대표적으로 스파크, 스톰 등이 있으며 배치 기반의 대용량 데이터 처리에 특화된 하둡 시스템보다 실시간 대용량 처리에 특화
    - IoT에서 발생하는 센서 데이터, 네트워크 모니터링 데이터, 에너지 관리 분야 데이터, 통신 데이터, 웹 로그, 주식 데이터나 생산형장에서 발생하는 데이터 등이 해당
- 대용량 실시간 서비스 데이터 저장
    - 실시간 빅데이터 처리를 위해 스톰을 사용한다고 가정하면, 스톰은 저장소가 없으므로 외부 저장 시스템과의 연동 필수적
    - 다양한 소스의 로그로부터 데이터가 발생하는 환경에서는 스톰을 통하여 데이터를 전처리한 후 HDFS나 MongoDB, Cassandra, HBase와 같은 NoSQL을 저장소로 사용하거나, 데이터를 정규화하여 일반 RDBMS를 저장소로 사용
    - 대표적인 실시간 데이터 처리 시스템인 스파크 역시 내장된 저장소를 제공하지 않기 때문에 외부 빅데이터 저장 시스템과의 연계가 필수적
    - 실시간 서비스를 웹 페이지로 제공하는 것이 필요한 환경에서는 Redis와 같은 메인 메모리 저장 시스템을 저장소로 사용

6) 안정성과 신뢰성 확보 및 접근성 제어계획 수립

- 빅데이터 저장시스템 안정성 및 신뢰성 확보
    - 안정성 및 신뢰성을 확보하고 보장하기 위해 저장 계획 수립단계에서 용량산정 필요
    - 조직의 빅데이터 활용목적에 부합하는 현재와 향후 증가 추세를 추정 반영
- 접근성 제어계획 수립 : 저장 시스템의 사용자와 관리자 유형, 역할 및 기능을 정의하고 각각에 해당하는 제어계획 수립